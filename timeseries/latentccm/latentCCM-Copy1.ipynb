{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Latent CCM : A Simple Worked-out Example\n",
    "\n",
    "In this notebook, we will use a simple RNN to model the dynamics of 2 Lorenz attractors. We then show that using that reconstruction, we can infer the causal direction.\n",
    "\n",
    "For sake of simplicity and illustration, we'll only use regularly sampled time series here. The general irregular case is discussed in details in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "import causal_inf\n",
    "import torch\n",
    "#import DATADIR\n",
    "from datagen_utils import generate_Lorenz_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "We use 2 Lorenz dynamical systems $X$ and $Y$ with unidirectional coupling. In our example, we set : \n",
    "\n",
    "$$ Y \\rightarrow X$$\n",
    "\n",
    "So Y is causing the X. The exact dynamical system is as follows : \n",
    "\n",
    "$$ \\frac{dX_0}{dt} = \\sigma (X_1-X_0) + c_{Y\\rightarrow X} (X_1-Y_0) $$\n",
    "$$ \\frac{dX_1}{dt} = X_0 \\cdot (\\rho-X_2)-X_1 $$\n",
    "$$ \\frac{dX_2}{dt} = (X_0 \\cdot X_1 - \\beta X_2) $$\n",
    "$$ \\frac{dY_0}{dt} = \\sigma (Y_1-Y_0) + c_{X\\rightarrow Y} (Y_1-X_0)  $$\n",
    "$$ \\frac{dY_1}{dt} = Y_0 \\cdot (\\rho-Y_2)-Y_1 $$\n",
    "$$ \\frac{dY_2}{dt} = (Y_0\\cdot Y_1 - \\beta Y_2) $$\n",
    "\n",
    "We set $c_{Y\\rightarrow X}=3.5$ and $c_{X\\rightarrow Y} = 0$ such that the coupling and the causal link is unidirectional. We further use the following parameters : \n",
    "\n",
    "$$\\sigma = 10$$\n",
    "$$\\rho = 28$$\n",
    "$$\\beta = 8/3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y,_ = generate_Lorenz_data()\n",
    "X = y[:,:3]\n",
    "Y = y[:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Below we plot the first samples of Y over the 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot(Y[:10000,0],Y[:10000,1],Y[:10000,2],color = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling the dynamics\n",
    "\n",
    "We first subsample the data to lower resolution. We then learn a GRU to model the dynamics of X and Y separetely.\n",
    "\n",
    "Once the models are trained, we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "X_tensor = torch.stack(torch.Tensor(X[::10,:]).chunk(1000))\n",
    "Y_tensor = torch.stack(torch.Tensor(Y[::10,:]).chunk(1000))\n",
    "\n",
    "datasetX = torch.utils.data.TensorDataset(X_tensor)\n",
    "datasetY = torch.utils.data.TensorDataset(Y_tensor)\n",
    "\n",
    "dlX = torch.utils.data.DataLoader(datasetX, batch_size = 32, shuffle = False)\n",
    "dlY = torch.utils.data.DataLoader(datasetY, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[::10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_reconstruction(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.RNN = torch.nn.GRU(input_size = 3, hidden_size = hidden_size, num_layers = 1)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size,3)\n",
    "    def forward(self,x):\n",
    "        output, _  = self.RNN(x)\n",
    "        output = self.output_layer(output)\n",
    "        return output\n",
    "    \n",
    "    def hidden_only(self,x):\n",
    "        output, _  = self.RNN(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest train loop\n",
    "dls = {\"X\":dlX, \"Y\":dlY}\n",
    "hiddens = {}\n",
    "for side in [\"X\",\"Y\"]:\n",
    "    loss_criterion = torch.nn.MSELoss()\n",
    "    model = GRU_reconstruction(hidden_size = 20)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = 0.01)\n",
    "    for epoch in range(50):\n",
    "        train_loss = 0\n",
    "        for i,b in enumerate(dls[side]):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(b[0])\n",
    "            loss1 = loss_criterion(y_hat[:,:-1,:],b[0][:,1:,:])\n",
    "            loss2 = loss_criterion(y_hat[:,:-2,:],b[0][:,2:,:])\n",
    "            loss = (loss1)# + 0.5*loss2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach()\n",
    "        train_loss /= (i+1)\n",
    "        if (epoch%10)==0:\n",
    "            print(f\"Training_loss at epoch {epoch}: {train_loss}\")\n",
    "\n",
    "    hidden_path = []\n",
    "    for i,b in enumerate(dls[side]):\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model.hidden_only(b[0])#[:,:,:]\n",
    "        hidden_path.append(y_hat.detach())\n",
    "\n",
    "    hiddens[side] = torch.cat(hidden_path).reshape(-1,y_hat.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Cross Mapping between Latent Processes\n",
    "\n",
    "Finally, we compute the correlation score of the reconstruction between the latent processes of $X$ and $Y$. We observe a clear signal for a reconstruction from $X$ to $Y$, suggesting a causal effect from $Y$ to $X$.\n",
    "\n",
    "$$ Y \\rightarrow X $$\n",
    "\n",
    "In accordance with the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1, sc2 = causal_inf.CCM_compute(hiddens[\"X\"].numpy()[::10],hiddens[\"Y\"].numpy()[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sc1,label=\"X->Y\")\n",
    "plt.plot(sc2,label = \"Y->X\")\n",
    "plt.legend()\n",
    "plt.title(\"Cross Mapping from the Latent Process of Lorenz Dynamical Systems\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more information : https://openreview.net/forum?id=4TSiOTkKe5P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_CDM_PY",
   "language": "python",
   "name": "aki_cdm_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
