{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a601ed-49fb-48f0-a16b-7cd6c183f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827f1ec-3215-4619-881a-74e2b3bd743f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load statistics\n",
    "stg = 'stg23'\n",
    "#fs = 'rmscrbun'\n",
    "fs = 'nofs'\n",
    "oversample='raw'\n",
    "model = 'catd'    \n",
    "rmcol = '005'\n",
    "year = '3000'\n",
    "\n",
    "def prenetagam(stg, fs, oversample='raw', model='catd',rmcol='005',year=3000, maxnum_features=40, maxmax_feature=40):\n",
    "\n",
    "    result = pd.read_pickle('../DEID_resultsplit_'+model+'_'+stg+'_'+str(year)+'_'+fs+'_'+oversample+'_005.pkl')\n",
    "\n",
    "    #Get sites\n",
    "    sites = list(result['site'].unique())\n",
    "\n",
    "    #Get site year\n",
    "    gb = result.loc[:,['site','year']].drop_duplicates().groupby('site')\n",
    "    years = {}\n",
    "    gbg = [gb.get_group(x) for x in gb.groups]\n",
    "    for t in gbg:\n",
    "            t = t.reset_index(drop=True)\n",
    "            site = t.loc[0,'site']\n",
    "            years[site] = list(t['year'])    \n",
    "\n",
    "    #Get cat and num features\n",
    "    cat_features = {}\n",
    "    num_features = {}\n",
    "    for s in sites:\n",
    "        for y in years[s]:\n",
    "            X_test =  pd.read_pickle('../data/'+s+ '/X_test_'+s+'_'+str(y)+'_'+stg+'_'+fs+'_'+oversample+'.pkl')\n",
    "            cat_features[(s,y)] = list(X_test.select_dtypes('bool').columns)\n",
    "            num_features[(s,y)] = [x for x in X_test.select_dtypes('bool').columns if x not in cat_features[(s,y)]]\n",
    "\n",
    "    def featuredecode(s):\n",
    "        return s.split(':')[-1].split('(')[0]\n",
    "    def featuredecodetable(result):\n",
    "        x = pd.DataFrame(result['Feature'].unique())\n",
    "        x.columns = ['Feature']\n",
    "        x['featuredecode'] = x['Feature'].map(featuredecode)\n",
    "        return x\n",
    "    decodetable = featuredecodetable(result)\n",
    "    result = pd.merge(result, decodetable, right_on='Feature', left_on='Feature', how='left')\n",
    "\n",
    "    #Get top features\n",
    "    def top_features(shap_data, importance_type = 'Importances', max_num_features = 30):\n",
    "    #    siteyr = shap_data['siteyr'].unique()\n",
    "        siteyrlen = shap_data.loc[:,['site', 'year']].drop_duplicates().shape[0]\n",
    "        #    years.sort()\n",
    "        rank_table = shap_data.sort_values(['site', 'year', importance_type], ascending=False).loc[:,['site', 'year', 'featuredecode']].drop_duplicates().groupby(['site', 'year']).head(max_num_features).reset_index(drop=True)\n",
    "        rank_table.loc[:, 'rank'] = list(range(1,max_num_features+1))*siteyrlen\n",
    "        rank_table = rank_table.pivot(index=['site', 'year'], columns='rank', values='featuredecode')\n",
    "        return rank_table   \n",
    "\n",
    "    #topnfeature = top_features(result, max_num_features=30)\n",
    "\n",
    "    #Get top of top features\n",
    "    #maxnum_features=40\n",
    "    #maxmax_feature=40\n",
    "    #maxnum_features=10\n",
    "    #maxmax_feature=3\n",
    "\n",
    "    topnfeature = top_features(result, max_num_features=maxnum_features, importance_type='Importances')\n",
    "    numsiteyr = topnfeature.shape[0]\n",
    "    toptopfeatureN = topnfeature.melt()['value'].value_counts()\n",
    "    toptopfeatureN = toptopfeatureN[range(maxmax_feature)]\n",
    "    toptopfeature = pd.DataFrame(toptopfeatureN.keys())\n",
    "    toptopfeature.columns = ['featuredecode']\n",
    "#    pd.DataFrame(toptopfeatureN)\n",
    "\n",
    "    # # Get top feature by median\n",
    "    # maxmax_feature=10\n",
    "\n",
    "    # mediantop = result[['siteyr', 'Feature', 'Importances']].drop_duplicates().groupby(['Feature']).median('Importances').nlargest(maxmax_feature, 'Importances')\n",
    "    # mediantop = list(mediantop.index)\n",
    "    # toptopfeature2 = pd.DataFrame([decodetable[decodetable['Feature'] == x].iloc[0,1] for x in mediantop])\n",
    "    # toptopfeature2.columns = ['featuredecode']\n",
    "\n",
    "\n",
    "    # toptopfeature = toptopfeature2\n",
    "    # toptopfeature2\n",
    "\n",
    "    result = result.drop(['valCI95_0', 'valCI95_1', 'absvalCI95_0', 'absvalCI95_1'],axis=1,errors='ignore')\n",
    "\n",
    "    #Get all curves\n",
    "    curves = pd.merge(result, toptopfeature, right_on='featuredecode', left_on='featuredecode', how='right')\n",
    "    existscurve = curves.loc[:,['site', 'year', 'featuredecode']].drop_duplicates().groupby('featuredecode').count().reset_index()\n",
    "    curves_new = curves.drop('Feature', axis=1)\n",
    "    curves_new.to_parquet('metadata_'+stg+'_'+fs+'_.parquet', compression=None)\n",
    "\n",
    "    curves_mean = curves_new.loc[curves_new['isCategorical']==False].loc[:,['site', 'year', 'featuredecode', 'mean_val']].groupby(['site', 'year', 'featuredecode']).mean().reset_index().rename(columns={'mean_val': 'mean_mean_val'})\n",
    "    curves_new_mean = pd.merge(curves_new, curves_mean, left_on=['site', 'year', 'featuredecode'], right_on=['site', 'year', 'featuredecode'], how='left').assign(mean_val=lambda x: x.mean_val-x.mean_mean_val).drop('mean_mean_val',axis=1)\n",
    "    curves_new_mean.to_parquet('metadata_mean_'+stg+'_'+fs+'_.parquet', compression=None)\n",
    "\n",
    "    #Assemble raw data\n",
    "    files = []\n",
    "    #start_dir = os.getcwd()\n",
    "    start_dir = \"../data\"\n",
    "    pattern   = \"shapdataraw*\"\n",
    "    for dir,_,_ in os.walk(start_dir):\n",
    "        files.extend(glob(os.path.join(dir,pattern))) \n",
    "    #files = [x for x in files if model in x and stg in x and oversample in x and fs in x and rmcol in x]\n",
    "    files = [x for x in files if model in x and stg in x and oversample in x and fs in x and rmcol in x and not ('drop' in x) and not ('BACKUP' in x) and '3000' in x]\n",
    "\n",
    "    for file in files:\n",
    "        print(file)    \n",
    "\n",
    "    # %%time\n",
    "    dfcollect = list()\n",
    "    newfeaturecode = pd.DataFrame(curves_new_mean['featuredecode'].unique())\n",
    "    newfeaturecode.columns = ['featuredecode']\n",
    "    newdecodetable = pd.merge(newfeaturecode, decodetable, right_on='featuredecode', left_on='featuredecode', how='left')\n",
    "    c = 0\n",
    "    for f in files:\n",
    "        c=c+1\n",
    "        print(c,len(files))\n",
    "        dft = pd.read_pickle(f)\n",
    "        dft = pd.merge(newdecodetable, dft, right_on='Feature', left_on='Feature', how='left')    \n",
    "        dft['site'] = f.split('_')[2]\n",
    "        dft['year'] = f.split('_')[3]\n",
    "        dft['siteyear'] = f.split('_')[2]+f.split('_')[3]\n",
    "        dfcollect.append(dft)\n",
    "    dfcollect = pd.concat(dfcollect)\n",
    "    dfcollect.to_parquet('metadata_raw_'+stg+'_'+fs+'_.parquet', compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f51e0-9ebd-43eb-a9f8-425a5e7f0e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stgs = [\"stg23\"]\n",
    "fss =  ['nofs']\n",
    "for stg in stgs:\n",
    "    for fs in fss:\n",
    "        prenetagam(stg, fs, oversample='raw', model='catd',rmcol='005',year=3000, maxnum_features=40, maxmax_feature=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25ba5d-7bd8-4272-9e73-6a630f6bf9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "\n",
    "# def worker(f):\n",
    "#     print(f)\n",
    "#     dft = pd.read_pickle(f)\n",
    "#     dft = pd.merge(newdecodetable, dft, right_on='Feature', left_on='Feature', how='left')    \n",
    "#     dft['site'] = f.split('_')[2]\n",
    "#     dft['year'] = f.split('_')[3]\n",
    "#     dft['siteyear'] = f.split('_')[2]+f.split('_')[3]\n",
    "#     return dft\n",
    "\n",
    "# dfcollect = list()\n",
    "# newfeaturecode = pd.DataFrame(curves_new_mean['featuredecode'].unique())\n",
    "# newfeaturecode.columns = ['featuredecode']\n",
    "# newdecodetable = pd.merge(newfeaturecode, decodetable, right_on='featuredecode', left_on='featuredecode', how='left')\n",
    "\n",
    "# pool = multiprocessing.Pool(processes = 16)\n",
    "# dfcollect = pool.map(worker, files)\n",
    "# dfcollect = pd.concat(dfcollect)\n",
    "# if ckd_group != 0:\n",
    "#     dfcollect = dfcollect[dfcollect['ckd_group'] == ckd_group]\n",
    "# dfcollect.to_parquet('metadata_raw_'+str(ckd_group)+'.parquet', compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38428b94-4c9a-44d5-addd-df9c6d11132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfcollect = pd.read_parquet('metadata_raw.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ed051-d297-4f81-b993-ade80ec6b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfcollect[dfcollect['Feature']=='LAB::2075-0(OT)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b1f4f3-3a57-42ea-9a8f-a390096f2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check Curve Domain\n",
    "# domain_check = curves_new_mean.loc[curves_new_mean['isCategorical']==False].loc[:,['site', 'year', 'featuredecode', 'fval']].groupby(['site', 'year', 'featuredecode']).agg(['min','max']).reset_index()\n",
    "# domain_check.columns = ['site', 'year', 'featuredecode', 'fvalmin', 'fvalmax']\n",
    "# domain_check = domain_check.astype({'year':'str'}).assign(siteyear=lambda x: x.site + x.year)\n",
    "# domain_checkX = domain_check.loc[domain_check['featuredecode']=='2823-3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ba8a7-1752-4e78-9734-9fadab2d89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(domain_checkX['siteyear'], domain_checkX['fvalmin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8c3a7-04b4-4c05-9336-c3161b0284b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(domain_checkX['siteyear'], domain_checkX['fvalmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48314ff5-d907-4606-a5fb-0494e2fbc685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef6af7ef-cb91-4ca0-b84c-a682317a7cce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# chk1=pd.read_pickle('/home/hchan2/AKI/AKI_Python/data/KUMC/shapdataraw_catd_KUMC_3000_stg01_rmscrbun_raw_005.pkl')\n",
    "# chk2=pd.read_pickle('/home/hchan2/AKI/AKI_Python/data/KUMC/shapdata_catd_KUMC_3000_stg01_rmscrbun_raw_005.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d3618-61e3-4b0a-a207-dc34023923a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chk2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837aa89-9552-4821-b720-e9c3a8f7bb1a",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
