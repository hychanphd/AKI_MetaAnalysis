{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9387011d-0244-4d22-a83d-cbafdd62d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a6e0e-29b7-4654-a45b-bfb02e3c7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_too_much_nan(site, year, newdfs, threshold, keep_med=True):\n",
    "    print('Remove sparse feature on site '+site+\":\"+str(year), flush = True)                        \n",
    "    allcols = []\n",
    "    for newdf in newdfs:\n",
    "        allcols = allcols + list(newdf.columns)\n",
    "    allcols = np.unique(np.array(allcols))\n",
    "    allcols = allcols[allcols != 'FLAG']\n",
    "    allcols = allcols[allcols != 'PATID']\n",
    "    allcols = allcols[allcols != 'ENCOUNTERID']\n",
    "\n",
    "    flag0nan = {key: 0 for key in allcols}\n",
    "    flag1nan = {key: 0 for key in allcols}\n",
    "    flag0total = 0\n",
    "    flag1total = 0\n",
    "\n",
    "    for newdf in newdfs:\n",
    "        btX = newdf.replace(False, np.nan)\n",
    "        flag0total += np.logical_not(btX['FLAG']).sum()\n",
    "        flag1total += btX['FLAG'].sum()    \n",
    "        for col in allcols:\n",
    "            if col in newdf.columns:\n",
    "                flag0nan[col] += np.logical_and(np.logical_not(btX['FLAG']), np.isnan(btX[col])).sum()\n",
    "                flag1nan[col] += np.logical_and(btX['FLAG'], np.isnan(btX[col])).sum()\n",
    "            else:\n",
    "                flag0nan[col] += np.logical_not(btX['FLAG']).sum()\n",
    "                flag1nan[col] += btX['FLAG'].sum()\n",
    "                \n",
    "    remlist = []        \n",
    "    for col in allcols:\n",
    "#        print(col, flag0nan[col]/flag0total, flag1nan[col]/flag1total)        \n",
    "        if flag0nan[col]/flag0total >= 1-threshold and flag1nan[col]/flag1total >= 1-threshold:\n",
    "            remlist = remlist + [col]\n",
    "\n",
    "    if keep_med:\n",
    "        remlist = [x for x in remlist if 'MED' not in x]\n",
    "            \n",
    "    for i in range(len(newdfs)):\n",
    "        newdfs[i] = newdfs[i].drop(remlist,axis=1, errors='ignore')\n",
    "\n",
    "    return newdfs, remlist, flag0nan, flag1nan, flag0total, flag1total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff4ddb-638f-4da1-9026-d3ac5185a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_too_much_nan_positive(site, year, newdfs, threshold, keep_med=True):\n",
    "    print('Remove sparse feature on site '+site+\":\"+str(year), flush = True)                        \n",
    "    allcols = []\n",
    "    for newdf in newdfs:\n",
    "        allcols = allcols + list(newdf.columns)\n",
    "    allcols = np.unique(np.array(allcols))\n",
    "    allcols = allcols[allcols != 'FLAG']\n",
    "    allcols = allcols[allcols != 'PATID']\n",
    "    allcols = allcols[allcols != 'ENCOUNTERID']\n",
    "\n",
    "    flag0nan = {key: 0 for key in allcols}\n",
    "    flag1nan = {key: 0 for key in allcols}\n",
    "    flag0total = 0\n",
    "    flag1total = 0\n",
    "\n",
    "    for newdf in newdfs:\n",
    "        btX = newdf.replace(False, np.nan)\n",
    "        flag0total += np.logical_not(btX['FLAG']).sum()\n",
    "        flag1total += btX['FLAG'].sum()    \n",
    "        for col in allcols:\n",
    "            if col in newdf.columns:\n",
    "#                flag0nan[col] += np.logical_and(np.logical_not(btX['FLAG']), np.isnan(btX[col])).sum()\n",
    "                flag1nan[col] += np.logical_and(btX['FLAG'], np.isnan(btX[col])).sum()\n",
    "            else:\n",
    "#                flag0nan[col] += np.logical_not(btX['FLAG']).sum()\n",
    "                flag1nan[col] += btX['FLAG'].sum()\n",
    "                \n",
    "    remlist = []        \n",
    "    for col in allcols:\n",
    "#        print(col, flag0nan[col]/flag0total, flag1nan[col]/flag1total)        \n",
    "#        if flag0nan[col]/flag0total >= 1-threshold and flag1nan[col]/flag1total >= 1-threshold:\n",
    "        if flag1nan[col]/flag1total >= 1-threshold:\n",
    "            remlist = remlist + [col]\n",
    "\n",
    "    if keep_med:\n",
    "        remlist = [x for x in remlist if 'MED' not in x]\n",
    "            \n",
    "    for i in range(len(newdfs)):\n",
    "        newdfs[i] = newdfs[i].drop(remlist,axis=1, errors='ignore')\n",
    "\n",
    "    return newdfs, remlist, flag0nan, flag1nan, flag0total, flag1total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d88df9-b3f1-4b8f-a698-ce1f15825d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_ckd(site, year, newdf):                                    \n",
    "    #lab_num\n",
    "    print('Merging ckd_info on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "        efgr2 = pd.read_pickle('data/'+site+'/p0_'+'ckdgroup'+'_'+site+'.pkl')\n",
    "        return pd.merge(newdf, efgr2, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No efgr table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No efgr table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963d82c-43de-4537-b073-e21a1db78ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_postprocess(site, year, newdf):\n",
    "    print('Finishing on site '+site+\":\"+str(year), flush = True)                    \n",
    "    newdf = newdf.drop(['PATID', 'ENCOUNTERID', 'AKI1_SINCE_ADMIT', 'SINCE_ADMIT', 'DAYS_SINCE_ADMIT','DAYS_SINCE_ADMIT_x'],axis=1, errors='ignore')\n",
    "    newdf.columns=newdf.columns.str.replace('<','st')\n",
    "    newdf.columns=newdf.columns.str.replace('>','bt')\n",
    "    newdf.columns=newdf.columns.str.replace('[','lb')\n",
    "    newdf.columns=newdf.columns.str.replace(']','rb')   \n",
    "    return newdf.dropna(axis=1, how='all')\n",
    "\n",
    "#    newdf_debug['drop'] = newdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dead02-3f31-431e-954c-e95dc05d946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_convert(dataX, stg):\n",
    "    data = dataX.copy()\n",
    "    \n",
    "    if stg == 'stg23':\n",
    "        data = data[data['FLAG']!=1]\n",
    "        data['FLAG'] = (data['FLAG']>1)*1\n",
    "        return data\n",
    "    \n",
    "    if stg == 'stg010':\n",
    "        data = data[data['FLAG']!=2]\n",
    "        data = data[data['FLAG']!=3]\n",
    "        return data\n",
    "    \n",
    "    if stg == 'stg123':\n",
    "        data = data[data['FLAG']!=0]\n",
    "        \n",
    "    if stg == 'stg01':\n",
    "        data['FLAG'] = (data['FLAG']>0)*1\n",
    "    else:\n",
    "        data['FLAG'] = (data['FLAG']>1)*1    \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f3a61-03d0-416b-bfd7-c9cfb207cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinebt(site, yearX, stg, threshold=0.01):\n",
    "    \n",
    "    onset = pd.read_pickle('data/'+site+'/p0_onset_'+site+'.pkl')\n",
    "    years = list(pd.to_datetime(onset['ADMIT_DATE']).dt.year.unique())    \n",
    "    bt_list = list()\n",
    "\n",
    "    for year in years:\n",
    "        try:\n",
    "            data = pd.read_pickle('data/'+site+'/bt3_'+site+'_'+str(year)+'.pkl')\n",
    "            data = flag_convert(data, stg)\n",
    "            bt_list.append(data.copy())\n",
    "        except:\n",
    "            print(str(year)+' not exists')\n",
    "\n",
    "            \n",
    "    bt_list, remlist, flag0nan, flag1nan, flag0total, flag1total = drop_too_much_nan(site, yearX, bt_list, threshold)\n",
    "#    return bt_list, remlist, flag0nan, flag1nan, flag0total, flag1total\n",
    "    bt_all = pd.concat(bt_list, ignore_index=True)\n",
    "    # replace nan in boolean columns with False\n",
    "    bt_bool = bt_all.select_dtypes('O').columns\n",
    "    bt_all[bt_bool] = bt_all[bt_bool].fillna(False)\n",
    "\n",
    "    bt_all = bt_ckd(site, yearX, bt_all)\n",
    "    bt_all = bt_postprocess(site, yearX, bt_all)\n",
    "    bt_all.to_pickle('data/'+site+'/bt3_'+site+'_'+stg+'_3000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a257834-e4ed-4464-b9c2-14097b47e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinebtpos(site, yearX, stg, threshold=0.01):\n",
    "    \n",
    "    onset = pd.read_pickle('data/'+site+'/p0_onset_'+site+'.pkl')\n",
    "    years = list(pd.to_datetime(onset['ADMIT_DATE']).dt.year.unique())    \n",
    "    bt_list = list()\n",
    "\n",
    "    for year in years:\n",
    "        try:\n",
    "            data = pd.read_pickle('data/'+site+'/bt3_'+site+'_'+str(year)+'.pkl')\n",
    "            data = flag_convert(data, stg)\n",
    "            bt_list.append(data.copy())\n",
    "        except:\n",
    "            print(str(year)+' not exists')\n",
    "            \n",
    "#    bt_list, remlist, flag0nan, flag1nan, flag0total, flag1total = drop_too_much_nan(site, yearX, bt_list, threshold)\n",
    "    bt_list, remlist, flag0nan, flag1nan, flag0total, flag1total = drop_too_much_nan_positive(site, yearX, bt_list, threshold)\n",
    "#    return bt_list, remlist, flag0nan, flag1nan, flag0total, flag1total\n",
    "    bt_all = pd.concat(bt_list, ignore_index=True)\n",
    "    # replace nan in boolean columns with False\n",
    "    bt_bool = bt_all.select_dtypes('O').columns\n",
    "    bt_all[bt_bool] = bt_all[bt_bool].fillna(False)\n",
    "\n",
    "    bt_all = bt_ckd(site, yearX, bt_all)\n",
    "    bt_all = bt_postprocess(site, yearX, bt_all)\n",
    "    bt_all.to_pickle('data/'+site+'/bt3pos_'+site+'_'+stg+'_3000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400cd88f-f6c8-4f3e-914d-2ba53d8c3cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def calculate_sparse(sites):\n",
    "if __name__ == \"__main__\":\n",
    "    sites = ['MCRI', 'MCW', 'UIOWA', 'UMHC', 'UNMC', 'UofU', 'UPITT', 'UTHSCSA', 'KUMC', 'UTSW']\n",
    "    sparsity = dict()\n",
    "    cols = dict()\n",
    "    shape_list = dict()\n",
    "    for site in sites:\n",
    "        print('Processing '+site)\n",
    "        newdf = pd.read_pickle('data/'+site+'/bt3_'+site+'_3000.pkl')\n",
    "        btX = newdf.replace(False, np.nan) \n",
    "        btX_objcol = btX.select_dtypes('O')\n",
    "#        bt_list.append(data.copy())\n",
    "        cols[site] = btX.columns\n",
    "        sp_site = dict()\n",
    "        for col in cols[site]:\n",
    "            if btX[col].dtype != 'O':\n",
    "#                sp_site[col] = (np.isnan(btX[col])).sum()/btX.shape[0]\n",
    "                sp_site[col] = np.logical_and(btX['FLAG']==1,np.isnan(btX[col])).sum()/(btX['FLAG']==1).sum()\n",
    "        sparsity[site] = sp_site\n",
    "        shape_list[site] = btX.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4056fa18-7eb2-413e-8363-4d8b2a9ca625",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    allcols = np.unique([y for x in list(cols.values()) for y in list(x)])\n",
    "    var_list = dict()\n",
    "    for col in allcols:\n",
    "        sp_col = list()\n",
    "        for sp in sparsity.values():\n",
    "            if col in sp:\n",
    "                sp_col = sp_col + [sp[col]]\n",
    "            else:\n",
    "                sp_col = sp_col + [1]\n",
    "        var_list[col] = np.var(sp_col)\n",
    "    var_list = pd.DataFrame(var_list, index=[0]).T.sort_values(by=0)\n",
    "    var_list['rank'] = var_list.rank()\n",
    "    var_list = var_list.reset_index()\n",
    "    var_list.to_pickle('spdf1.pkl')        \n",
    "#    return var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad647657-a9f5-49eb-b9cf-f978956d72bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sites = ['MCRI', 'MCW', 'UIOWA', 'UMHC', 'UNMC', 'UofU', 'UPITT', 'UTHSCSA', 'KUMC', 'UTSW']\n",
    "    var_list = calculate_sparse(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703d1eb-9b94-4392-ae9b-dda00bf503d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
