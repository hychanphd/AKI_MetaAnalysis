{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936897e1-0e30-4bc7-a43a-32b8e1870b85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:37:41.318338Z",
     "start_time": "2023-12-03T23:37:39.324512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import python packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9c938-2c74-485c-893d-e62b42fb7a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_file_path(pdatpath, site):\n",
    "    input_path = os.path.join(pdatpath, site) + '/'\n",
    "    output_path = os.path.join(pdatpath, site, 'aug24') + '/'\n",
    "    aux_path = os.path.join(pdatpath, 'aux_files') + '/'\n",
    "    # Check if the output path exists, if not, create it\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    return [input_path, output_path, aux_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696b011-3605-4f9a-bf0b-7b05846a273f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_onset_data(filepath_lst):\n",
    "    xxx = pd.read_pickle(filepath_lst[0] + 'AKI_LAB_SCR'+'.pkl')\n",
    "    yyy = pd.read_pickle(filepath_lst[0] + 'AKI_ONSETS'+'.pkl') \n",
    "    yyy = yyy[['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'DISCHARGE_DATE']]\n",
    "    xxx = xxx[['ENCOUNTERID', 'PATID', 'SPECIMEN_DATE',  'RESULT_NUM']] \n",
    "    xxx = xxx.merge(yyy, on = ['ENCOUNTERID', 'PATID'], how='left')\n",
    "    xxx = xxx.dropna()\n",
    "    xxx['DAYS_SINCE_ADMIT'] = (xxx['SPECIMEN_DATE']-xxx['ADMIT_DATE']).dt.days\n",
    "    # take daily average\n",
    "    xxx = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'DAYS_SINCE_ADMIT', 'RESULT_NUM', 'ADMIT_DATE']].groupby(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'DAYS_SINCE_ADMIT', 'ADMIT_DATE']).mean()\n",
    "    xxx = xxx.sort_values(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'])\n",
    "    xxx = xxx.reset_index()\n",
    "    return xxx, yyy.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964fffc-3979-4c26-bb02-97f18e985d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scr_baseline_new(df_scr, df_admit, filepath_lst,  aggfunc_7d = 'last', aggfunc_1y = 'mean', keep_ckd = False):\n",
    "    cohort_table = dict()\n",
    "    \n",
    "    # load & process dx data\n",
    "    dx = pd.read_pickle(filepath_lst[0]+'AKI_DX.pkl') \n",
    "    \n",
    "    existing_columns = [col for col in ['PATID', 'ENCOUNTERID', 'DX', 'DX_DATE', 'DX_TYPE', 'DAYS_SINCE_ADMIT']\n",
    "                        if col in dx.columns]\n",
    "    dx = dx[existing_columns]\n",
    "    dx = df_admit[['PATID', 'ENCOUNTERID', 'ADMIT_DATE']].merge(dx, on = ['PATID', 'ENCOUNTERID'], how = 'inner')\n",
    "\n",
    "    if 'DAYS_SINCE_ADMIT' not in dx.columns:\n",
    "        dx['DAYS_SINCE_ADMIT'] = (dx['DX_DATE']-dx['ADMIT_DATE']).dt.days\n",
    "        \n",
    "\n",
    "    dx['DX'] = dx['DX'].astype(str)\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].astype(str)\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].replace('09', '9')\n",
    "    \n",
    "    # load & process demo data\n",
    "    demo = pd.read_pickle(filepath_lst[0]+'AKI_DEMO'+'.pkl')  \n",
    "    demo['MALE'] = demo['SEX'] == 'M'\n",
    "\n",
    "    demo['RACE_WHITE'] = demo['RACE'] == '05'\n",
    "    demo['RACE_BLACK'] = demo['RACE'] == '03'\n",
    "    demo = demo[['PATID', 'ENCOUNTERID', 'AGE', 'MALE', 'RACE_WHITE', 'RACE_BLACK']]\n",
    "    demo = demo.drop_duplicates()\n",
    "    \n",
    "    # estimate SCr Baseline\n",
    "    pat_id_cols = ['PATID', 'ENCOUNTERID']\n",
    "    complete_df = df_scr[['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    " \n",
    "    # 1. min between the min of 1-week prior admission SCr and within 24 hour after admission SCr\n",
    "    # SCr within 24 hour after admission, that is admission day and one day after, get mean\n",
    "    admission_SCr = complete_df[(complete_df.SPECIMEN_DATE >= complete_df.ADMIT_DATE) & \\\n",
    "                                (complete_df.SPECIMEN_DATE <= (complete_df.ADMIT_DATE + pd.Timedelta(days=1)))].copy()\n",
    "\n",
    "    # Admission SCr is the mean of all the SCr within 24h admission\n",
    "    admission_SCr = admission_SCr.groupby(pat_id_cols)['RESULT_NUM'].mean().reset_index()\n",
    "\n",
    "    admission_SCr.rename(columns = {'RESULT_NUM': 'ADMISSION_SCR'}, inplace = True)\n",
    "\n",
    "    # merge the ADMISSION_SCR back to the main frame\n",
    "    complete_df = complete_df.merge(admission_SCr, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    # SCr within 7 days prior to admission\n",
    "    one_week_prior_admission = complete_df[(complete_df.SPECIMEN_DATE >= complete_df.ADMIT_DATE - pd.Timedelta(days=7)) & \\\n",
    "                                           (complete_df.SPECIMEN_DATE < complete_df.ADMIT_DATE)].copy()\n",
    "    one_week_prior_admission = one_week_prior_admission.sort_values(by = ['PATID', 'ENCOUNTERID','SPECIMEN_DATE'])\n",
    "    \n",
    "    # ! record the scr times \n",
    "    scr_1w_time = one_week_prior_admission.copy() #.groupby(pat_id_cols).last().reset_index()\n",
    "    scr_1w_time['days_before_admit']  = (scr_1w_time['ADMIT_DATE']  -  scr_1w_time['SPECIMEN_DATE']).dt.days\n",
    "    cohort_table['scr_1w_df'] = scr_1w_time\n",
    "    \n",
    "    one_week_prior_admission = one_week_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].agg(aggfunc_7d).reset_index()\n",
    "\n",
    "        \n",
    "    one_week_prior_admission = one_week_prior_admission.rename(columns = {'RESULT_NUM': 'ONE_WEEK_SCR'})\n",
    "\n",
    "    complete_df = complete_df.merge(one_week_prior_admission, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    # take the min between one week SCr and admission SCr\n",
    "    complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), 'BASELINE_EST_1'] = \\\n",
    "                np.nanmin(complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), ['ONE_WEEK_SCR','ADMISSION_SCR']], axis = 1)\n",
    "\n",
    "    complete_dfe = complete_df.drop(['SPECIMEN_DATE', 'RESULT_NUM'],axis=1).drop_duplicates()\n",
    "    \n",
    "    \n",
    "    cohort_table['ALL_ENCOUNTERS'] = len(complete_dfe[['PATID','ENCOUNTERID']].drop_duplicates())\n",
    "    cohort_table['ALL_PATIENTS'] = complete_dfe.PATID.nunique()\n",
    "    cohort_table['ADMISSION_SCR_YES'] = complete_dfe.ADMISSION_SCR.notna().sum()\n",
    "    cohort_table['ADMISSION_SCR_NO'] = complete_dfe.ADMISSION_SCR.isna().sum()\n",
    "    cohort_table['ONE_WEEK_SCR_YES'] = complete_dfe.ONE_WEEK_SCR.notna().sum()\n",
    "    cohort_table['ONE_WEEK_SCR_NO'] = complete_dfe.ONE_WEEK_SCR.isna().sum()    \n",
    "    cohort_table['ADMISSION_AND_1W_SCR'] = (complete_dfe.ADMISSION_SCR.notna() & complete_dfe.ONE_WEEK_SCR.notna()).sum()\n",
    "    cohort_table['ADMISSION_AND_1W_SCR_MIN'] = (complete_dfe.BASELINE_EST_1.notna()).sum()\n",
    "    cohort_table['ADMISSION_OR_1W_SCR'] = (complete_dfe.ADMISSION_SCR.notna() | complete_dfe.ONE_WEEK_SCR.notna()).sum()\n",
    "    cohort_table['ONE_WEEK_SCR_ENC'] = complete_dfe[(complete_dfe.ONE_WEEK_SCR.notna() & (complete_dfe['ONE_WEEK_SCR']==complete_dfe['BASELINE_EST_1']))]['ENCOUNTERID'].unique()\n",
    "    cohort_table['ADMISSION_SCR_1W_ENC'] = complete_dfe[(complete_dfe.ONE_WEEK_SCR.notna() & (complete_dfe['ONE_WEEK_SCR']!=complete_dfe['BASELINE_EST_1']))]['ENCOUNTERID'].unique()\n",
    "        \n",
    "    ori_num_unique_combinations = df_scr.groupby(['PATID', 'ENCOUNTERID']).ngroups\n",
    "    # get the percentage of encounters that do not have past 7-day records\n",
    "    criterion1_no_missing = complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), :].groupby(pat_id_cols).ngroups\n",
    "    criterion1_missing_rate = 1 - criterion1_no_missing / ori_num_unique_combinations\n",
    "\n",
    "    # 2. pre-admission 365-7 day mean\n",
    "    # here we only care about SCr measurements within 1 year before hospitalization\n",
    "    one_year_prior_admission = complete_df[(complete_df.SPECIMEN_DATE < (complete_df.ADMIT_DATE - pd.Timedelta(days=7))) & \\\n",
    "                                     (complete_df.SPECIMEN_DATE >= (complete_df.ADMIT_DATE - pd.Timedelta(days=365.25)))].copy()\n",
    "    one_year_prior_admission = one_year_prior_admission.sort_values(by = ['PATID', 'ENCOUNTERID','SPECIMEN_DATE'])\n",
    "    \n",
    "    # ! record the scr times\n",
    "    scr_1y_time = one_year_prior_admission[one_year_prior_admission.ENCOUNTERID.isin(complete_dfe[complete_dfe.ONE_WEEK_SCR.isna()].ENCOUNTERID.unique())] #.groupby(pat_id_cols).last().reset_index()\n",
    "    scr_1y_time['days_before_admit']  = (scr_1y_time['ADMIT_DATE']  -  scr_1y_time['SPECIMEN_DATE']).dt.days\n",
    "    cohort_table['scr_1y_df'] = scr_1y_time\n",
    "    \n",
    "    \n",
    "    one_year_prior_admission = one_year_prior_admission.loc[:, pat_id_cols + ['RESULT_NUM']]\n",
    "    \n",
    "\n",
    "    one_year_prior_admission = one_year_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].agg(aggfunc_1y).reset_index()\n",
    "\n",
    "    \n",
    "    one_year_prior_admission.rename(columns = {'RESULT_NUM': 'ONE_YEAR_SCR'}, inplace = True)\n",
    "    \n",
    "    complete_df = complete_df.merge(one_year_prior_admission, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "    \n",
    "    # take the min between one week SCr and admission SCr\n",
    "    complete_df.loc[complete_df.ONE_YEAR_SCR.notna(), 'BASELINE_EST_2'] = \\\n",
    "                np.nanmin(complete_df.loc[complete_df.ONE_YEAR_SCR.notna(), ['ONE_YEAR_SCR', 'ADMISSION_SCR']], axis = 1)\n",
    "\n",
    "    # priority 1: 7day SCr, priority 2: one year SCr\n",
    "    complete_df['BASELINE_NO_INVERT'] = \\\n",
    "                np.where(complete_df['BASELINE_EST_1'].isna(), complete_df['BASELINE_EST_2'], complete_df['BASELINE_EST_1'])\n",
    "\n",
    "    complete_dfe = complete_df.drop(['SPECIMEN_DATE', 'RESULT_NUM'],axis=1).drop_duplicates()\n",
    "    cohort_table['ONE_YEAR_SCR_YES'] = (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna()).sum()\n",
    "    cohort_table['ONE_YEAR_SCR_NO'] = (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.isna()).sum()\n",
    "    \n",
    "    cohort_table['ADMISSION_AND_1Y_SCR'] = (complete_dfe.ADMISSION_SCR.notna() & (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna())).sum()\n",
    "    cohort_table['ADMISSION_AND_1Y_SCR_MIN'] = (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.BASELINE_EST_2.notna()).sum()\n",
    "    \n",
    "    cohort_table['ADMISSION_OR_1Y_SCR'] = (complete_dfe.ADMISSION_SCR.notna() | (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna())).sum()\n",
    "    \n",
    "    cohort_table['ONE_YEAR_SCR_ENC'] = complete_dfe[(complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna() & (complete_dfe['ONE_YEAR_SCR']==complete_dfe['BASELINE_EST_2']))]['ENCOUNTERID'].unique()\n",
    "    cohort_table['ADMISSION_SCR_1Y_ENC'] = complete_dfe[(complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna() & (complete_dfe['ONE_YEAR_SCR']!=complete_dfe['BASELINE_EST_2']))]['ENCOUNTERID'].unique()\n",
    "    \n",
    "    # 3. Invert MDRD equation to estimate baseline (only for non-CKD patients)\n",
    "    # get those encounters for which we need to impute baseline\n",
    "    pat_to_invert = complete_df.loc[complete_df.BASELINE_NO_INVERT.isna(), pat_id_cols+['ADMIT_DATE', 'ADMISSION_SCR']]\n",
    "    \n",
    "    cohort_table['MDRD_TO_INVERT'] = pat_to_invert['ENCOUNTERID'].nunique()\n",
    "    cohort_table['MDRD_TO_INVERT_ENC'] = pat_to_invert['ENCOUNTERID'].unique()\n",
    "    # one patient one row\n",
    "    pat_to_invert.drop_duplicates(subset=pat_id_cols, keep='first', inplace = True)\n",
    "\n",
    "\n",
    "    pat_dx = pat_to_invert.merge(dx.drop(['ENCOUNTERID', 'ADMIT_DATE'], axis = 1), \n",
    "                                              on = 'PATID', \n",
    "                                              how = 'left')\n",
    "\n",
    "    # calculate DX_DATE when it is missing\n",
    "    pat_dx.loc[pat_dx.DX_DATE.isna(), 'DX_DATE'] = \\\n",
    "            pat_dx.loc[pat_dx.DX_DATE.isna(), 'ADMIT_DATE'] + \\\n",
    "            pd.to_timedelta(pat_dx.loc[pat_dx.DX_DATE.isna(), 'DAYS_SINCE_ADMIT'], unit='D')\n",
    "\n",
    "#     # check patients that do not have DX in the database\n",
    "#     print(pat_dx.DX_DATE.isna().mean())\n",
    "\n",
    "    # filter out those DX after admission\n",
    "    pat_dx = pat_dx[pat_dx.DX_DATE <= pat_dx.ADMIT_DATE]   #pat_dx.DAYS_SINCE_ADMIT <= 0\n",
    "    pat_dx = pat_dx.merge(pat_to_invert[['PATID', 'ENCOUNTERID']], \n",
    "                          on = ['PATID', 'ENCOUNTERID'], \n",
    "                          how = 'outer')\n",
    "    \n",
    "    \n",
    "    # get the default eGFR for inversion: default to 75 for non-CKD patients, average of eGFR from staging criteria for CKD patients\n",
    "    pat_dx['DFLT_eGFR'] = 75\n",
    "\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.3', 'N18.3']), 'DFLT_eGFR'] = 90/2\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.4', 'N18.4']), 'DFLT_eGFR'] = 45/2\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.5', 'N18.5']), 'DFLT_eGFR'] = 15/2\n",
    "\n",
    "    pat_def_egfr = pat_dx.groupby(pat_id_cols)['DFLT_eGFR'].min().reset_index()\n",
    "    \n",
    "    \n",
    "    cohort_table['ALL_CKD3_ENC'] = dx[(dx['DX'].isin(['585.3', 'N18.3']) )  & (dx['ENCOUNTERID'].isin(df_admit['ENCOUNTERID'].unique()))]['ENCOUNTERID'].unique()\n",
    "    cohort_table['ALL_CKD4_ENC'] = dx[(dx['DX'].isin(['585.4', 'N18.4']) )  & (dx['ENCOUNTERID'].isin(df_admit['ENCOUNTERID'].unique()))]['ENCOUNTERID'].unique()\n",
    "    cohort_table['ALL_CKD5_ENC'] = dx[(dx['DX'].isin(['585.5', 'N18.5']) )  & (dx['ENCOUNTERID'].isin(df_admit['ENCOUNTERID'].unique()))]['ENCOUNTERID'].unique()\n",
    "\n",
    "    \n",
    "    cohort_table['MDRD_NOCKD'] = (pat_def_egfr['DFLT_eGFR'] == 75).sum()\n",
    "    \n",
    "    cohort_table['ADMISSION_OR_MDRD_NOCKD'] = (complete_dfe.ADMISSION_SCR.notna() | complete_dfe['ENCOUNTERID'].isin(pat_def_egfr[pat_def_egfr['DFLT_eGFR'] == 75]['ENCOUNTERID'].unique())).sum()\n",
    "    \n",
    "    cohort_table['MDRD_CKD3']  = (pat_def_egfr['DFLT_eGFR'] == 90/2).sum()\n",
    "    cohort_table['MDRD_CKD4']  = (pat_def_egfr['DFLT_eGFR'] == 45/2).sum()\n",
    "    cohort_table['MDRD_CKD5']  = (pat_def_egfr['DFLT_eGFR'] == 15/2).sum()\n",
    "        \n",
    "    pat_to_invert= pat_to_invert.merge(pat_def_egfr, on = pat_id_cols, how = 'left')\n",
    "    pat_to_invert['DFLT_eGFR'] = pat_to_invert['DFLT_eGFR'].fillna(75)\n",
    "\n",
    "    pat_to_invert['CKD345'] = pat_to_invert['DFLT_eGFR'] != 75\n",
    "    \n",
    "    #pat_to_invert.DFLT_eGFR.value_counts()\n",
    "\n",
    "    # Backcalculation for patients\n",
    "    # merge DEMO with pat_to_invert\n",
    "    pat_to_invert = pat_to_invert.merge(demo, on = pat_id_cols, how = 'left')\n",
    "    \n",
    "    KDIGO_baseline = np.array([\n",
    "        [1.5, 1.3, 1.2, 1.0],\n",
    "        [1.5, 1.2, 1.1, 1.0],\n",
    "        [1.4, 1.2, 1.1, 0.9],\n",
    "        [1.3, 1.1, 1.0, 0.9],\n",
    "        [1.3, 1.1, 1.0, 0.8],\n",
    "        [1.2, 1.0, 0.9, 0.8]\n",
    "    ])\n",
    "    KDIGO_baseline = pd.DataFrame(KDIGO_baseline, columns = [\"Black males\", \"Other males\",\n",
    "                                                            \"Black females\", \"Other females\"],\n",
    "                                 index = [\"20-24\", \"25-29\", \"30-39\", \"40-54\", \"55-65\", \">65\"])    \n",
    "    \n",
    "    \n",
    "    # estimate SCr from eGFR\n",
    "    pat_to_invert.loc[~pat_to_invert['CKD345'], 'BASELINE_INVERT'] = pat_to_invert.loc[~pat_to_invert['CKD345'], :].apply(inverse_MDRD, args = (KDIGO_baseline,), axis = 1) \n",
    "    pat_to_invert.loc[pat_to_invert['CKD345'], 'BASELINE_INVERT'] = pat_to_invert.loc[pat_to_invert['CKD345'], :].apply(inverse_MDRD_raw, axis = 1) \n",
    "\n",
    "    # take minimum of inverted SCr and admission SCr\n",
    "    pat_to_invert['BASELINE_EST_3'] = np.min(pat_to_invert[['ADMISSION_SCR', 'BASELINE_INVERT']], axis = 1)\n",
    "\n",
    "    cohort_table['MDRD_ENC'] = pat_to_invert[(~pat_to_invert['CKD345']) & (pat_to_invert['BASELINE_EST_3'] == pat_to_invert['BASELINE_INVERT'])]['ENCOUNTERID'].unique()\n",
    "    cohort_table['ADMISSION_SCR_MDRD_ENC'] = pat_to_invert[(~pat_to_invert['CKD345']) & (pat_to_invert['BASELINE_EST_3'] != pat_to_invert['BASELINE_INVERT'])]['ENCOUNTERID'].unique()\n",
    "        \n",
    "    \n",
    "    # merge back the computation results\n",
    "    complete_df = complete_df.merge(pat_to_invert[pat_id_cols + ['BASELINE_EST_3', 'CKD345']], \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    # replace the old baseline\n",
    "    complete_df['SERUM_CREAT_BASE'] = np.min(complete_df[['BASELINE_NO_INVERT', 'BASELINE_EST_3']], axis = 1)\n",
    "\n",
    "    if not keep_ckd:\n",
    "        complete_df = complete_df[~(complete_df['CKD345'] & complete_df['BASELINE_NO_INVERT'].isna())]\n",
    "\n",
    "    complete_df = complete_df.drop('CKD345', axis=1)\n",
    "        \n",
    "    # drop those still cannot find baseline\n",
    "    complete_df = complete_df.dropna(subset=['SERUM_CREAT_BASE'])\n",
    "\n",
    "    return complete_df.drop_duplicates(), cohort_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34bb3cc-b925-4c1d-b0fd-afb1e40245e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inverse_MDRD_raw(row):\n",
    "    eGFR = row['DFLT_eGFR']  # or 75\n",
    "    male = row['MALE']\n",
    "    black = row['RACE_BLACK']\n",
    "    age = row['AGE']\n",
    "\n",
    "    # Constants specific to the MDRD study\n",
    "    if male:\n",
    "        gender_factor = 1.0\n",
    "    else:\n",
    "        gender_factor = 0.742\n",
    "\n",
    "    if black:\n",
    "        race_factor = 1.212\n",
    "    else:\n",
    "        race_factor = 1.0\n",
    "\n",
    "    # MDRD equation rearranged to solve for Scr\n",
    "    Scr = (eGFR / (175 * (age ** -0.203) * gender_factor * race_factor)) ** (1 / -1.154)\n",
    "    return Scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f59fea-fd63-4f37-aba4-71ef198a00df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KDIGO_baseline = np.array([\n",
    "    [1.5, 1.3, 1.2, 1.0],\n",
    "    [1.5, 1.2, 1.1, 1.0],\n",
    "    [1.4, 1.2, 1.1, 0.9],\n",
    "    [1.3, 1.1, 1.0, 0.9],\n",
    "    [1.3, 1.1, 1.0, 0.8],\n",
    "    [1.2, 1.0, 0.9, 0.8]\n",
    "])\n",
    "KDIGO_baseline = pd.DataFrame(KDIGO_baseline, columns = [\"Black males\", \"Other males\",\n",
    "                                                        \"Black females\", \"Other females\"],\n",
    "                             index = [\"20-24\", \"25-29\", \"30-39\", \"40-54\", \"55-65\", \">65\"])\n",
    "\n",
    "def inverse_MDRD(row, KDIGO_baseline):\n",
    "    age = row[\"AGE\"]\n",
    "    is_male = True if row[\"MALE\"]  else False\n",
    "    is_black = True if row[\"RACE_BLACK\"] else False\n",
    "        \n",
    "    if is_male and is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Black males\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Black males\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Black males\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Black males\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Black males\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Black males\"]\n",
    "    \n",
    "    if is_male and not is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Other males\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Other males\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Other males\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Other males\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Other males\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Other males\"]\n",
    "\n",
    "    if not is_male and is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Black females\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Black females\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Black females\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Black females\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Black females\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Black females\"]\n",
    "    \n",
    "    if not is_male and not is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Other females\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Other females\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Other females\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Other females\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Other females\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Other females\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc6cd7-d054-453a-8cfe-106de613dddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_onset_stats(onsets):\n",
    "    df_stats = []\n",
    "    site_list =  ['KUMC', 'MCW', 'UIOWA', 'UMHC', 'UNMC', 'UTHSCSA', 'UTSW','UofU', 'UPITT', 'MCRI']  #\n",
    "\n",
    "    for site in site_list:\n",
    "        cohort_tbl = onsets[site][2]\n",
    "        onset_df = onsets[site][0]\n",
    "        #adm_scr_encs = list(set(cohort_tbl['ADMISSION_SCR_1W_ENC']) | set(cohort_tbl['ADMISSION_SCR_1Y_ENC']) | set(cohort_tbl['ADMISSION_SCR_MDRD_ENC']))\n",
    "\n",
    "        stats = {\n",
    "             # Cohort tbl\n",
    "            'c' : cohort_tbl['ALL_ENCOUNTERS'] == (cohort_tbl['ONE_WEEK_SCR_YES'] + \n",
    "                                                   cohort_tbl['ONE_YEAR_SCR_YES'] + \n",
    "                                                   cohort_tbl['MDRD_NOCKD'] + \n",
    "                                                   cohort_tbl['MDRD_CKD3'] + cohort_tbl['MDRD_CKD4'] + cohort_tbl['MDRD_CKD5']),\n",
    "            'all_enc_count': cohort_tbl['ALL_ENCOUNTERS'],\n",
    "            'all_pat_count': cohort_tbl['ALL_PATIENTS'],\n",
    "            'scr_1w': cohort_tbl['ONE_WEEK_SCR_YES'],\n",
    "            'scr_1w_no': cohort_tbl['ONE_WEEK_SCR_NO'],\n",
    "            'scr_1y': cohort_tbl['ONE_YEAR_SCR_YES'],\n",
    "            'scr_1y_no': cohort_tbl['ONE_YEAR_SCR_NO'],\n",
    "            'mdrd_nockd': cohort_tbl['MDRD_NOCKD'],\n",
    "            'ckd345': cohort_tbl['MDRD_CKD3'] + cohort_tbl['MDRD_CKD4'] + cohort_tbl['MDRD_CKD5'],\n",
    "             # Onset tbl\n",
    "            'onset_enc_count': len(onset_df),\n",
    "            'onset_pat_count': onset_df['PATID'].nunique(),\n",
    "            'aki1_enc_count': len(onset_df[onset_df['AKI_STAGE'] == 1]),\n",
    "            'aki2_enc_count': len(onset_df[onset_df['AKI_STAGE'] == 2]),\n",
    "            'aki3_enc_count': len(onset_df[onset_df['AKI_STAGE'] == 3]),\n",
    "            #'aki1c': \n",
    "            # 'aki1_7denc_count': len(onset_df[(onset_df['AKI_STAGE'] == 1) & (onset_df['AKI1_7D'])]),\n",
    "            # 'aki2_7denc_count': len(onset_df[(onset_df['AKI_STAGE'] == 2) & (onset_df['AKI1_7D'])]),\n",
    "            # 'aki3_7denc_count': len(onset_df[(onset_df['AKI_STAGE'] == 3) & (onset_df['AKI1_7D'])]),\n",
    "            # 'aki1_scr1w_count': len(onset_df[(onset_df['AKI_STAGE'] == 1)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(cohort_tbl['ONE_WEEK_SCR_ENC']))]),\n",
    "            # 'aki2_scr1w_count': len(onset_df[(onset_df['AKI_STAGE'] == 2)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(cohort_tbl['ONE_WEEK_SCR_ENC']))]),\n",
    "            # 'aki3_scr1w_count': len(onset_df[(onset_df['AKI_STAGE'] == 3)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(cohort_tbl['ONE_WEEK_SCR_ENC']))]),\n",
    "            # 'aki1_scr1y_count': len(onset_df[(onset_df['AKI_STAGE'] == 1)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(cohort_tbl['ONE_YEAR_SCR_ENC']))]),\n",
    "            # 'aki2_scr1y_count': len(onset_df[(onset_df['AKI_STAGE'] == 2)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(cohort_tbl['ONE_YEAR_SCR_ENC']))]),\n",
    "            # 'aki3_scr1y_count': len(onset_df[(onset_df['AKI_STAGE'] == 3)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(cohort_tbl['ONE_YEAR_SCR_ENC']))]),\n",
    "            # 'aki1_mdrd_count': len(onset_df[(onset_df['AKI_STAGE'] == 1)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(cohort_tbl['MDRD_ENC']))]),\n",
    "            # 'aki2_mdrd_count': len(onset_df[(onset_df['AKI_STAGE'] == 2)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(cohort_tbl['MDRD_ENC']))]),\n",
    "            # 'aki3_mdrd_count': len(onset_df[(onset_df['AKI_STAGE'] == 3)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(cohort_tbl['MDRD_ENC']))]),\n",
    "            # 'aki1_adm_count': len(onset_df[(onset_df['AKI_STAGE'] == 1)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(adm_scr_encs))]),\n",
    "            # 'aki2_adm_count': len(onset_df[(onset_df['AKI_STAGE'] == 2)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(adm_scr_encs))]),\n",
    "            # 'aki3_adm_count': len(onset_df[(onset_df['AKI_STAGE'] == 3)& (onset_df['AKI1_7D']) & (onset_df['ENCOUNTERID'].isin(adm_scr_encs))])\n",
    "        }\n",
    "        df_stats.append(pd.Series(stats, name=site))\n",
    "\n",
    "    df_stats = pd.DataFrame(df_stats)\n",
    "    # #df_stats.index = site_list  # Set sites as the index\n",
    "    # df_stats['aki1c'] = df_stats['aki1_scr1w_count'] + df_stats['aki1_scr1y_count'] + df_stats['aki1_mdrd_count'] + df_stats['aki1_adm_count']\n",
    "    # df_stats['aki2c'] = df_stats['aki2_scr1w_count'] + df_stats['aki2_scr1y_count'] + df_stats['aki2_mdrd_count'] + df_stats['aki2_adm_count']\n",
    "    # df_stats['aki3c'] = df_stats['aki3_scr1w_count'] + df_stats['aki3_scr1y_count'] + df_stats['aki3_mdrd_count'] + df_stats['aki3_adm_count']\n",
    "    return df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de654d1-9873-4093-96cd-1040e7f37a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_stats.apply('sum', axis = 0)\n",
    "#df_stats.apply('sum', axis = 0)/640568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ea302-66b6-46d3-9247-714758f2882f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eGFR_MDRD(df, scr_label):\n",
    "    # Adjust Scr for units; assuming Scr is given in mg/dL\n",
    "    Scr = df[scr_label]\n",
    "\n",
    "    # Coefficients for gender and race\n",
    "    gender_coeff = np.where(df['MALE'], 1, 0.742)\n",
    "    race_coeff = np.where(df['RACE_BLACK'], 1.212, 1)\n",
    "\n",
    "    # MDRD equation components\n",
    "    Scr_component = (Scr) ** -1.154\n",
    "    age_component = df['AGE'] ** -0.203\n",
    "\n",
    "    # eGFR calculation\n",
    "    eGFR = 175 * Scr_component * age_component * gender_coeff * race_coeff\n",
    "    return eGFR\n",
    "\n",
    "\n",
    "def eGFR_CKDEPI09(df, scr_label):\n",
    "    # Determine kappa and alpha based on 'MALE' column\n",
    "    kappa = np.where(df['MALE'], 0.9, 0.7)\n",
    "    alpha = np.where(df['MALE'],  -0.411, -0.329)\n",
    "\n",
    "    # Coefficients for gender\n",
    "    gender_coeff = np.where(df['MALE'], 1, 1.018)\n",
    "    race_coeff = np.where(df['RACE_BLACK'], 1.159, 1)\n",
    "    \n",
    "    # Calculate eGFR\n",
    "    Scr_over_kappa = df[scr_label] / kappa\n",
    "    min_term = np.where(Scr_over_kappa <= 1, Scr_over_kappa**alpha, 1)\n",
    "    max_term = np.where(Scr_over_kappa > 1, Scr_over_kappa**(-1.209), 1)\n",
    "    age_term = 0.993 ** df['AGE']\n",
    "\n",
    "    # eGFR calculation\n",
    "    eGFR = 141 * min_term * max_term * age_term * gender_coeff * race_coeff\n",
    "    return eGFR\n",
    "    \n",
    "\n",
    "def eGFR_CKDEPI21(df, scr_label):\n",
    "    # Determine kappa and alpha based on 'MALE' column\n",
    "    kappa = np.where(df['MALE'], 0.9, 0.7)\n",
    "    alpha = np.where(df['MALE'], -0.302, -0.241)\n",
    "\n",
    "    # Coefficients for gender\n",
    "    gender_coeff = np.where(df['MALE'], 1, 1.012)\n",
    "\n",
    "    # Calculate eGFR\n",
    "    Scr_over_kappa = df[scr_label] / kappa\n",
    "    min_term = np.where(Scr_over_kappa <= 1, Scr_over_kappa**alpha, 1)\n",
    "    max_term = np.where(Scr_over_kappa > 1, Scr_over_kappa**(-1.200), 1)\n",
    "    age_term = 0.9938 ** df['AGE']\n",
    "\n",
    "    # eGFR calculation\n",
    "    eGFR = 142 * min_term * max_term * age_term * gender_coeff\n",
    "    return eGFR\n",
    "    \n",
    "def ckd_staging(df, egfr_label):\n",
    "    # Assuming ckd_egfr is your DataFrame and 'eGFR' is a column in this DataFrame\n",
    "    conditions = [\n",
    "        (df[egfr_label] >= 90),\n",
    "        (df[egfr_label] >= 60) & (df[egfr_label] < 90),\n",
    "        (df[egfr_label] >= 45) & (df[egfr_label] < 60),\n",
    "        (df[egfr_label] >= 30) & (df[egfr_label] < 45),\n",
    "        (df[egfr_label] >= 15) & (df[egfr_label] < 30),\n",
    "        (df[egfr_label] < 15)\n",
    "    ]\n",
    "\n",
    "    # Define the CKD stages corresponding to the above conditions\n",
    "    choices = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "    # Apply the conditions and choices to the DataFrame\n",
    "    ckd_stage = np.select(conditions, choices, default=np.nan) \n",
    "    return ckd_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3dc74-0507-4f8e-94c5-55d818c0ef48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rrt(df_admit, filepath_lst):\n",
    "    px = pd.read_pickle(filepath_lst[0]+'AKI_PX.pkl')   \n",
    "\n",
    "    idx_transplant = np.logical_or(np.logical_or(\n",
    "                           np.logical_and(px['PX_TYPE']=='CH',px['PX'].isin(['50300','50320','50323','50325','50327','50328','50329','50340','50360','50365','50370','50380'])),\n",
    "                           np.logical_and(px['PX_TYPE']=='09',px['PX'].isin(['55.51','55.52','55.53','55.54','55.61','55.69']))),np.logical_or(\n",
    "                           np.logical_and(px['PX_TYPE']=='9',px['PX'].isin(['55.51','55.52','55.53','55.54','55.61','55.69'])),                       \n",
    "                           np.logical_and(px['PX_TYPE']=='10',px['PX'].isin(['0TY00Z0','0TY00Z1','0TY00Z2','0TY10Z0','0TY10Z1','0TY10Z2','0TB00ZZ','0TB10ZZ','0TT00ZZ','0TT10ZZ','0TT20ZZ']))))\n",
    "\n",
    "    idx_dialysis =(((px['PX_TYPE']=='CH') & (px['PX'].isin(['90935', '90937']))) |  \n",
    "                  ((px['PX_TYPE']=='CH') & (pd.to_numeric(px['PX'], errors='coerce').between(90940, 90999))) |   \n",
    "                  ((px['PX_TYPE']=='9') & ((px['PX'].isin(['39.93','39.95','54.98', 'V45.11'])))) | \n",
    "                  ((px['PX_TYPE']=='09') & (px['PX'].isin(['39.93','39.95','54.98', 'V45.11']))) |  \n",
    "                  ((px['PX_TYPE']=='10') & (px['PX'].isin(['5A1D00Z','5A1D60Z','5A1D70Z','5A1D80Z','5A1D90Z', 'Z99.2'])))) \n",
    " \n",
    "    rrt_stage =  px[idx_transplant | idx_dialysis] \n",
    "\n",
    "    rrt_stage = rrt_stage[['PATID','ENCOUNTERID','PX_DATE']]\n",
    "    rrt_stage.columns = ['PATID','ENCOUNTERID','RRT_ONSET_DATE']\n",
    "\n",
    "    rrt_stage = rrt_stage.merge(df_admit, on=['PATID', 'ENCOUNTERID'], how='inner')\n",
    "    rrt_stage['RRT_SINCE_ADMIT'] = (rrt_stage['RRT_ONSET_DATE']-rrt_stage['ADMIT_DATE']).dt.total_seconds()/(3600*24)\n",
    "    rrt_stage = rrt_stage.loc[rrt_stage[['ENCOUNTERID', 'RRT_SINCE_ADMIT']].groupby('ENCOUNTERID').idxmin().reset_index()['RRT_SINCE_ADMIT']]\n",
    "    rrt_stage.drop('ADMIT_DATE', axis = 1, inplace = True)\n",
    "    return rrt_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876c218-312f-48a8-b136-9d64a38f2d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def determine_initial_aki_stage(row):\n",
    "    # Extract the AKI onset days\n",
    "    aki_days = {\n",
    "        1: row['AKI1_SINCE_ADMIT'],\n",
    "        2: row['AKI2_SINCE_ADMIT'],\n",
    "        3: row['AKI3_SINCE_ADMIT']\n",
    "    }\n",
    "    \n",
    "    # Remove NaN values\n",
    "    aki_days = {key: val for key, val in aki_days.items() if not pd.isnull(val)}\n",
    "    \n",
    "    if not aki_days:\n",
    "        return np.nan\n",
    "    \n",
    "    # Find the minimum value and handle ties by prioritizing higher stages\n",
    "    min_value = min(aki_days.values())\n",
    "    highest_stage = max(stage for stage, day in aki_days.items() if day == min_value)\n",
    "    \n",
    "    return highest_stage\n",
    "\n",
    "\n",
    "def get_aki_onset(df_scr, df_admit, df_rrt, df_baseline, aki_criteria = 'either'):\n",
    "    xxx = df_scr.copy()\n",
    "    yyy = df_admit.copy()\n",
    "    ##### Filter out the CKD patients that does not have baseline \n",
    "    # Create a set of tuples representing (ENCOUNTERID, PATID) combinations in df_baseline\n",
    "    valid_combinations = df_baseline[['ENCOUNTERID', 'PATID']].drop_duplicates()\n",
    "\n",
    "    # Use merge to keep only rows with matching (ENCOUNTERID, PATID) combinations\n",
    "    xxx = xxx.merge(valid_combinations, on=['ENCOUNTERID', 'PATID'], how='inner')\n",
    "    yyy = yyy.merge(valid_combinations, on=['ENCOUNTERID', 'PATID'], how='inner')\n",
    "    \n",
    "    ######\n",
    "    zzz = df_baseline[['PATID', 'ENCOUNTERID', 'SERUM_CREAT_BASE']].drop_duplicates()\n",
    "    zzz.columns= ['PATID', 'ENCOUNTERID',  'RESULT_NUM_BASE_7d']\n",
    "    xxx = xxx.merge(zzz, on = ['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "    zzz2 = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']].groupby(['PATID', 'ENCOUNTERID']).rolling('2d', on='SPECIMEN_DATE').min().reset_index()\n",
    "    zzz2 = zzz2[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    "    zzz2.columns= ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM_BASE_2d']\n",
    "    xxx = xxx.merge(zzz2, on = ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'], how='left')\n",
    "\n",
    "    # Check condition for AKI1\n",
    "    #1.5 increase in 7 days\n",
    "    xxx['AKI1.5'] = (xxx['RESULT_NUM']>=1.5*xxx['RESULT_NUM_BASE_7d']) & (xxx['DAYS_SINCE_ADMIT']>=0) \n",
    "    #0.3 increase in 48 hours\n",
    "    xxx['AKI0.3'] = (xxx['RESULT_NUM']-xxx['RESULT_NUM_BASE_2d']>=0.3) & (xxx['DAYS_SINCE_ADMIT']>=0)      \n",
    "    \n",
    "    if aki_criteria == '2d':\n",
    "        xxx = xxx[xxx['AKI0.3']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "        \n",
    "    elif aki_criteria == '7d':\n",
    "        xxx = xxx[xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "\n",
    "    elif aki_criteria == 'both':\n",
    "        xxx = xxx[xxx['AKI0.3'] & xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "        \n",
    "    elif aki_criteria == 'either':\n",
    "        xxx = xxx[xxx['AKI0.3'] | xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "\n",
    "    xxx['AKI1_SINCE_ADMIT'] = xxx['DAYS_SINCE_ADMIT'].copy()\n",
    "    xxx['AKI1_DATE'] = xxx['SPECIMEN_DATE'].copy()\n",
    "    xxx['AKI1_SCR'] = xxx['RESULT_NUM'].copy()\n",
    "    xxx['SCR_BASELINE'] = xxx['RESULT_NUM_BASE'].copy()\n",
    "    xxx['SCR_REFERENCE'] = xxx['RESULT_NUM_BASE_2d'].copy()\n",
    "    xxx['AKI1_7D'] = xxx['AKI1.5'].copy()\n",
    "    xxx['AKI1_2D'] = xxx['AKI0.3'].copy()\n",
    "    xxx = xxx[['PATID', 'ENCOUNTERID', 'SCR_BASELINE', 'SCR_REFERENCE',  'AKI1_DATE', 'AKI1_SCR', 'AKI1_SINCE_ADMIT', 'AKI1_7D', 'AKI1_2D']]\n",
    "\n",
    "    # Check condition for AKI2: 2.0x - <3.0x\n",
    "    aki2 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki2 = aki2[aki2['SPECIMEN_DATE']>=aki2['AKI1_DATE']]\n",
    "    aki2 = aki2[aki2['RESULT_NUM']>=2*aki2['SCR_BASELINE']]\n",
    "    aki2 = aki2.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki2['AKI2_SINCE_ADMIT'] = aki2['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki2['AKI2_DATE'] = aki2['SPECIMEN_DATE'].copy()\n",
    "    aki2['AKI2_SCR'] = aki2['RESULT_NUM'].copy()\n",
    "    aki2 = aki2[['PATID', 'ENCOUNTERID', 'AKI2_DATE', 'AKI2_SCR', 'AKI2_SINCE_ADMIT']]\n",
    "    \n",
    "    # Check condition for AKI3: SCR >= 3.0x Baseline\n",
    "    aki3 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki3 = aki3[aki3['SPECIMEN_DATE']>=aki3['AKI1_DATE']]\n",
    "    aki3 = aki3[(aki3['RESULT_NUM']>=3*aki3['SCR_BASELINE']) | (aki3['RESULT_NUM']>=4)]\n",
    "    aki3 = aki3.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki3['AKI3_SINCE_ADMIT'] = aki3['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki3['AKI3_DATE'] = aki3['SPECIMEN_DATE'].copy()\n",
    "    aki3['AKI3_SCR'] = aki3['RESULT_NUM'].copy()\n",
    "    aki3 = aki3[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    \n",
    "    # Check condition for AKI3: initiation of RRT\n",
    "    #df_rrt = get_rrt(path, ext, sep, yyy)\n",
    "    rrt = df_rrt.merge(xxx[['PATID', 'ENCOUNTERID', 'AKI1_DATE']], on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    rrt = rrt[rrt['RRT_ONSET_DATE'] >= rrt['AKI1_DATE']]\n",
    "    aki3b =  aki3.merge(rrt, on = ['PATID', 'ENCOUNTERID'], how = 'outer')\n",
    "    cond_rrt = (aki3b['RRT_SINCE_ADMIT'] < aki3b['AKI3_SINCE_ADMIT']) | (aki3b['AKI3_SINCE_ADMIT'].isna() & aki3b['RRT_SINCE_ADMIT'].notna())\n",
    "    aki3b.loc[cond_rrt, 'AKI3_SINCE_ADMIT'] = aki3b.loc[cond_rrt, 'RRT_SINCE_ADMIT']\n",
    "    aki3b.loc[cond_rrt, 'AKI3_DATE'] = aki3b.loc[cond_rrt, 'RRT_ONSET_DATE']\n",
    "    \n",
    "    aki3_all = aki3b[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    \n",
    "    # Merge AKI staging information\n",
    "    onset = xxx.merge(aki2, on=['PATID', 'ENCOUNTERID'], how='outer').merge(aki3_all, on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    onset = onset.merge(yyy, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "    onset.columns = onset.columns.str.upper()\n",
    "    onset['ONSET_DATE'] = onset['AKI1_DATE'].copy()  \n",
    "    onset['SCR_ONSET'] = onset['AKI1_SCR'].copy() \n",
    "    \n",
    "    onset['DISCHARGE_SINCE_ONSET'] = (onset['DISCHARGE_DATE'] - onset['ONSET_DATE']).dt.days\n",
    "    \n",
    "    onset = onset[['PATID','ENCOUNTERID', 'ADMIT_DATE', 'DISCHARGE_DATE', \n",
    "                   'ONSET_DATE', 'AKI1_SINCE_ADMIT', 'AKI2_SINCE_ADMIT', \n",
    "                   'AKI3_SINCE_ADMIT',  'DISCHARGE_SINCE_ONSET','SCR_ONSET', \n",
    "                   'SCR_BASELINE',  'SCR_REFERENCE', 'AKI1_7D', 'AKI1_2D']]\n",
    "\n",
    "    onset['FLAG'] = (onset['AKI2_SINCE_ADMIT'].notna()) | (onset['AKI3_SINCE_ADMIT'].notna())\n",
    "    onset['ONSET_SINCE_ADMIT'] = onset['AKI1_SINCE_ADMIT'].copy()  #onset[['AKI1_SINCE_ADMIT', 'AKI2_SINCE_ADMIT', 'AKI3_SINCE_ADMIT']].min(axis=1)\n",
    "    \n",
    "    #Generate onset staging by taking the first stage\n",
    "    onset['AKI_STAGE'] = 0\n",
    "    filter_aki3 = onset['AKI3_SINCE_ADMIT'].notna()\n",
    "    filter_aki2 = onset['AKI2_SINCE_ADMIT'].notna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    filter_aki1 = onset['AKI1_SINCE_ADMIT'].notna() & onset['AKI2_SINCE_ADMIT'].isna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    \n",
    "    onset.loc[filter_aki3, 'AKI_STAGE'] = 3\n",
    "    onset.loc[filter_aki2, 'AKI_STAGE'] = 2\n",
    "    onset.loc[filter_aki1, 'AKI_STAGE'] = 1\n",
    "    \n",
    "    \n",
    "    # Determine the initial AKI stage by finding the column with the smallest onset day\n",
    "    onset['AKI_INIT_STG'] = onset.apply(determine_initial_aki_stage, axis=1)\n",
    "    \n",
    "    return onset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff831a-ad8d-4e7f-aa4e-e6a94d7756f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def process_onset_and_recovery_new(filepath_lst, aggfunc_7d, aggfunc_1y, keep_ckd):\n",
    "    filepath_lst = get_data_file_path(pdatpath, site)\n",
    "    df_scr, df_admit = load_onset_data(filepath_lst)\n",
    "    df_baseline, cohort_table = get_scr_baseline_new(df_scr, df_admit, filepath_lst, aggfunc_7d, aggfunc_1y, keep_ckd)\n",
    "    df_rrt = get_rrt(df_admit, filepath_lst)\n",
    "\n",
    "    onset = get_aki_onset(df_scr, df_admit, df_rrt, df_baseline)\n",
    "    #onset.to_pickle(filepath_lst[1] + 'onset.pkl')\n",
    "\n",
    "    print(f\"Finish generating AKI onset and recovery for {site}.\", flush = True)\n",
    "    return onset, df_baseline, cohort_table\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "pdatpath = '/blue/yonghui.wu/qixu/aki/'\n",
    "# Initialize logging\n",
    "logging.basicConfig(filename='processing_errors.log', level=logging.ERROR)\n",
    "\n",
    "site_list = ['KUMC', 'MCW', 'UIOWA', 'UMHC', 'UNMC', 'UTHSCSA', 'UTSW','UofU', 'UPITT', 'MCRI'] #['KUMC', 'UMHC', 'UNMC', 'UTHSCSA']  \n",
    "\n",
    "onset_dict4 = {}\n",
    "\n",
    "for site in site_list:\n",
    "    try:\n",
    "        filepath_lst = get_data_file_path(pdatpath, site)\n",
    "        onset_dict4[site] = process_onset_and_recovery_new(filepath_lst, \n",
    "                                            aggfunc_7d = 'last', \n",
    "                                            aggfunc_1y = 'mean', \n",
    "                                            keep_ckd = False)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing site {site}: {e}\")\n",
    "        print(f\"Error processing site {site}. Check log for details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
