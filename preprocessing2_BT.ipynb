{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d250c-f012-4186-bfbe-31fd7f61954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Depreciated, see preprocessing2.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e99a71-fac3-4bb4-8b65-6646efb82a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This module contains a set of function that process different table from preprocess1 module into one Big Table\n",
    "\n",
    "Input:\n",
    "px_{site}_{str(year)}.pkl\n",
    "dx_{site}_{str(year)}.pkl\n",
    "onset_{site}_{str(year)}.pkl\n",
    "vital_{site}_{str(year)}.pkl\n",
    "amed_{site}_{str(year)}.pkl\n",
    "lab_{site}_{str(year)}.pkl\n",
    "\n",
    "Output:\n",
    "bt3_{site}_{str(year)}.pkl\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097aaf32-f964-41b3-8e46-5bd60085eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "#import rpy2.robjects as robjects\n",
    "#from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "import logging\n",
    "import utils_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799e9f8-05fc-41b8-abf3-fce92d3e42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_onset(site, year):\n",
    "    '''\n",
    "    This module read and return the onset table\n",
    "    '''\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    print('Merging onset on site '+site+\":\"+str(year), flush = True)    \n",
    "    try:\n",
    "        return pd.datafolder+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "#        newdf_debug['onset'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No onset table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No onset table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0627c-5d5c-4e5b-9ffb-2e90d44ee329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_px(site, year, newdf):        \n",
    "    '''\n",
    "    This module read the px table and merge with the big table\n",
    "    '''\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    print('Merging px on site '+site+\":\"+str(year), flush = True)        \n",
    "    try:\n",
    "        px = pd.datafolder+site+'/px_'+site+'_'+str(year)+'.pkl')\n",
    "        #depreciate Since ADMIT\n",
    "#        newdf = pd.merge(newdf, px, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "        return pd.merge(newdf, px, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna(False)\n",
    "#        newdf_debug['px'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No px table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No px table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e9d6f-36d7-4062-aabd-73b1485793c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_dx(site, year, newdf):                \n",
    "    '''\n",
    "    This module read the dx table and merge with the big table\n",
    "    '''\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    \n",
    "    print('Merging dx on site '+site+\":\"+str(year), flush = True)            \n",
    "    try:\n",
    "        dx = pd.datafolder+site+'/dx_'+site+'_'+str(year)+'.pkl')\n",
    "#        newdf = pd.merge(newdf, dx, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "        return pd.merge(newdf, dx, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna(False)       \n",
    "#        newdf_debug['dx'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No onset table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No onset table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6181b15-8b9f-40a6-8864-e857c659828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_amed(site, year, newdf):                    \n",
    "    '''\n",
    "    This module read the amed table and merge with the big table\n",
    "    '''\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    print('Merging amed on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "        amed = pd.datafolder+site+'/amed_'+site+'_'+str(year)+'.pkl')\n",
    "#        newdf = pd.merge(newdf, amed, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "        return pd.merge(newdf, amed, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna(False)        \n",
    "#        newdf = newdf.combine_first(newdf[list(amed.select_dtypes('bool').columns)].fillna(False))    \n",
    "#        newdf_debug['amed'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No amed table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No amed table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()    \n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08cfab5-89b0-4293-85ee-030526543362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_labcat(site, year, newdf):                        \n",
    "    '''\n",
    "    This module read the lab(categorical) table and merge with the big table\n",
    "    '''\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    print('Merging lab_cat on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "        labcat = pd.datafolder+site+'/labcat_'+site+'_'+str(year)+'.pkl')\n",
    "#        newdf = pd.merge(newdf, amed, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "        return pd.merge(newdf, labcat, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna(False)        \n",
    "#        newdf = newdf.combine_first(newdf[list(amed.select_dtypes('bool').columns)].fillna(False))    \n",
    "#        newdf_debug['amed'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No lab_cat table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No lab_cat table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()          \n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd28c5b-cfbd-4d98-86ed-d9686cbacaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bt_demo(site, year, newdf):                            \n",
    "    '''\n",
    "    This module read the demographic table and merge with the big table\n",
    "    '''\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    print('Merging demo on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "        demo = pd.datafolder+site+'/demo_'+site+'_'+str(year)+'.pkl')\n",
    "        newdf2 = pd.merge(newdf, demo, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "        return newdf2.combine_first(newdf2[list(demo.select_dtypes('bool').columns)].fillna(False))            \n",
    "#        newdf_debug['demo'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No demo table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No demo table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()    \n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2db93-4a80-467c-85fd-779d3635422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_vital(site, year, newdf):                                \n",
    "    '''\n",
    "    This module read the vital table and merge with the big table\n",
    "    '''\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    print('Merging vital on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "#        vital = pd.datafolder+site+'/vital_'+site+'_'+str(year)+'.pkl')\n",
    "        vital = pd.datafolder+site+'/vital_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "#        newdf = pd.merge(newdf, vital, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "        return pd.merge(newdf, vital, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left')        \n",
    "#        newdf_debug['vital'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No vital table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No vital table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fda8cd-c776-430a-a28f-2adfcf289862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_labnum(site, year, newdf):                                    \n",
    "    '''\n",
    "    This module read the lab(numerical) table and merge with the big table\n",
    "    '''\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']    \n",
    "    \n",
    "    print('Merging lab_num on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "#        labnum = pd.datafolder+site+'/labnum_'+site+'_'+str(year)+'.pkl')\n",
    "        labnum = pd.datafolder+site+'/labnum_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "#        newdf = pd.merge(newdf, lab_t, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')   \n",
    "        return pd.merge(newdf, labnum, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "#        newdf_debug['lab'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No lab_num table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No lab_num table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb52972e-f302-405d-968a-1242a70173ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_too_much_nan(site, year, newdf, threshold=0.05):\n",
    "    '''\n",
    "    This module read drop columns with 95% missing data in targeted class\n",
    "    '''\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    print('Remove sparse feature on site '+site+\":\"+str(year), flush = True)                        \n",
    "    btX = newdf.replace(False, np.nan)\n",
    "    #limitPer = len(btX) * threshold\n",
    "    #col = btX.dropna(thresh=limitPer, axis=1).columns\n",
    "    btX0 = btX[btX['FLAG']==0]\n",
    "    btX1 = btX[btX['FLAG']==1]\n",
    "    limitPer0 = len(btX0) * threshold\n",
    "    limitPer1 = len(btX1) * threshold\n",
    "    col0 = btX0.dropna(thresh=limitPer0, axis=1).columns\n",
    "    col1 = btX1.dropna(thresh=limitPer1, axis=1).columns\n",
    "    col = list(set(list(col1)+list(col0)))\n",
    "    return newdf[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91facabb-5217-4777-93c0-e52f7ddac18a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handpickremoval(site, year, newdf):   \n",
    "    '''\n",
    "    This module read drop the service CPT code and weight\n",
    "    '''\n",
    "    \n",
    "    # drop CPT service code between 99202 and 99499 \n",
    "    cptcode0 = np.array([x for x in newdf.columns if 'PX' in x and x.split(':')[2].isnumeric()])\n",
    "    cptcode = np.array([int(x.split(':')[2]) if x.split(':')[2].isnumeric() else 0 for x in cptcode0])\n",
    "    cptcodebool = np.logical_or(np.logical_and(cptcode >= 99202, cptcode <= 99499),np.logical_and(cptcode >= 80047, cptcode <= 89398))\n",
    "    remlist = cptcode0[cptcodebool]\n",
    "    \n",
    "    # Additional drop\n",
    "    remlist2 = ['WT']\n",
    "    \n",
    "    remlist = list(remlist)+remlist2\n",
    "    return newdf.drop(remlist,axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86e42d-a85a-40d1-aee2-f4374a985d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def drop_corr(site, year, newdf, threshold=0.5):\n",
    "#     print('Remove correlated feature on site '+site+\":\"+str(year), flush = True)                        \n",
    "#     corr = newdf.corr()\n",
    "#     columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "#     for i in range(corr.shape[0]):\n",
    "#         for j in range(i+1, corr.shape[0]):\n",
    "#             if corr.iloc[i,j] >= threshold:\n",
    "#                 # if corr.columns[j] == 'ORIGINAL_BMI':\n",
    "#                 #     if columns[i]:\n",
    "#                 #         columns[i] = False\n",
    "#                 if columns[j]:\n",
    "#                     columns[j] = False\n",
    "#     selected_columns = newdf.columns[columns]\n",
    "#     return newdf[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c566b6ec-ab5c-4bb9-a930-229424b58795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pearson\n",
    "def pearson_list(bt, threshold):\n",
    "    corr = bt.corr()\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i,j] >= threshold:\n",
    "#                print(bt.columns[i], btcont.columns[j], corr.iloc[i,j])\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    return columns   \n",
    "\n",
    "def point_biserial(btcat, btcon, threshold):\n",
    "    from scipy import stats\n",
    "    columns = np.full((btcat.shape[1],), True, dtype=bool)    \n",
    "    for i in range(btcon.shape[1]):\n",
    "        for j in range(btcat.shape[1]):\n",
    "            if stats.pointbiserialr(btcat.iloc[:,j], btcon.iloc[:,i])[0] >= threshold:            \n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    return columns\n",
    "    \n",
    "def drop_corr(site, year, bt, threshold):\n",
    "    bt2 = bt.reindex(sorted(bt.columns), axis=1)\n",
    "    btcat = bt2.select_dtypes('bool')\n",
    "    btcont = bt2.select_dtypes(exclude='bool')\n",
    "    return pd.concat([btcont.loc[:,pearson_list(btcont, threshold)], btcat.loc[:, (pearson_list(btcat, threshold) | point_biserial(btcat, btcon, threshold))]], axis=1)\n",
    "\n",
    "def drop_corr2(site, year, bt, threshold):\n",
    "    return bt.loc[:,pearson_list(bt, threshold)]\n",
    "\n",
    "def generate_drop_list(site, year, bt, threshold):\n",
    "    corr = bt.corr()\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i,j] >= threshold:\n",
    "                print(site, year, bt.columns[i], btcont.columns[j], corr.iloc[i,j])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b98942-e2db-4852-8f39-0092e94a5c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bigtable(site, year):\n",
    "    '''\n",
    "    This module combine different tables into one big table\n",
    "    \n",
    "    Input:\n",
    "    px_{site}_{str(year)}.pkl\n",
    "    dx_{site}_{str(year)}.pkl\n",
    "    onset_{site}_{str(year)}.pkl\n",
    "    vital_{site}_{str(year)}.pkl\n",
    "    amed_{site}_{str(year)}.pkl\n",
    "    lab_{site}_{str(year)}.pkl\n",
    "    \n",
    "    Output:\n",
    "    bt3_{site}_{str(year)}.pkl - vital table (cont)    \n",
    "    '''\n",
    "    \n",
    "    print('Merging bt on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    #load tables\n",
    "    newdf_debug = dict()\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    try:\n",
    "        #read onset table\n",
    "        newdf = bt_onset(site, year)    \n",
    "        # boolean table must merge first\n",
    "        # merge boolean tables\n",
    "        newdf = bt_px(site, year, newdf)\n",
    "        newdf = bt_dx(site, year, newdf)\n",
    "        newdf = bt_amed(site, year, newdf)\n",
    "        newdf = bt_labcat(site, year, newdf)\n",
    "\n",
    "        # merge continuous tables\n",
    "        newdf = bt_demo(site, year, newdf)\n",
    "        newdf = bt_vital(site, year, newdf)\n",
    "        newdf = bt_labnum(site, year, newdf)\n",
    "        newdf = handpickremoval(site, year, newdf)\n",
    "        \n",
    "        # Migrated to preprocessing4 module\n",
    "#        newdf = drop_too_much_nan(site, year, newdf, threshold=0.05)\n",
    "#        newdf = bt_postprocess(site, year, newdf)\n",
    "#        newdf = drop_corr2(site, year, newdf, threshold=0.5)        \n",
    "        \n",
    "        #Save table\n",
    "        newdf.to_pickle(datafolder+site+'/bt3_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "        #consistency check\n",
    "        if newdf.empty:\n",
    "            logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "            print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "            logging.error('BT: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "            logging.shutdown()\n",
    "\n",
    "        print('Finished bt on site '+site+\":\"+str(year), flush = True)        \n",
    "    except Exception as e:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('OTHER ERROR!!!!! '+site+\":\"+str(year)+'\\n+++++++++++++++++\\n'+str(e)+'\\n-------------------\\n', flush = True)\n",
    "        logging.error('OTHER ERROR!!!!! '+site+\":\"+str(year)+'\\n+++++++++++++++++\\n'+str(e)+'\\n-------------------\\n')\n",
    "        logging.shutdown()       \n",
    "        raise    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e35c1b-a2d4-46df-a769-9223b11e2506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigtable_removal_only(site, year, stg, newdf):\n",
    "    #Big Table\n",
    "    print('BT removal on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    try:\n",
    "        newdf = pd.datafolder+site+'/bt3_'+site+'_'+stg+'_3000.pkl')      \n",
    "        newdf = handpickremoval(site, year, newdf)        \n",
    "        newdf.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/bt3_'+site+'_'+stg+'_3000.pkl')\n",
    "\n",
    "        #consistency check\n",
    "        if newdf.empty:\n",
    "            logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "            print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "            logging.error('BT: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "            logging.shutdown()\n",
    "\n",
    "        print('Finished bt on site '+site+\":\"+str(year), flush = True)        \n",
    "    except Exception as e:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('OTHER ERROR!!!!! '+site+\":\"+str(year)+'\\n+++++++++++++++++\\n'+str(e)+'\\n-------------------\\n', flush = True)\n",
    "        logging.error('OTHER ERROR!!!!! '+site+\":\"+str(year)+'\\n+++++++++++++++++\\n'+str(e)+'\\n-------------------\\n')\n",
    "        logging.shutdown()       \n",
    "        raise    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1785bc-509d-44c8-8c69-5379f8590e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
