{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936897e1-0e30-4bc7-a43a-32b8e1870b85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:37:41.318338Z",
     "start_time": "2023-12-03T23:37:39.324512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import python packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import importlib\n",
    "#from scipy.stats import chi2_contingency\n",
    "#from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "#from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5828b-fe13-4187-b261-8daacd5bb6ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_list = ['KUMC', 'UTSW', 'MCW', 'UofU', 'UIOWA', 'UMHC', 'UPITT', 'UTHSCSA', 'UNMC']\n",
    "ext_list = ['csv','dsv', 'dsv', 'csv', 'csv', 'csv', 'csv', 'csv', 'csv']\n",
    "sep_list = [',','|', '|', '|', ',', ',', ',', ',', '|']\n",
    "encoding_list = ['utf-8','utf-8','utf-8','utf-8','utf-8','utf-8', 'windows-1252', 'utf-8','utf-16'] \n",
    "ct = 0\n",
    "\n",
    "daily_average = True\n",
    "\n",
    "\n",
    "site = site_list[ct]\n",
    "ext = ext_list[ct]\n",
    "sep = sep_list[ct]\n",
    "encoding = encoding_list[ct]\n",
    "path = []\n",
    "\n",
    "if site != 'KUMC':\n",
    "    rawpath = '/blue/yonghui.wu/hoyinchan/Data/data2022raw/' + site + '/raw/'\n",
    "else: \n",
    "    rawpath = '/blue/yonghui.wu/hoyinchan/Data/data2022raw/' + site + '_ORCALE/raw/'\n",
    "path.append(rawpath)\n",
    "path.append('/blue/yonghui.wu/hoyinchan/Data/data2022/' + site + '/')\n",
    "pdata = '/blue/yonghui.wu/hoyinchan/Data/data2022/'+ site \n",
    "site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e08053-c887-4392-b136-43c756e8ec6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_load_csv(filename, ext, sep, path):\n",
    "    filename = filename.replace('AKI_', '').lower()\n",
    "    df = pd.read_parquet(path[1] + 'p0_' + filename + '_' + site +'.parquet')\n",
    "    df['PATID'] = df['PATID'].astype(str)\n",
    "    df['ENCOUNTERID'] = df['ENCOUNTERID'].astype(str)    \n",
    "    return df\n",
    "\n",
    "\n",
    "def try_load_csv_old(filename, ext, sep, path):\n",
    "    if 'UMHC' in path[0]:\n",
    "        filename = 'DEID_'+filename    \n",
    "        \n",
    "    try:\n",
    "        # Try to load the file from the first path\n",
    "        df = pd.read_csv( path[0] +  filename + '.' + ext, engine=\"pyarrow\", sep=sep, encoding=encoding)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load from {path[0]}: {e}. Loading converted csv...\")\n",
    "        try:\n",
    "            # If the first attempt fails, try to load the file from the second path\n",
    "            #df = pd.read_csv( path[0] +  filename + '.' + ext, engine=\"pyarrow\", sep=sep)\n",
    "            if 'UofU' in path[0]: \n",
    "                df = pd.read_csv(path[0] + filename + '.' + ext,  sep=sep, on_bad_lines = 'skip') #pd.read_csv(path[1] + filename + '.csv', engine=\"pyarrow\", sep=',')\n",
    "            else:\n",
    "                df = pd.read_csv(path[1] +  filename + '.csv', engine=\"pyarrow\", sep=',', encoding=encoding)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                # If the first attempt fails, try to load the file from the second path\n",
    "                if 'UofU' in path[0]: \n",
    "                    df = pd.read_csv(path[0] +  filename + '.csv', sep=sep, on_bad_lines = 'skip', encoding=encoding)\n",
    "                else:                \n",
    "                    df = pd.read_csv(path[1] +  filename + '.csv', sep=',', on_bad_lines = 'skip', encoding=encoding)\n",
    "            except Exception as e:\n",
    "                # If the second attempt also fails, handle or re-raise the exception\n",
    "                print(f\"Failed to load from {path[1]} as well: {e}\")\n",
    "                raise Exception(f\"Could not load the file from either path.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a31ac8-6e98-45ff-9220-aa70eec5d2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pat_count(df, var = 'PATID'):\n",
    "    return df[var].nunique()\n",
    "\n",
    "def null_count(data):\n",
    "    return data.isnull().sum()/data.shape[0]\n",
    "\n",
    "def enc_count(df):\n",
    "    return df['ENCOUNTERID'].nunique()\n",
    "\n",
    "def aki_count(df):\n",
    "    return ( [pat_count(df), enc_count(df)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696b011-3605-4f9a-bf0b-7b05846a273f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_onset_data(path, ext, sep):\n",
    "    ## AKI staging\n",
    "    xxx = try_load_csv_old('AKI_LAB_SCR', ext, sep, path) #pd.read_csv(path+\"AKI_LAB_SCR.\"+ext, engine=\"pyarrow\", sep=sep)\n",
    "    yyy = try_load_csv_old('AKI_ONSETS', ext, sep, path) #pd.read_csv(path+\"AKI_ONSETS.\"+ext, engine=\"pyarrow\", sep=sep)\n",
    "    xxx.columns = xxx.columns.str.upper()\n",
    "    yyy.columns = yyy.columns.str.upper()\n",
    "    xxx.rename(columns = {'SPECIMEN_DATE\"+PD.DATE_SHIFT\"': 'SPECIMEN_DATE'}, inplace = True)\n",
    "    yyy.rename(columns = {'ADMIT_DATE\"+PD.DATE_SHIFT\"': 'ADMIT_DATE'}, inplace = True)\n",
    "    yyy = yyy[['ENCOUNTERID', 'PATID', 'ADMIT_DATE']]\n",
    "    yyy.columns = ['ENCOUNTERID', 'PATID', 'ADMIT_DATE']\n",
    "    \n",
    "#    xxx = xxx[['ONSETS_ENCOUNTERID', 'PATID', 'SPECIMEN_DATE', 'SPECIMEN_TIME', 'RESULT_NUM', 'DAYS_SINCE_ADMIT']]\n",
    "#    xxx.columns = ['ENCOUNTERID', 'PATID', 'SPECIMEN_DATE', 'SPECIMEN_TIME', 'RESULT_NUM', 'DAYS_SINCE_ADMIT']\n",
    "\n",
    "    xxx = xxx[['ONSETS_ENCOUNTERID', 'PATID', 'SPECIMEN_DATE', 'RESULT_NUM', 'DAYS_SINCE_ADMIT']]\n",
    "    xxx.columns = ['ENCOUNTERID', 'PATID', 'SPECIMEN_DATE', 'RESULT_NUM', 'DAYS_SINCE_ADMIT']\n",
    "\n",
    "    xxx = xxx.merge(yyy, on = ['ENCOUNTERID', 'PATID'], how='left')\n",
    "    #xxx['SPECIMEN_TIME'] = xxx['SPECIMEN_TIME'].fillna('00:00:00')\n",
    "    xxx = xxx.dropna()\n",
    "    xxx['SPECIMEN_DATE'] = pd.to_datetime(pd.to_datetime(xxx['SPECIMEN_DATE']).dt.date)\n",
    "    # xxx['SPECIMEN_DATETIME'] = xxx['SPECIMEN_DATE'].astype(str)+' '+xxx['SPECIMEN_TIME'].astype(str)\n",
    "    # xxx['SPECIMEN_DATETIME'] = pd.to_datetime(xxx['SPECIMEN_DATETIME'])\n",
    "    \n",
    "    xxx['ADMIT_DATE'] = pd.to_datetime(pd.to_datetime(xxx['ADMIT_DATE']).dt.date)\n",
    "    xxx['DAYS_SINCE_ADMIT'] = (xxx['SPECIMEN_DATE']-xxx['ADMIT_DATE']).dt.days\n",
    "    # xxx = xxx.sort_values('SPECIMEN_DATE')\n",
    "    # xxx = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATETIME', 'DAYS_SINCE_ADMIT', 'RESULT_NUM']].groupby(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATETIME', 'DAYS_SINCE_ADMIT']).mean()\n",
    "    xxx = xxx.sort_values(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'])\n",
    "    xxx = xxx.reset_index()\n",
    "    return xxx.drop_duplicates(), yyy.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433c1865-fcca-4ac4-b57b-3930c7100bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onset, df_y = read_onset_data(path, ext, sep)\n",
    "\n",
    "df_onsetw2i = df_onset.copy()\n",
    "df_onsetw2i['RESULT_NUM'] = df_onsetw2i['RESULT_NUM']*10000\n",
    "df_onsetw2i['RESULT_NUM'] = df_onsetw2i['RESULT_NUM'].astype(int)\n",
    "\n",
    "if daily_average:\n",
    "    df_onset = df_onset.groupby(['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'DAYS_SINCE_ADMIT']).mean().reset_index()\n",
    "\n",
    "if daily_average:\n",
    "    df_onsetw2i = df_onsetw2i.groupby(['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'DAYS_SINCE_ADMIT']).mean().reset_index()\n",
    "\n",
    "df_onsetw2i['RESULT_NUM'] = df_onsetw2i['RESULT_NUM']/10000.0\n",
    "\n",
    "# df_onsetw2i['INT_RESULT_NUM'] = df_onsetw2i['RESULT_NUM']\n",
    "# df_onsetw2i = df_onsetw2i.drop('RESULT_NUM',axis=1)\n",
    "\n",
    "# df_onsetw3i = df_onset.merge(df_onsetw2i, on = ['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'DAYS_SINCE_ADMIT'], how='outer')\n",
    "\n",
    "# df_onsetw4i = df_onsetw3i[df_onsetw3i['RESULT_NUM']!=df_onsetw3i['INT_RESULT_NUM']]\n",
    "\n",
    "# for i in range(df_onsetw4i.shape[0]):\n",
    "#     print('---')\n",
    "#     print(df_onsetw4i['RESULT_NUM'].iloc[i])\n",
    "#     print(df_onsetw4i['INT_RESULT_NUM'].iloc[i])\n",
    "\n",
    "#df_onset = df_onsetw2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e3e95-b0da-4f60-8c8c-fc5d4f3859fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onset.to_pickle(pdata+'/scr00.pkl')\n",
    "df_y.to_pickle(pdata+'/admit00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df36dd-8912-48ae-9bdd-47710082d405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d801c-7170-4349-94b8-65d80e59e635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d037b-9207-425c-ae19-276ef0413d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648e6d5-f38e-46a9-8671-caf442f53694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671206b4-0ca3-4b68-b7e8-f8878af383a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae3483-af01-4301-93fb-4e2cc274382c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a746fe-8350-4e34-ba43-cd528718c987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7275ce-79ce-402c-a040-0218403b2b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_onset.to_pickle(pdata+'/scr00.pkl')\n",
    "# df_y.to_pickle(pdata+'/admit00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9378d01-023d-4034-9089-92e82a079bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_onset = pd.read_pickle(pdata+'/scr00.pkl')\n",
    "# df_y = pd.read_pickle(pdata+'/admit00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5dbaa0-5725-4174-9556-7b1e47f8b64e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_y['PATID'] = df_y['PATID'].astype(str)\n",
    "df_y['ENCOUNTERID'] = df_y['ENCOUNTERID'].astype(str)\n",
    "df_onset['PATID'] = df_onset['PATID'].astype(str)\n",
    "df_onset['ENCOUNTERID'] = df_onset['ENCOUNTERID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34600e89-8fe4-4c50-8852-9f34ef7df6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inverse_MDRD(row, KDIGO_baseline):\n",
    "    age = row[\"AGE\"]\n",
    "    is_male = True if row[\"MALE\"]  else False\n",
    "    is_black = True if row[\"RACE_BLACK\"] else False\n",
    "        \n",
    "    if is_male and is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Black males\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Black males\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Black males\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Black males\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Black males\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Black males\"]\n",
    "    \n",
    "    if is_male and not is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Other males\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Other males\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Other males\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Other males\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Other males\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Other males\"]\n",
    "\n",
    "    if not is_male and is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Black females\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Black females\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Black females\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Black females\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Black females\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Black females\"]\n",
    "    \n",
    "    if not is_male and not is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Other females\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Other females\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Other females\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Other females\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Other females\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Other females\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96090d82-5f05-437b-afc8-afb6f56679a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scr_baseline_old(path, df_onset, df_y, ext, sep):\n",
    "\n",
    "    # process dx data\n",
    "    dx = try_load_csv_old('AKI_DX', ext, sep, path) # pd.read_csv(path+\"AKI_DX.\"+ext, sep= sep, engine=\"pyarrow\")\n",
    "    print(dx.shape[0])\n",
    "    dx.columns = dx.columns.str.upper()\n",
    "    dx.rename(columns = {'DX_DATE\"+PD.DATE_SHIFT\"': 'DX_DATE'}, inplace = True)\n",
    "    dx[\"ENCOUNTERID\"] = dx[\"ONSETS_ENCOUNTERID\"]\n",
    "    dx['ENCOUNTERID'] = dx['ENCOUNTERID'].astype(str)\n",
    "    dx['PATID'] = dx['PATID'].astype(str)\n",
    "    dx = dx[[\"PATID\", \"ENCOUNTERID\", \"DX\", \"DAYS_SINCE_ADMIT\", \"DX_DATE\", \"DX_TYPE\"]]\n",
    "    print(dx.shape[0])\n",
    "    \n",
    "    dx = df_y[[\"PATID\",\"ENCOUNTERID\", 'ADMIT_DATE']].merge(dx, on = [\"PATID\",\"ENCOUNTERID\"], how = \"inner\")\n",
    "    print(dx.shape[0])\n",
    "\n",
    "    dx['ADMIT_DATE'] = pd.to_datetime(pd.to_datetime(dx['ADMIT_DATE']).dt.date)\n",
    "    dx['DX_DATE'] = pd.to_datetime(pd.to_datetime(dx['DX_DATE']).dt.date)\n",
    "#    dx['DAYS_SINCE_ADMIT'] = (dx['DX_DATE']-dx['ADMIT_DATE']).dt.days\n",
    "\n",
    "    # process demo data\n",
    "    demo = try_load_csv('AKI_DEMO', ext, sep, path) # pd.read_csv(path+'AKI_DEMO.'+ext, sep=sep,  engine=\"pyarrow\")\n",
    "    demo.columns = demo.columns.str.upper()\n",
    "    demo['ENCOUNTERID'] = demo['ONSETS_ENCOUNTERID']\n",
    "    demo['ENCOUNTERID'] = demo['ENCOUNTERID'].astype(str)\n",
    "    demo['PATID'] = demo['PATID'].astype(str)\n",
    "    demo['MALE'] = demo['SEX'] == 'M'\n",
    "    demo['HISPANIC'] = demo['HISPANIC'] == 'Y'\n",
    "    demo['RACE_WHITE'] = demo['RACE'] == '05'\n",
    "    demo['RACE_BLACK'] = demo['RACE'] == '03'\n",
    "    demo = demo[['PATID', 'ENCOUNTERID', 'AGE', 'MALE', 'RACE_WHITE', 'RACE_BLACK', 'HISPANIC']]\n",
    "    demo = demo.drop_duplicates()\n",
    "    \n",
    "    return dx, demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d902185-2b0d-4230-bd27-5dd851f01c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scr_baseline(path, df_onset, df_y, ext, sep):\n",
    "\n",
    "    # process dx data\n",
    "    dx = try_load_csv('AKI_DX', ext, sep, path) # pd.read_csv(path+\"AKI_DX.\"+ext, sep= sep, engine=\"pyarrow\")\n",
    "    print(dx.shape[0])\n",
    "    dx.columns = dx.columns.str.upper()\n",
    "    dx.rename(columns = {'DX_DATE\"+PD.DATE_SHIFT\"': 'DX_DATE'}, inplace = True)\n",
    "    dx[\"ENCOUNTERID\"] = dx[\"ONSETS_ENCOUNTERID\"]\n",
    "    dx['ENCOUNTERID'] = dx['ENCOUNTERID'].astype(str)\n",
    "    dx['PATID'] = dx['PATID'].astype(str)\n",
    "    dx = dx[[\"PATID\", \"ENCOUNTERID\", \"DX\", \"DAYS_SINCE_ADMIT\", \"DX_DATE\", \"DX_TYPE\"]]\n",
    "    print(dx.shape[0])\n",
    "    \n",
    "    dx = df_y[[\"PATID\",\"ENCOUNTERID\", 'ADMIT_DATE']].merge(dx, on = [\"PATID\",\"ENCOUNTERID\"], how = \"inner\")\n",
    "    print(dx.shape[0])\n",
    "\n",
    "    dx['ADMIT_DATE'] = pd.to_datetime(pd.to_datetime(dx['ADMIT_DATE']).dt.date)\n",
    "    dx['DX_DATE'] = pd.to_datetime(pd.to_datetime(dx['DX_DATE']).dt.date)\n",
    "#    dx['DAYS_SINCE_ADMIT'] = (dx['DX_DATE']-dx['ADMIT_DATE']).dt.days\n",
    "\n",
    "    # process demo data\n",
    "    demo = try_load_csv('AKI_DEMO', ext, sep, path) # pd.read_csv(path+'AKI_DEMO.'+ext, sep=sep,  engine=\"pyarrow\")\n",
    "    demo.columns = demo.columns.str.upper()\n",
    "    demo['ENCOUNTERID'] = demo['ONSETS_ENCOUNTERID']\n",
    "    demo['ENCOUNTERID'] = demo['ENCOUNTERID'].astype(str)\n",
    "    demo['PATID'] = demo['PATID'].astype(str)\n",
    "    demo['MALE'] = demo['SEX'] == 'M'\n",
    "    demo['HISPANIC'] = demo['HISPANIC'] == 'Y'\n",
    "    demo['RACE_WHITE'] = demo['RACE'] == '05'\n",
    "    demo['RACE_BLACK'] = demo['RACE'] == '03'\n",
    "    demo = demo[['PATID', 'ENCOUNTERID', 'AGE', 'MALE', 'RACE_WHITE', 'RACE_BLACK', 'HISPANIC']]\n",
    "    demo = demo.drop_duplicates()\n",
    "    \n",
    "    return dx, demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf0293-fd6c-42ca-a5f8-6c331242405b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_scr = df_onset\n",
    "# df_admit = df_y\n",
    "# c7day = 'MOST_RECENT'\n",
    "# c365day = 'AVERAGE'\n",
    "# cckd = 'DROP'\n",
    "\n",
    "def get_scr_baseline_new(df_scr, df_admit, dx, demo, c7day = 'MOST_RECENT', c365day = 'AVERAGE', cckd = 'DROP'):\n",
    "#if True:\n",
    "    cohort_table = dict()\n",
    "    \n",
    "    # load & process dx data\n",
    "#    dx = pd.read_pickle(filepath_lst[0]+'AKI_DX.pkl')  \n",
    "    dx = dx[['PATID', 'ENCOUNTERID', 'DX', 'DX_DATE', 'DX_TYPE', 'DAYS_SINCE_ADMIT']] \n",
    "    dx = df_admit[['PATID', 'ENCOUNTERID', 'ADMIT_DATE']].merge(dx, on = ['PATID', 'ENCOUNTERID'], how = 'inner')\n",
    "#    dx['DAYS_SINCE_ADMIT'] = (dx['DX_DATE']-dx['ADMIT_DATE']).dt.days\n",
    "\n",
    "    # calculate DX_DATE when it is missing\n",
    "    dx['ADMIT_DATE'] = pd.to_datetime(dx['ADMIT_DATE'])\n",
    "    dx.loc[dx.DX_DATE.isna(), 'DX_DATE'] =  dx.loc[dx.DX_DATE.isna(), 'ADMIT_DATE'] +  pd.to_timedelta(dx.loc[dx.DX_DATE.isna(), 'DAYS_SINCE_ADMIT'], unit='D')\n",
    "\n",
    "    dx['DX'] = dx['DX'].astype(str)\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].astype(str)\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].replace('09', '9')\n",
    "    \n",
    "    # load & process demo data\n",
    "#    demo = pd.read_pickle(filepath_lst[0]+'AKI_DEMO'+'.pkl')  \n",
    "#    demo['MALE'] = demo['SEX'] == 'M'\n",
    "\n",
    "#    demo['RACE_WHITE'] = demo['RACE'] == '05'\n",
    "#    demo['RACE_BLACK'] = demo['RACE'] == '03'\n",
    "    demo = demo[['PATID', 'ENCOUNTERID', 'AGE', 'MALE', 'RACE_WHITE', 'RACE_BLACK']]\n",
    "    demo['AGE'] = demo['AGE'].astype(int)    \n",
    "    demo = demo.drop_duplicates()\n",
    "    \n",
    "    # estimate SCr Baseline\n",
    "    pat_id_cols = ['PATID', 'ENCOUNTERID']\n",
    "    complete_df = df_scr[['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    " \n",
    "    # 1. min between the min of 1-week prior admission SCr and within 24 hour after admission SCr\n",
    "    # SCr within 24 hour after admission, that is admission day and one day after, get mean\n",
    "    admission_SCr = complete_df[(complete_df.SPECIMEN_DATE >= complete_df.ADMIT_DATE) & \\\n",
    "                                (complete_df.SPECIMEN_DATE <= (complete_df.ADMIT_DATE + pd.Timedelta(days=1)))].copy()\n",
    "\n",
    "    # Admission SCr is the mean of all the SCr within 24h admission\n",
    "    admission_SCr = admission_SCr.groupby(pat_id_cols)['RESULT_NUM'].mean().reset_index()\n",
    "\n",
    "    admission_SCr.rename(columns = {'RESULT_NUM': 'ADMISSION_SCR'}, inplace = True)\n",
    "\n",
    "    # merge the ADMISSION_SCR back to the main frame\n",
    "    complete_df = complete_df.merge(admission_SCr, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    # SCr within 7 days prior to admission\n",
    "    one_week_prior_admission = complete_df[(complete_df.SPECIMEN_DATE >= complete_df.ADMIT_DATE - pd.Timedelta(days=7)) & \\\n",
    "                                           (complete_df.SPECIMEN_DATE < complete_df.ADMIT_DATE)].copy()\n",
    "    one_week_prior_admission = one_week_prior_admission.sort_values(by = ['PATID', 'ENCOUNTERID','SPECIMEN_DATE'])\n",
    "    \n",
    "    if c7day == 'MOST_RECENT':\n",
    "        one_week_prior_admission = one_week_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].last().reset_index()\n",
    "    else:\n",
    "        one_week_prior_admission = one_week_prior_admission.groupby(pat_id_cols)[\"RESULT_NUM\"].nanmin().reset_index()\n",
    "        \n",
    "    one_week_prior_admission = one_week_prior_admission.rename(columns = {'RESULT_NUM': 'ONE_WEEK_SCR'})\n",
    "\n",
    "    complete_df = complete_df.merge(one_week_prior_admission, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    # take the min between one week SCr and admission SCr\n",
    "    complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), 'BASELINE_EST_1'] = \\\n",
    "                np.nanmin(complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), ['ONE_WEEK_SCR','ADMISSION_SCR']], axis = 1)\n",
    "\n",
    "    complete_dfe = complete_df.drop(['SPECIMEN_DATE', 'RESULT_NUM'],axis=1).drop_duplicates()\n",
    "    cohort_table['ONE_WEEK_SCR_YES'] = complete_dfe.ONE_WEEK_SCR.notna().sum()\n",
    "    cohort_table['ONE_WEEK_SCR_NO'] = complete_dfe.ONE_WEEK_SCR.isna().sum()    \n",
    "    cohort_table['ONE_WEEK_SCR_ONE_WEEK_SCR'] = (complete_dfe.ONE_WEEK_SCR.notna() & (complete_dfe['ONE_WEEK_SCR']==complete_dfe['BASELINE_EST_1'])).sum()\n",
    "    cohort_table['ONE_WEEK_SCR_ADMISSION_SCR'] = (complete_dfe.ONE_WEEK_SCR.notna() & (complete_dfe['ONE_WEEK_SCR']!=complete_dfe['BASELINE_EST_1'])).sum()\n",
    "\n",
    "        \n",
    "    ori_num_unique_combinations = df_scr.groupby(['PATID', 'ENCOUNTERID']).ngroups\n",
    "    # get the percentage of encounters that do not have past 7-day records\n",
    "    criterion1_no_missing = complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), :].groupby(pat_id_cols).ngroups\n",
    "    criterion1_missing_rate = 1 - criterion1_no_missing / ori_num_unique_combinations\n",
    "\n",
    "    # 2. pre-admission 365-7 day mean\n",
    "    # here we only care about SCr measurements within 1 year before hospitalization\n",
    "    one_year_prior_admission = complete_df[(complete_df.SPECIMEN_DATE < (complete_df.ADMIT_DATE - pd.Timedelta(days=7))) & \\\n",
    "                                     (complete_df.SPECIMEN_DATE >= (complete_df.ADMIT_DATE - pd.Timedelta(days=365.25)))].copy()\n",
    "    one_year_prior_admission = one_year_prior_admission.sort_values(by = ['PATID', 'ENCOUNTERID','SPECIMEN_DATE'])\n",
    "    one_year_prior_admission = one_year_prior_admission.loc[:, pat_id_cols + ['RESULT_NUM']]\n",
    "    \n",
    "    if c365day == 'AVERAGE':\n",
    "        one_year_prior_admission = one_year_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].mean().reset_index()\n",
    "    else:\n",
    "        one_year_prior_admission = one_year_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].last().reset_index()  # or mean()\n",
    "    \n",
    "    one_year_prior_admission.rename(columns = {'RESULT_NUM': 'ONE_YEAR_SCR'}, inplace = True)\n",
    "    \n",
    "    complete_df = complete_df.merge(one_year_prior_admission, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "    \n",
    "    # take the min between one week SCr and admission SCr\n",
    "    complete_df.loc[complete_df.ONE_YEAR_SCR.notna(), 'BASELINE_EST_2'] = \\\n",
    "                np.nanmin(complete_df.loc[complete_df.ONE_YEAR_SCR.notna(), ['ONE_YEAR_SCR', 'ADMISSION_SCR']], axis = 1)\n",
    "\n",
    "    # priority 1: 7day SCr, priority 2: one year SCr\n",
    "    complete_df['BASELINE_NO_INVERT'] = \\\n",
    "                np.where(complete_df['BASELINE_EST_1'].isna(), complete_df['BASELINE_EST_2'], complete_df['BASELINE_EST_1'])\n",
    "\n",
    "    complete_dfe = complete_df.drop(['SPECIMEN_DATE', 'RESULT_NUM'],axis=1).drop_duplicates()\n",
    "    cohort_table['ONE_YEAR_SCR_YES'] = (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna()).sum()\n",
    "    cohort_table['ONE_YEAR_SCR_NO'] = (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.isna()).sum()\n",
    "    cohort_table['ONE_YEAR_SCR_ONE_WEEK_SCR'] = (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna() & (complete_dfe['ONE_YEAR_SCR']==complete_dfe['BASELINE_EST_2'])).sum()\n",
    "    cohort_table['ONE_YEAR_SCR_ADMISSION_SCR'] = (complete_dfe.ONE_WEEK_SCR.isna() & complete_dfe.ONE_YEAR_SCR.notna() & (complete_dfe['ONE_YEAR_SCR']!=complete_dfe['BASELINE_EST_2'])).sum()    \n",
    "    \n",
    "    # 3. Invert CKD-EPI (2021) to estimate baseline (only for non-CKD patients)\n",
    "    # get those encounters for which we need to impute baseline\n",
    "    pat_to_invert = complete_df.loc[complete_df.BASELINE_NO_INVERT.isna(), pat_id_cols+['ADMIT_DATE', 'ADMISSION_SCR']]\n",
    "    # one patient one row\n",
    "    pat_to_invert.drop_duplicates(subset=pat_id_cols, keep='first', inplace = True)\n",
    "\n",
    "\n",
    "    pat_dx = pat_to_invert.merge(dx.drop(['ENCOUNTERID', 'ADMIT_DATE'], axis = 1), \n",
    "                                              on = 'PATID', \n",
    "                                              how = 'left')\n",
    "\n",
    "\n",
    "    # check patients that do not have DX in the database\n",
    "    #pat_dx.DX_DATE.isna().mean()\n",
    "\n",
    "    # filter out those DX after admission\n",
    "    pat_dx = pat_dx[pat_dx.DX_DATE <= pat_dx.ADMIT_DATE]\n",
    "    \n",
    "#    pat_dx = pat_dx.merge(pat_to_invert[['PATID', 'ENCOUNTERID']], on = ['PATID', 'ENCOUNTERID'], how = 'outer')\n",
    "    \n",
    "    # get the default eGFR for inversion: default to 75 for non-CKD patients, average of eGFR from staging criteria for CKD patients\n",
    "    pat_dx['DFLT_eGFR'] = 75\n",
    "\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.3', 'N18.3']), 'DFLT_eGFR'] = 90/2\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.4', 'N18.4']), 'DFLT_eGFR'] = 45/2\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.5', 'N18.5']), 'DFLT_eGFR'] = 15/2\n",
    "#    pat_dx.loc[pat_dx['DX'].isin(['585.6', 'N18.6']), 'DFLT_eGFR'] = 15/2\n",
    "\n",
    "    pat_def_egfr = pat_dx.groupby(pat_id_cols)['DFLT_eGFR'].min().reset_index()\n",
    "        \n",
    "    pat_to_invert= pat_to_invert.merge(pat_def_egfr, on = pat_id_cols, how = 'left')\n",
    "    pat_to_invert['DFLT_eGFR'] = pat_to_invert['DFLT_eGFR'].fillna(75)\n",
    "\n",
    "    cohort_table['MDRD_NOCKD'] = (pat_to_invert['DFLT_eGFR'] == 75).sum()\n",
    "    cohort_table['MDRD_CKD3']  = (pat_to_invert['DFLT_eGFR'] == 90/2).sum()\n",
    "    cohort_table['MDRD_CKD4']  = (pat_to_invert['DFLT_eGFR'] == 45/2).sum()\n",
    "    cohort_table['MDRD_CKD5']  = (pat_to_invert['DFLT_eGFR'] == 15/2).sum()\n",
    "    \n",
    "    pat_to_invert['KEEPNONCKD'] = pat_to_invert['DFLT_eGFR'] == 75\n",
    "    \n",
    "    #pat_to_invert.DFLT_eGFR.value_counts()\n",
    "\n",
    "    # Backcalculation for patients\n",
    "    # merge DEMO with pat_to_invert\n",
    "    pat_to_invert = pat_to_invert.merge(demo, on = pat_id_cols, how = 'left')\n",
    "    \n",
    "    KDIGO_baseline = np.array([\n",
    "        [1.5, 1.3, 1.2, 1.0],\n",
    "        [1.5, 1.2, 1.1, 1.0],\n",
    "        [1.4, 1.2, 1.1, 0.9],\n",
    "        [1.3, 1.1, 1.0, 0.9],\n",
    "        [1.3, 1.1, 1.0, 0.8],\n",
    "        [1.2, 1.0, 0.9, 0.8]\n",
    "    ])\n",
    "    KDIGO_baseline = pd.DataFrame(KDIGO_baseline, columns = [\"Black males\", \"Other males\",\n",
    "                                                            \"Black females\", \"Other females\"],\n",
    "                                 index = [\"20-24\", \"25-29\", \"30-39\", \"40-54\", \"55-65\", \">65\"])    \n",
    "    \n",
    "    \n",
    "    # estimate SCr from eGFR\n",
    "    pat_to_invert.loc[:, 'BASELINE_INVERT'] = pat_to_invert.apply(inverse_MDRD, args = (KDIGO_baseline,), axis = 1) #pat_to_invert.apply(inverse_CKDEPI21, axis = 1)\n",
    "\n",
    "    # take minimum of inverted SCr and admission SCr\n",
    "    pat_to_invert['BASELINE_EST_3'] = np.nanmin(pat_to_invert[['ADMISSION_SCR', 'BASELINE_INVERT']], axis = 1)\n",
    "\n",
    "    # merge back the computation results\n",
    "    complete_df = complete_df.merge(pat_to_invert[pat_id_cols + ['BASELINE_EST_3', 'KEEPNONCKD']], \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    # replace the old baseline\n",
    "    complete_df['SERUM_CREAT_BASE'] = np.nanmin(complete_df[['BASELINE_NO_INVERT', 'BASELINE_EST_3']], axis = 1)\n",
    "\n",
    "    complete_ckd = complete_df[~(complete_df['KEEPNONCKD'].fillna(True) | complete_df['BASELINE_NO_INVERT'].notna())]\n",
    "        \n",
    "    if cckd == 'DROP':\n",
    "        complete_df = complete_df[complete_df['KEEPNONCKD'].fillna(True) | complete_df['BASELINE_NO_INVERT'].notna()]\n",
    "    complete_df = complete_df.drop('KEEPNONCKD', axis=1)\n",
    "\n",
    "    \n",
    "    # drop those still cannot find baseline\n",
    "    complete_df = complete_df.dropna(subset=['SERUM_CREAT_BASE'])\n",
    "\n",
    "    return complete_df.drop_duplicates(), cohort_table, complete_ckd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9f57c-a0eb-437f-8668-288ab2a72160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dx, demo = get_scr_baseline_old(path, df_onset, df_y, ext, sep)\n",
    "dx.to_pickle(pdata+'/dx00.pkl')\n",
    "demo.to_pickle(pdata+'/demo00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09adc0a0-eb71-4da8-99fc-b33ebb8719f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dx, demo = get_scr_baseline(path, df_onset, df_y, ext, sep)\n",
    "# dx.to_pickle(pdata+'/dx00.pkl')\n",
    "# demo.to_pickle(pdata+'/demo00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9320e-e216-4b41-8a6a-5def0bfddb52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_onset = pd.read_pickle(pdata+'/scr00.pkl')\n",
    "# df_y = pd.read_pickle(pdata+'/admit00.pkl')\n",
    "df_y['PATID'] = df_y['PATID'].astype(str)\n",
    "df_y['ENCOUNTERID'] = df_y['ENCOUNTERID'].astype(str)\n",
    "df_onset['PATID'] = df_onset['PATID'].astype(str)\n",
    "df_onset['ENCOUNTERID'] = df_onset['ENCOUNTERID'].astype(str)\n",
    "# dx = pd.read_pickle(pdata+'/dx00.pkl')\n",
    "# demo = pd.read_pickle(pdata+'/demo00.pkl')\n",
    "dx['PATID'] = dx['PATID'].astype(str)\n",
    "dx['ENCOUNTERID'] = dx['ENCOUNTERID'].astype(str)\n",
    "demo['PATID'] = demo['PATID'].astype(str)\n",
    "demo['ENCOUNTERID'] = demo['ENCOUNTERID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67473d6c-b880-4ce2-80a0-8eb719140906",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a64fa-178c-4df0-8a3c-a843ab7a3b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_base = get_scr_baseline2(path, df_onset, df_y,  ext, sep, dx, demo)\n",
    "df_base, cohort_table, complete_ckd = get_scr_baseline_new(df_onset, df_y, dx, demo, c7day = 'MOST_RECENT', c365day = 'AVERAGE', cckd = 'DROP')\n",
    "df_base.to_pickle(pdata+'/df_base00.pkl')\n",
    "\n",
    "cohort_table['TOTAL'] = cohort_table['ONE_WEEK_SCR_YES'] + cohort_table['ONE_WEEK_SCR_NO']\n",
    "cohort_table['site'] = site\n",
    "site_column = cohort_table.pop('site')\n",
    "cohort_table_df = pd.DataFrame(list(cohort_table.items()), columns=['Category', site_column])\n",
    "cohort_table_df.index = cohort_table_df['Category']\n",
    "cohort_table_df = cohort_table_df.drop('Category', axis=1)\n",
    "cohort_table_df.to_pickle(pdata+'/cohort_table00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251e731-fe78-47c9-880d-f4843cca7e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cohort_table_df = pd.read_pickle(pdata+'/cohort_table00.pkl')\n",
    "cohort_table_df.loc['MDRD_CKD3',]+cohort_table_df.loc['MDRD_CKD4',]+cohort_table_df.loc['MDRD_CKD5',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f0bef6-290d-4118-b5c1-9609a06464e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cohort_table_df.loc[['TOTAL','ONE_WEEK_SCR_YES', 'ONE_WEEK_SCR_NO', 'ONE_YEAR_SCR_YES', 'ONE_YEAR_SCR_NO', 'MDRD_NOCKD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876c218-312f-48a8-b136-9d64a38f2d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rrt(path, ext, sep, df_y):\n",
    "    px = try_load_csv('AKI_PX', ext, sep, path) # pd.read_csv(path+\"AKI_PX.\"+ext, engine=\"pyarrow\", sep= sep)\n",
    "    px.columns = px.columns.str.upper()\n",
    "    px.rename(columns = {'PX_DATE\"+PD.DATE_SHIFT\"': 'PX_DATE'}, inplace = True)\n",
    "    px[\"ENCOUNTERID\"] = px[\"ONSETS_ENCOUNTERID\"]\n",
    "    px['PATID'] = px['PATID'].astype(str)\n",
    "    px['ENCOUNTERID'] = px['ENCOUNTERID'].astype(str)\n",
    "    \n",
    "    \n",
    "    idx_transplant = np.logical_or(np.logical_or(\n",
    "                           np.logical_and(px['PX_TYPE']=='CH',px['PX'].isin(['50300','50320','50323','50325','50327','50328','50329','50340','50360','50365','50370','50380'])),\n",
    "                           np.logical_and(px['PX_TYPE']=='09',px['PX'].isin(['55.51','55.52','55.53','55.54','55.61','55.69']))),np.logical_or(\n",
    "                           np.logical_and(px['PX_TYPE']=='9',px['PX'].isin(['55.51','55.52','55.53','55.54','55.61','55.69'])),                       \n",
    "                           np.logical_and(px['PX_TYPE']=='10',px['PX'].isin(['0TY00Z0','0TY00Z1','0TY00Z2','0TY10Z0','0TY10Z1','0TY10Z2','0TB00ZZ','0TB10ZZ','0TT00ZZ','0TT10ZZ','0TT20ZZ']))))\n",
    "\n",
    "    idx_dialysis =( ( (px['PX_TYPE']=='CH')  &  (px['PX'].isin(['90935', '90937'])))  |   # ( (px['PX_TYPE']=='CH')  & \n",
    "        ( (px['PX_TYPE']=='CH')  & (pd.to_numeric(px['PX'], errors='coerce').between(90940, 90999))) |   #(px['PX_TYPE']=='CH')  &\n",
    "        ( (px['PX_TYPE']=='9')  & ( (px['PX'].isin(['39.93','39.95','54.98', 'V45.11']))) )  |  #(px['PX_TYPE']=='9')  &\n",
    "        ( (px['PX_TYPE']=='09')  & (px['PX'].isin(['39.93','39.95','54.98', 'V45.11'])) ) |  #(px['PX_TYPE']=='09')  &\n",
    "        ( (px['PX_TYPE']=='10')  & (px['PX'].isin(['5A1D00Z','5A1D60Z','5A1D70Z','5A1D80Z','5A1D90Z', 'Z99.2']))) )  #(px['PX_TYPE']=='10')  &\n",
    "   \n",
    " \n",
    "    rrt_stage =  px[idx_transplant | idx_dialysis] \n",
    "\n",
    "    \n",
    "    rrt_stage['PX_DATE'] = pd.to_datetime(pd.to_datetime(rrt_stage['PX_DATE']).dt.date )\n",
    "    df_y['ADMIT_DATE'] = pd.to_datetime(pd.to_datetime(df_y['ADMIT_DATE']).dt.date ) \n",
    "    rrt_stage = rrt_stage[['PATID','ENCOUNTERID','PX_DATE']]\n",
    "    rrt_stage.columns = ['PATID','ENCOUNTERID','RRT3_ONSET_DATE']\n",
    "    rrt_stage['RRT3_ONSET_DATE'] = pd.to_datetime(pd.to_datetime(rrt_stage['RRT3_ONSET_DATE']).dt.date )\n",
    "\n",
    "    rrt_stage = rrt_stage.merge(df_y, on=['PATID', 'ENCOUNTERID'], how='inner')\n",
    "    \n",
    "    rrt_stage['RRT3_ONSET_DATE'] = pd.to_datetime(rrt_stage['RRT3_ONSET_DATE'])\n",
    "    rrt_stage['ADMIT_DATE'] = pd.to_datetime(rrt_stage['ADMIT_DATE'])\n",
    "    rrt_stage['RRT3_SINCE_ADMIT'] = (rrt_stage['RRT3_ONSET_DATE']-rrt_stage['ADMIT_DATE']).dt.total_seconds()/(3600*24)\n",
    "    rrt_stage = rrt_stage.loc[rrt_stage[['ENCOUNTERID', 'RRT3_SINCE_ADMIT']].groupby('ENCOUNTERID').idxmin().reset_index()['RRT3_SINCE_ADMIT']]\n",
    "    rrt_stage.drop('ADMIT_DATE', axis = 1, inplace = True)\n",
    "    return rrt_stage, px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f88d39-a3d9-4e14-a897-7ca83e875403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_rrt, px = get_rrt(path, ext, sep, df_y)\n",
    "#df_rrt.to_pickle(pdata+'/df_rrt00.pkl')\n",
    "#px.to_pickle(pdata+'/px.pkl')\n",
    "\n",
    "df_rrt = pd.read_pickle(pdata+'/df_rrt00.pkl')\n",
    "px = pd.read_pickle(pdata+'/px.pkl')\n",
    "#df_rrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e197ee88-d28f-4330-81b9-24321dabd29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_onset_new(df_onset, df_y, df_rrt, df_base, part = 'either'):\n",
    "    xxx = df_onset.copy()\n",
    "    yyy = df_y.copy()\n",
    "\n",
    "    zzz = df_base[['PATID', 'ENCOUNTERID', 'SERUM_CREAT_BASE']].drop_duplicates()\n",
    "    \n",
    "    # zzz['SERUM_CREAT_BASE'] = zzz['SERUM_CREAT_BASE'].round(4)\n",
    "    # xxx['RESULT_NUM'] = xxx['RESULT_NUM'].round(4)\n",
    "    \n",
    "    zzz.columns= ['PATID', 'ENCOUNTERID',  'RESULT_NUM_BASE_7d']\n",
    "    xxx = xxx.merge(zzz, on = ['PATID', 'ENCOUNTERID'], how='inner')\n",
    "\n",
    "    zzz2 = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']].groupby(['PATID', 'ENCOUNTERID']).rolling('2d', on='SPECIMEN_DATE').min().reset_index()\n",
    "    zzz2 = zzz2[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    "    zzz2.columns= ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM_BASE_2d']\n",
    "    xxx = xxx.merge(zzz2, on = ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'], how='left')\n",
    "\n",
    "    #0.3 increase in 48 hours\n",
    "    #1.5 increase in 7 days\n",
    "    xxx['AKI1.5'] = (xxx['RESULT_NUM']>=1.5*xxx['RESULT_NUM_BASE_7d']) & (xxx['DAYS_SINCE_ADMIT']>=0)                        \n",
    "    xxx['AKI0.3'] = (xxx['RESULT_NUM']-xxx['RESULT_NUM_BASE_2d']>=0.3) & (xxx['DAYS_SINCE_ADMIT']>=0)      \n",
    "    \n",
    "    if part == '2d':\n",
    "        xxx = xxx[xxx['AKI0.3']]\n",
    "        xxx = xxx.sort_values('SPECIMEN_DATE')\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "        \n",
    "    elif part == '7d':\n",
    "        xxx = xxx[xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values('SPECIMEN_DATE')\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "\n",
    "    elif part == 'both':\n",
    "        xxx = xxx[xxx['AKI0.3'] & xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values('SPECIMEN_DATE')\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "        \n",
    "    elif part == 'either':\n",
    "        xxx = xxx[xxx['AKI0.3'] | xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "\n",
    "    #xxx = xxx.apply(set_result_base,axis=1)\n",
    "    xxx['AKI1_SINCE_ADMIT'] = xxx['DAYS_SINCE_ADMIT'].copy()\n",
    "    xxx['AKI1_DATE'] = xxx['SPECIMEN_DATE'].copy()\n",
    "    #xxx['AKI1_DATETIME'] = xxx['SPECIMEN_DATETIME'].copy()\n",
    "    xxx['AKI1_SCR'] = xxx['RESULT_NUM'].copy()\n",
    "    xxx['SCR_BASELINE'] = xxx['RESULT_NUM_BASE'].copy()\n",
    "    xxx['SCR_REFERENCE'] = xxx['RESULT_NUM_BASE_2d'].copy()\n",
    "    xxx['AKI1_7D'] = xxx['AKI1.5'].copy()\n",
    "    xxx['AKI1_2D'] = xxx['AKI0.3'].copy()\n",
    "    xxx = xxx[['PATID', 'ENCOUNTERID', 'SCR_BASELINE', 'SCR_REFERENCE',  'AKI1_DATE', 'AKI1_SCR', 'AKI1_SINCE_ADMIT', 'AKI1_7D', 'AKI1_2D']]\n",
    "\n",
    "   \n",
    "    # Check condition for AKI2: 2.0x - <3.0x\n",
    "    aki2 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    \n",
    "    # print(aki2)\n",
    "    # print(aki2['RESULT_NUM']>=2*aki2['SCR_BASELINE'])\n",
    "    # print(aki2['RESULT_NUM'])\n",
    "    # print(aki2['RESULT_NUM'].iloc[0])\n",
    "    # print(aki2['SCR_BASELINE'].iloc[0])\n",
    "    \n",
    "    aki2 = aki2[aki2['SPECIMEN_DATE']>=aki2['AKI1_DATE']]\n",
    "    aki2 = aki2[aki2['RESULT_NUM']>=2*aki2['SCR_BASELINE']]\n",
    "    aki2 = aki2.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki2['AKI2_SINCE_ADMIT'] = aki2['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki2['AKI2_DATE'] = aki2['SPECIMEN_DATE'].copy()\n",
    "    #aki2['AKI2_DATETIME'] = aki2['SPECIMEN_DATETIME'].copy()\n",
    "    aki2['AKI2_SCR'] = aki2['RESULT_NUM'].copy()\n",
    "    aki2 = aki2[['PATID', 'ENCOUNTERID', 'AKI2_DATE', 'AKI2_SCR', 'AKI2_SINCE_ADMIT']]\n",
    "    \n",
    "    # Check condition for AKI3: SCR >= 3.0x Baseline\n",
    "    aki3 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki3 = aki3[aki3['SPECIMEN_DATE']>=aki3['AKI1_DATE']]\n",
    "    aki3 = aki3[(aki3['RESULT_NUM']>=3*aki3['SCR_BASELINE']) | (aki3['RESULT_NUM']>=4)]\n",
    "    aki3 = aki3.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki3['AKI3_SINCE_ADMIT'] = aki3['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki3['AKI3_DATE'] = aki3['SPECIMEN_DATE'].copy()\n",
    "    #aki3['AKI3_DATETIME'] = aki3['SPECIMEN_DATETIME'].copy()\n",
    "    aki3['AKI3_SCR'] = aki3['RESULT_NUM'].copy()\n",
    "    aki3 = aki3[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    \n",
    "    # Check condition for AKI3: initiation of RRT\n",
    "    #df_rrt = get_rrt(path, ext, sep, yyy)\n",
    "    rrt = df_rrt.merge(xxx[['PATID', 'ENCOUNTERID', 'AKI1_DATE']], on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    rrt = rrt[rrt['RRT3_ONSET_DATE'] >= rrt['AKI1_DATE']]\n",
    "    aki3b =  aki3.merge(rrt, on = ['PATID', 'ENCOUNTERID'], how = 'outer')\n",
    "    cond_rrt = (aki3b['RRT3_SINCE_ADMIT'] < aki3b['AKI3_SINCE_ADMIT']) | (aki3b['AKI3_SINCE_ADMIT'].isna() & aki3b['RRT3_SINCE_ADMIT'].notna())\n",
    "    aki3b.loc[cond_rrt, 'AKI3_SINCE_ADMIT'] = aki3b.loc[cond_rrt, 'RRT3_SINCE_ADMIT']\n",
    "    aki3b.loc[cond_rrt, 'AKI3_DATE'] = aki3b.loc[cond_rrt, 'RRT3_ONSET_DATE']\n",
    "    #aki3b.loc[cond_rrt, 'AKI3_DATETIME'] = aki3b.loc[cond_rrt, 'RRT3_ONSET_DATETIME']\n",
    "    \n",
    "    aki3_all = aki3b[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    \n",
    "    onset = xxx.merge(aki2, on=['PATID', 'ENCOUNTERID'], how='outer').merge(aki3_all, on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    onset = onset.merge(yyy, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "    onset.columns = onset.columns.str.upper()\n",
    "    onset['ONSET_DATE'] = onset['AKI1_DATE'].copy()  \n",
    "    \n",
    "    onset['SCR_ONSET'] = onset['AKI1_SCR'].copy() \n",
    "\n",
    "    onset = onset[[\"PATID\",\"ENCOUNTERID\", \"ADMIT_DATE\", \n",
    "                   'ONSET_DATE', \"AKI1_SINCE_ADMIT\", \"AKI2_SINCE_ADMIT\", \n",
    "                   \"AKI3_SINCE_ADMIT\", \"SCR_ONSET\", \n",
    "                   'SCR_BASELINE',  'SCR_REFERENCE', 'AKI1_7D', 'AKI1_2D']]\n",
    "\n",
    "    onset['FLAG'] = (onset['AKI2_SINCE_ADMIT'].notnull()) | (onset['AKI3_SINCE_ADMIT'].notnull())\n",
    "    onset['ONSET_SINCE_ADMIT'] = onset[\"AKI1_SINCE_ADMIT\"].copy()  #onset[[\"AKI1_SINCE_ADMIT\", \"AKI2_SINCE_ADMIT\", \"AKI3_SINCE_ADMIT\"]].min(axis=1)\n",
    "        \n",
    "    \n",
    "    onset['AKI_STAGE'] = 0\n",
    "    \n",
    "    aki_s3 = onset['AKI3_SINCE_ADMIT'].notna()\n",
    "    aki_s2 = onset['AKI2_SINCE_ADMIT'].notna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    aki_s1 = onset['AKI1_SINCE_ADMIT'].notna() & onset['AKI2_SINCE_ADMIT'].isna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    \n",
    "    onset.loc[aki_s3, 'AKI_STAGE'] = 3\n",
    "    onset.loc[aki_s2, 'AKI_STAGE'] = 2\n",
    "    onset.loc[aki_s1, 'AKI_STAGE'] = 1\n",
    "    \n",
    "    return onset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638d3e9-af17-471b-bde6-3ed23ccb5adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onset = read_onset_new(df_onset = df_onset, df_y = df_y, df_rrt= df_rrt, df_base = df_base, part = 'either')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ec0c3-1ddf-44b9-b8d0-7361395adc90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onset[['ENCOUNTERID', 'AKI_STAGE']].groupby('AKI_STAGE').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a0fc7-d7a7-4f81-a05a-f6946eefd52d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onset.to_pickle(pdata+'/onset00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f394a6-1263-4871-b631-f9d8e35ff803",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset[['AKI_STAGE','ENCOUNTERID']].drop_duplicates().groupby('AKI_STAGE').count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1bdf1-990f-4702-94e8-369b1ece1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_table_df.loc['MDRD_CKD3',]+cohort_table_df.loc['MDRD_CKD4',]+cohort_table_df.loc['MDRD_CKD5',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225effc-9ea3-45b7-85f4-aa2c1f2a701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aki_onset2(df_scr, df_admit, df_rrt, df_baseline, aki_criteria = 'either'):\n",
    "    xxx = df_scr.copy()\n",
    "    yyy = df_admit.copy()\n",
    "    ##### Filter out the CKD patients that does not have baseline \n",
    "    xxx = xxx[xxx.ENCOUNTERID.isin(df_baseline.ENCOUNTERID.unique())]  \n",
    "    yyy = yyy[yyy.ENCOUNTERID.isin(df_baseline.ENCOUNTERID.unique())]  \n",
    "    ######\n",
    "    zzz = df_baseline[['PATID', 'ENCOUNTERID', 'SERUM_CREAT_BASE']].drop_duplicates()\n",
    "    zzz.columns= ['PATID', 'ENCOUNTERID',  'RESULT_NUM_BASE_7d']\n",
    "    xxx = xxx.merge(zzz, on = ['PATID', 'ENCOUNTERID'], how='left')\n",
    " \n",
    "    zzz2 = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']].groupby(['PATID', 'ENCOUNTERID']).rolling('2d', on='SPECIMEN_DATE').min().reset_index()\n",
    "    zzz2 = zzz2[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    "    zzz2.columns= ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM_BASE_2d']\n",
    "    xxx = xxx.merge(zzz2, on = ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'], how='left')\n",
    " \n",
    "    # Check condition for AKI1\n",
    "    #1.5 increase in 7 days\n",
    "    xxx['AKI1.5'] = (xxx['RESULT_NUM']>=1.5*xxx['RESULT_NUM_BASE_7d']) & (xxx['DAYS_SINCE_ADMIT']>=0) \n",
    "    #0.3 increase in 48 hours\n",
    "    xxx['AKI0.3'] = (xxx['RESULT_NUM']-xxx['RESULT_NUM_BASE_2d']>=0.3) & (xxx['DAYS_SINCE_ADMIT']>=0)      \n",
    "    if aki_criteria == '2d':\n",
    "        xxx = xxx[xxx['AKI0.3']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "    elif aki_criteria == '7d':\n",
    "        xxx = xxx[xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    " \n",
    "    elif aki_criteria == 'both':\n",
    "        xxx = xxx[xxx['AKI0.3'] & xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "    elif aki_criteria == 'either':\n",
    "        xxx = xxx[xxx['AKI0.3'] | xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    " \n",
    "    xxx['AKI1_SINCE_ADMIT'] = xxx['DAYS_SINCE_ADMIT'].copy()\n",
    "    xxx['AKI1_DATE'] = xxx['SPECIMEN_DATE'].copy()\n",
    "    xxx['AKI1_SCR'] = xxx['RESULT_NUM'].copy()\n",
    "    xxx['SCR_BASELINE'] = xxx['RESULT_NUM_BASE'].copy()\n",
    "    xxx['SCR_REFERENCE'] = xxx['RESULT_NUM_BASE_2d'].copy()\n",
    "    xxx['AKI1_7D'] = xxx['AKI1.5'].copy()\n",
    "    xxx['AKI1_2D'] = xxx['AKI0.3'].copy()\n",
    "    xxx = xxx[['PATID', 'ENCOUNTERID', 'SCR_BASELINE', 'SCR_REFERENCE',  'AKI1_DATE', 'AKI1_SCR', 'AKI1_SINCE_ADMIT', 'AKI1_7D', 'AKI1_2D']]\n",
    " \n",
    "    # Check condition for AKI2: 2.0x - <3.0x\n",
    "    aki2 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki2 = aki2[aki2['SPECIMEN_DATE']>=aki2['AKI1_DATE']]\n",
    "    aki2 = aki2[aki2['RESULT_NUM']>=2*aki2['SCR_BASELINE']]\n",
    "    aki2 = aki2.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki2['AKI2_SINCE_ADMIT'] = aki2['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki2['AKI2_DATE'] = aki2['SPECIMEN_DATE'].copy()\n",
    "    aki2['AKI2_SCR'] = aki2['RESULT_NUM'].copy()\n",
    "    aki2 = aki2[['PATID', 'ENCOUNTERID', 'AKI2_DATE', 'AKI2_SCR', 'AKI2_SINCE_ADMIT']]\n",
    "    # Check condition for AKI3: SCR >= 3.0x Baseline\n",
    "    aki3 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki3 = aki3[aki3['SPECIMEN_DATE']>=aki3['AKI1_DATE']]\n",
    "    aki3 = aki3[(aki3['RESULT_NUM']>=3*aki3['SCR_BASELINE']) | (aki3['RESULT_NUM']>=4)]\n",
    "    aki3 = aki3.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki3['AKI3_SINCE_ADMIT'] = aki3['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki3['AKI3_DATE'] = aki3['SPECIMEN_DATE'].copy()\n",
    "    aki3['AKI3_SCR'] = aki3['RESULT_NUM'].copy()\n",
    "    aki3 = aki3[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    # Check condition for AKI3: initiation of RRT\n",
    "    #df_rrt = get_rrt(path, ext, sep, yyy)\n",
    "    rrt = df_rrt.merge(xxx[['PATID', 'ENCOUNTERID', 'AKI1_DATE']], on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    rrt = rrt[rrt['RRT3_ONSET_DATE'] >= rrt['AKI1_DATE']]\n",
    "    aki3b =  aki3.merge(rrt, on = ['PATID', 'ENCOUNTERID'], how = 'outer')\n",
    "    cond_rrt = (aki3b['RRT3_SINCE_ADMIT'] < aki3b['AKI3_SINCE_ADMIT']) | (aki3b['AKI3_SINCE_ADMIT'].isna() & aki3b['RRT3_SINCE_ADMIT'].notna())\n",
    "    aki3b.loc[cond_rrt, 'AKI3_SINCE_ADMIT'] = aki3b.loc[cond_rrt, 'RRT3_SINCE_ADMIT']\n",
    "    aki3b.loc[cond_rrt, 'AKI3_DATE'] = aki3b.loc[cond_rrt, 'RRT3_ONSET_DATE']\n",
    "    aki3_all = aki3b[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    # Merge AKI staging information\n",
    "    onset = xxx.merge(aki2, on=['PATID', 'ENCOUNTERID'], how='outer').merge(aki3_all, on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    onset = onset.merge(yyy, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    " \n",
    "    onset.columns = onset.columns.str.upper()\n",
    "    onset['ONSET_DATE'] = onset['AKI1_DATE'].copy()  \n",
    "    onset['SCR_ONSET'] = onset['AKI1_SCR'].copy() \n",
    "#    onset['DISCHARGE_SINCE_ONSET'] = (onset['DISCHARGE_DATE'] - onset['ONSET_DATE']).dt.days\n",
    "    onset['DISCHARGE_SINCE_ONSET'] = 0\n",
    "    onset['DISCHARGE_DATE'] = 0    \n",
    "    onset = onset[['PATID','ENCOUNTERID', 'ADMIT_DATE', 'DISCHARGE_DATE', \n",
    "                   'ONSET_DATE', 'AKI1_SINCE_ADMIT', 'AKI2_SINCE_ADMIT', \n",
    "                   'AKI3_SINCE_ADMIT',  'DISCHARGE_SINCE_ONSET','SCR_ONSET', \n",
    "                   'SCR_BASELINE',  'SCR_REFERENCE', 'AKI1_7D', 'AKI1_2D']]\n",
    " \n",
    "    onset['FLAG'] = (onset['AKI2_SINCE_ADMIT'].notna()) | (onset['AKI3_SINCE_ADMIT'].notna())\n",
    "    onset['ONSET_SINCE_ADMIT'] = onset['AKI1_SINCE_ADMIT'].copy()  #onset[['AKI1_SINCE_ADMIT', 'AKI2_SINCE_ADMIT', 'AKI3_SINCE_ADMIT']].min(axis=1)\n",
    "    #Generate onset staging by taking the first stage\n",
    "    onset['AKI_STAGE'] = 0\n",
    "    filter_aki3 = onset['AKI3_SINCE_ADMIT'].notna()\n",
    "    filter_aki2 = onset['AKI2_SINCE_ADMIT'].notna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    filter_aki1 = onset['AKI1_SINCE_ADMIT'].notna() & onset['AKI2_SINCE_ADMIT'].isna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    onset.loc[filter_aki3, 'AKI_STAGE'] = 3\n",
    "    onset.loc[filter_aki2, 'AKI_STAGE'] = 2\n",
    "    onset.loc[filter_aki1, 'AKI_STAGE'] = 1\n",
    "\n",
    "    # Determine the initial AKI stage by finding the column with the smallest onset day\n",
    "    #onset['AKI_INIT_STG'] = onset.apply(determine_initial_aki_stage, axis=1)\n",
    "    #onset3 = onset2.sort_values('ONSET_SINCE_ADMIT').groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    return onset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957dc10-96b6-4d91-8280-194d5b1631ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onset2 = get_aki_onset2(df_scr = df_onset, df_admit = df_y, df_rrt = df_rrt, df_baseline = df_base, aki_criteria = 'either')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c30f5-8920-485a-bc3d-f26e5c85b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset2[['AKI_STAGE','ENCOUNTERID']].drop_duplicates().groupby('AKI_STAGE').count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be37431-846e-4012-89fd-3ab37c0e9b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_PM_TEMPORAL_MOEA",
   "language": "python",
   "name": "aki_pm_temporal_moea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
