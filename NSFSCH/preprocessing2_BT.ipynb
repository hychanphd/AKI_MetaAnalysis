{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f485a-31dd-482c-b8ea-e3ad018edc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4f969-90f0-4819-9046-499aad2fe9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    site = 'MCRI'\n",
    "    year = 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799e9f8-05fc-41b8-abf3-fce92d3e42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_onset(site, year):\n",
    "    #onset\n",
    "    print('Merging onset on site '+site+\":\"+str(year), flush = True)    \n",
    "    try:\n",
    "        return pd.read_pickle('data/'+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "#        newdf_debug['onset'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No onset table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No onset table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0627c-5d5c-4e5b-9ffb-2e90d44ee329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_px(site, year, newdf):        \n",
    "    #px \n",
    "    print('Merging px on site '+site+\":\"+str(year), flush = True)        \n",
    "    try:\n",
    "        px = pd.read_pickle('data/'+site+'/px_'+site+'_'+str(year)+'.pkl')\n",
    "        #depreciate Since ADMIT\n",
    "#        newdf = pd.merge(newdf, px, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "        return pd.merge(newdf, px, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna(False)\n",
    "#        newdf_debug['px'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No px table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No px table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e9d6f-36d7-4062-aabd-73b1485793c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_dx(site, year, newdf):                \n",
    "    #dx\n",
    "    print('Merging dx on site '+site+\":\"+str(year), flush = True)            \n",
    "    try:\n",
    "        dx = pd.read_pickle('data/'+site+'/dx_'+site+'_'+str(year)+'.pkl')\n",
    "#        newdf = pd.merge(newdf, dx, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "        return pd.merge(newdf, dx, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna(False)       \n",
    "#        newdf_debug['dx'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No onset table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No onset table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6181b15-8b9f-40a6-8864-e857c659828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_amed(site, year, newdf):                    \n",
    "    #amed\n",
    "    print('Merging amed on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "        amed = pd.read_pickle('data/'+site+'/amed_'+site+'_'+str(year)+'.pkl')\n",
    "#        newdf = pd.merge(newdf, amed, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "        return pd.merge(newdf, amed, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna(False)        \n",
    "#        newdf = newdf.combine_first(newdf[list(amed.select_dtypes('bool').columns)].fillna(False))    \n",
    "#        newdf_debug['amed'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No amed table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No amed table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()    \n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08cfab5-89b0-4293-85ee-030526543362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_labcat(site, year, newdf):                        \n",
    "    #labcat\n",
    "    print('Merging lab_cat on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "        labcat = pd.read_pickle('data/'+site+'/labcat_'+site+'_'+str(year)+'.pkl')\n",
    "#        newdf = pd.merge(newdf, amed, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "        return pd.merge(newdf, labcat, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna(False)        \n",
    "#        newdf = newdf.combine_first(newdf[list(amed.select_dtypes('bool').columns)].fillna(False))    \n",
    "#        newdf_debug['amed'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No lab_cat table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No lab_cat table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()          \n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd28c5b-cfbd-4d98-86ed-d9686cbacaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bt_demo(site, year, newdf):                            \n",
    "    #demo\n",
    "    print('Merging demo on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "        demo = pd.read_pickle('data/'+site+'/demo_'+site+'_'+str(year)+'.pkl')\n",
    "        newdf2 = pd.merge(newdf, demo, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "        return newdf2.combine_first(newdf2[list(demo.select_dtypes('bool').columns)].fillna(False))            \n",
    "#        newdf_debug['demo'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No demo table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No demo table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()    \n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2db93-4a80-467c-85fd-779d3635422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_vital(site, year, newdf):                                \n",
    "    #vital\n",
    "    print('Merging vital on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "#        vital = pd.read_pickle('data/'+site+'/vital_'+site+'_'+str(year)+'.pkl')\n",
    "        vital = pd.read_pickle('data/'+site+'/vital_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "#        newdf = pd.merge(newdf, vital, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "        return pd.merge(newdf, vital, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left')        \n",
    "#        newdf_debug['vital'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No vital table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No vital table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fda8cd-c776-430a-a28f-2adfcf289862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_labnum(site, year, newdf):                                    \n",
    "    #lab_num\n",
    "    print('Merging lab_num on site '+site+\":\"+str(year), flush = True)                \n",
    "    try:\n",
    "#        labnum = pd.read_pickle('data/'+site+'/labnum_'+site+'_'+str(year)+'.pkl')\n",
    "        labnum = pd.read_pickle('data/'+site+'/labnum_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "#        newdf = pd.merge(newdf, lab_t, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')   \n",
    "        return pd.merge(newdf, labnum, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "#        newdf_debug['lab'] = newdf.copy()\n",
    "    except FileNotFoundError:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('No lab_num table!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('No lab_num table!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "        return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb52972e-f302-405d-968a-1242a70173ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_too_much_nan(site, year, newdf, threshold=0.05):\n",
    "    print('Remove sparse feature on site '+site+\":\"+str(year), flush = True)                        \n",
    "    btX = newdf.replace(False, np.nan)\n",
    "    #limitPer = len(btX) * threshold\n",
    "    #col = btX.dropna(thresh=limitPer, axis=1).columns\n",
    "    btX0 = btX[btX['FLAG']==0]\n",
    "    btX1 = btX[btX['FLAG']==1]\n",
    "    limitPer0 = len(btX0) * threshold\n",
    "    limitPer1 = len(btX1) * threshold\n",
    "    col0 = btX0.dropna(thresh=limitPer0, axis=1).columns\n",
    "    col1 = btX1.dropna(thresh=limitPer1, axis=1).columns\n",
    "    col = list(set(list(col1)+list(col0)))\n",
    "    return newdf[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91facabb-5217-4777-93c0-e52f7ddac18a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handpickremoval(site, year, newdf):   \n",
    "    # drop CPT service code between 99202 and 99499 \n",
    "    cptcode0 = np.array([x for x in newdf.columns if 'PX' in x and x.split(':')[2].isnumeric()])\n",
    "    cptcode = np.array([int(x.split(':')[2]) if x.split(':')[2].isnumeric() else 0 for x in cptcode0])\n",
    "    cptcodebool = np.logical_or(np.logical_and(cptcode >= 99202, cptcode <= 99499),np.logical_and(cptcode >= 80047, cptcode <= 89398))\n",
    "    remlist = cptcode0[cptcodebool]\n",
    "    \n",
    "    # Additional drop\n",
    "    remlist2 = ['WT']\n",
    "    \n",
    "    remlist = list(remlist)+remlist2\n",
    "    return newdf.drop(remlist,axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86e42d-a85a-40d1-aee2-f4374a985d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def drop_corr(site, year, newdf, threshold=0.5):\n",
    "#     print('Remove correlated feature on site '+site+\":\"+str(year), flush = True)                        \n",
    "#     corr = newdf.corr()\n",
    "#     columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "#     for i in range(corr.shape[0]):\n",
    "#         for j in range(i+1, corr.shape[0]):\n",
    "#             if corr.iloc[i,j] >= threshold:\n",
    "#                 # if corr.columns[j] == 'ORIGINAL_BMI':\n",
    "#                 #     if columns[i]:\n",
    "#                 #         columns[i] = False\n",
    "#                 if columns[j]:\n",
    "#                     columns[j] = False\n",
    "#     selected_columns = newdf.columns[columns]\n",
    "#     return newdf[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41959950-173c-4696-a6d7-8ef5b4b02cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pearson\n",
    "def pearson_list(bt, threshold):\n",
    "    corr = bt.corr()\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i,j] >= threshold:\n",
    "#                print(bt.columns[i], btcont.columns[j], corr.iloc[i,j])\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    return columns   \n",
    "\n",
    "def point_biserial(btcat, btcon, threshold):\n",
    "    from scipy import stats\n",
    "    columns = np.full((btcat.shape[1],), True, dtype=bool)    \n",
    "    for i in range(btcon.shape[1]):\n",
    "        for j in range(btcat.shape[1]):\n",
    "            if stats.pointbiserialr(btcat.iloc[:,j], btcon.iloc[:,i])[0] >= threshold:            \n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    return columns\n",
    "    \n",
    "def drop_corr(site, year, bt, threshold):\n",
    "    bt2 = bt.reindex(sorted(bt.columns), axis=1)\n",
    "    btcat = bt2.select_dtypes('bool')\n",
    "    btcont = bt2.select_dtypes(exclude='bool')\n",
    "    return pd.concat([btcont.loc[:,pearson_list(btcont, threshold)], btcat.loc[:, (pearson_list(btcat, threshold) | point_biserial(btcat, btcon, threshold))]], axis=1)\n",
    "\n",
    "def drop_corr2(site, year, bt, threshold):\n",
    "    return bt.loc[:,pearson_list(bt, threshold)]\n",
    "\n",
    "def generate_drop_list(site, year, bt, threshold):\n",
    "    corr = bt.corr()\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i,j] >= threshold:\n",
    "                print(site, year, bt.columns[i], btcont.columns[j], corr.iloc[i,j])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f8349-fa29-413c-91a7-f4d20b0b4cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_unit(site, year, data):\n",
    "    print('Filtering units on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    bool_col = data.columns[np.logical_and(data.dtypes == 'bool', data.columns.str.contains('LAB'))]\n",
    "    data_bool = data[bool_col]\n",
    "    bool_col_drop = [x for x in bool_col[data_bool.sum() < 40]]\n",
    "    return data.drop(bool_col_drop, axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba951f7-5721-444c-adab-c776a43528d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nsf_feature_filter(site, year, df):\n",
    "    print('Filtering NSF on site '+site+\":\"+str(year), flush = True)\n",
    "    \n",
    "    df_filter = pd.read_pickle('nsfsch_feature.pkl')\n",
    "    filter_dc = list()\n",
    "    filter_conv = list()\n",
    "    for dc in df.columns:\n",
    "        if dc.split(':')[0] == 'LAB':\n",
    "            lonic = dc.split(':')[-1].split('(')[0]\n",
    "            if df_filter[df_filter['type']=='LAB']['key'].eq(lonic).any():\n",
    "                filter_dc.append(dc)\n",
    "                filter_conv.append(dc.split(':')[0]+':'+dc.split(':')[1]+lonic+'('+dc.split(':')[-1].split('(')[-1])\n",
    "\n",
    "        elif dc.split(':')[0] == 'DX':\n",
    "            code  = dc.split(':')[1]\n",
    "            icd = dc.split(':')[-1].split('(')[0]\n",
    "            if df_filter[np.logical_and(df_filter['type']==dc.split(':')[0], df_filter['code']==code)]['key'].eq(icd).any():\n",
    "                row = df_filter[np.logical_and(df_filter['key']==icd ,np.logical_and(df_filter['type']==dc.split(':')[0], df_filter['code']==code))].head(1)\n",
    "                filter_dc.append(dc)\n",
    "                filter_conv.append(dc.split(':')[0]+':'+row.iloc[0,:]['source_code']+row.iloc[0,:]['source_key']+'('+dc.split(':')[-1].split('(')[-1])\n",
    "\n",
    "        elif dc.split(':')[0] == 'PX':\n",
    "            filter_dc.append(dc)\n",
    "            filter_conv.append(dc)\n",
    "\n",
    "        elif dc.split(':')[0] == 'MED':\n",
    "            code  = dc.split(':')[1]\n",
    "            rxndc = dc.split(':')[-1].split('(')[0]\n",
    "            if df_filter[np.logical_and(df_filter['type']==dc.split(':')[0], df_filter['code']==code)]['key'].eq(rxndc).any():\n",
    "                row = df_filter[np.logical_and(df_filter['key']==rxndc ,np.logical_and(df_filter['type']==dc.split(':')[0], df_filter['code']==code))].head(1)\n",
    "                filter_dc.append(dc)\n",
    "                filter_conv.append(dc.split(':')[0]+':'+row.iloc[0,:]['source_code']+row.iloc[0,:]['source_key']+'('+dc.split(':')[-1].split('(')[-1])\n",
    "        else:\n",
    "            if df_filter['key'].str.contains(dc).any() or df_filter['key'].str.contains(dc.split('_')[0]).any():\n",
    "                filter_dc.append(dc)\n",
    "                filter_conv.append(dc)\n",
    "    return df[filter_dc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b98942-e2db-4852-8f39-0092e94a5c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bigtable(site, year):\n",
    "    #Big Table\n",
    "    print('Merging bt on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    #load tables\n",
    "    newdf_debug = dict()\n",
    "\n",
    "    try:\n",
    "        newdf = bt_onset(site, year)    \n",
    "        # boolean table must merge first\n",
    "\n",
    "        newdf = bt_px(site, year, newdf)\n",
    "        newdf = bt_dx(site, year, newdf)\n",
    "        newdf = bt_amed(site, year, newdf)\n",
    "        newdf = bt_labcat(site, year, newdf)\n",
    "\n",
    "        newdf = bt_demo(site, year, newdf)\n",
    "        newdf = bt_vital(site, year, newdf)\n",
    "        newdf = bt_labnum(site, year, newdf)\n",
    "        newdf = handpickremoval(site, year, newdf)\n",
    "\n",
    "        newdf = filter_unit(site, year, newdf)\n",
    "        newdf = nsf_feature_filter(site, year, newdf)\n",
    "        \n",
    "#        newdf = drop_too_much_nan(site, year, newdf, threshold=0.05)\n",
    "#        newdf = bt_postprocess(site, year, newdf)\n",
    "#        newdf = drop_corr2(site, year, newdf, threshold=0.5)        \n",
    "        \n",
    "        #Save table\n",
    "#        newdf.to_pickle('data/'+site+'/bt_'+site+'_'+str(year)+'.pkl')   #Old data (wrong 24 hours)\n",
    "#        newdf.to_pickle('data/'+site+'/bt2_'+site+'_'+str(year)+'.pkl')  #Old data (per year drop nan)      \n",
    "        newdf.to_pickle('data/'+site+'/bt3_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "        #consistency check\n",
    "        if newdf.empty:\n",
    "            logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "            print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "            logging.error('BT: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "            logging.shutdown()\n",
    "\n",
    "        print('Finished bt on site '+site+\":\"+str(year), flush = True)        \n",
    "    except Exception as e:\n",
    "        logging.basicConfig(filename='BT.log', filemode='a')    \n",
    "        print('OTHER ERROR!!!!! '+site+\":\"+str(year)+'\\n+++++++++++++++++\\n'+str(e)+'\\n-------------------\\n', flush = True)\n",
    "        logging.error('OTHER ERROR!!!!! '+site+\":\"+str(year)+'\\n+++++++++++++++++\\n'+str(e)+'\\n-------------------\\n')\n",
    "        logging.shutdown()       \n",
    "        raise    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20342255-2972-428e-88fd-f7ca27afc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_feature(site, year):\n",
    "\n",
    "    newdf = pd.read_pickle('data/'+site+'/bt3_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    feature_df = pd.DataFrame(newdf.columns)\n",
    "    feature_df.columns = ['Feature']\n",
    "    feature_df['site'] = site\n",
    "    feature_df.to_pickle('data/'+site+'/bt3Features_'+site+'_'+str(year)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69725b80-138b-4f86-b46c-865a2d5894d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_feature():\n",
    "    sites = ['MCRI', 'MCW', 'UIOWA', 'UMHC', 'UNMC', 'UofU', 'UPITT', 'UTHSCSA', 'KUMC', 'UTSW']\n",
    "    feature_df_list = list()\n",
    "    for site in sites:\n",
    "        print(site)    \n",
    "        onset = pd.read_pickle(\"/home/hchan2/AKI/AKI_Python/\"+'data/'+site+'/p0_onset_'+site+'.pkl')\n",
    "        years = list(pd.to_datetime(onset['ADMIT_DATE']).dt.year.unique()) \n",
    "        for year in years:\n",
    "            try:\n",
    "                feature_df_list.append(pd.read_pickle('data/'+site+'/bt3Features_'+site+'_'+str(year)+'.pkl'))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    feature_df = pd.concat(feature_df_list).drop_duplicates()\n",
    "\n",
    "    selected_sites = ['MCW', 'UIOWA', 'UMHC', 'UNMC', 'UofU', 'UPITT', 'UTHSCSA', 'KUMC', 'UTSW']\n",
    "    feature_df = feature_df[feature_df['site'].isin(selected_sites)]\n",
    "\n",
    "    fcount = feature_df.groupby('Feature').count().reset_index()\n",
    "    fcount = fcount[fcount['site']==len(selected_sites)]\n",
    "\n",
    "    fcount[['Feature']].to_pickle('common_feature.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
