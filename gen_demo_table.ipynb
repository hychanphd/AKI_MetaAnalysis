{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea320ac-b3b5-469c-8f56-64393ab3aa19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import ipynb.fs.full.preprocessing0\n",
    "import ipynb.fs.full.preprocessing05\n",
    "#import ipynb.fs.full.prepossessing075_akistage\n",
    "import preprocessing1\n",
    "#import ipynb.fs.full.preprocessing2_BT\n",
    "import preprocessing2_BT\n",
    "\n",
    "import ipynb.fs.full.preprocessing25_BTcorr\n",
    "import ipynb.fs.full.preprocessing3_smote\n",
    "#import ipynb.fs.full.preprocessing4\n",
    "import preprocessing4\n",
    "\n",
    "#import ipynb.fs.full.runxgboost\n",
    "import runxgboost\n",
    "\n",
    "#import ipynb.fs.full.postprocessing1_SHAP\n",
    "\n",
    "import postprocessing1_SHAP\n",
    "\n",
    "import ipynb.fs.full.postprocessing3_collect\n",
    "\n",
    "from ipynb.fs.full.slackbot import ping_slack\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "#import rpy2.robjects as robjects\n",
    "#from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os.path import exists\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import scipy\n",
    "import dataframe_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104e36e-6a86-417a-a519-4bb6dfeca5b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(ipynb.fs.full.preprocessing0)\n",
    "importlib.reload(ipynb.fs.full.preprocessing05)\n",
    "#importlib.reload(ipynb.fs.full.prepossessing075_akistage)\n",
    "importlib.reload(preprocessing1)\n",
    "importlib.reload(preprocessing2_BT)\n",
    "importlib.reload(ipynb.fs.full.preprocessing25_BTcorr)\n",
    "importlib.reload(ipynb.fs.full.preprocessing3_smote)\n",
    "# #importlib.reload(ipynb.fs.full.preprocessing4)\n",
    "importlib.reload(preprocessing4)\n",
    "importlib.reload(runxgboost)\n",
    "importlib.reload(postprocessing1_SHAP)\n",
    "importlib.reload(ipynb.fs.full.postprocessing3_collect)\n",
    "\n",
    "#sites = ['MCRI', 'IUR', 'MCW', 'UIOWA', 'UMHC', 'UNMC', 'UofU', 'UPITT', 'UTHSCSA', 'KUMC', 'UTSW']\n",
    "sites = ['KUMC', 'UTSW', 'MCW', 'UofU', 'UIOWA', 'UMHC', 'UPITT', 'UTHSCSA', 'UNMC']\n",
    "\n",
    "stg = 'stg01'\n",
    "year = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129f7b1-f650-448d-a955-32816c1b5317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c8f4c-dc03-442b-ab0a-89765a2e6bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cal_demo_table(level=\"encounter\"):\n",
    "\n",
    "    sumeries = list()\n",
    "    sumerisc = list()\n",
    "\n",
    "    for site in sites:\n",
    "        summary = dict()\n",
    "        summaryc = dict()\n",
    "\n",
    "        print(site)\n",
    "        demo = pd.read_parquet('/home/hoyinchan/blue/Data/data2022/'+site+'/p0_demo_'+site+'.parquet')\n",
    "        demo['PATID'] = demo['PATID'].astype(str)\n",
    "        demo['ENCOUNTERID'] = demo['ENCOUNTERID'].astype(str)    \n",
    "        demo['AGE'] = demo['AGE'].astype(float)\n",
    "\n",
    "        # Remove duplicates death source\n",
    "        demo = demo.loc[:, ~demo.columns.str.contains(\"DEATH\", case=False)].drop_duplicates()\n",
    "\n",
    "        final_model_data = pd.read_pickle('/home/hoyinchan/blue/Data/data2022/'+site+'/bt3pos_'+site+'_'+'stg01'+'_3000.pkl')\n",
    "        final_model_data['PATID'] = final_model_data['PATID'].astype(str)\n",
    "        final_model_data['ENCOUNTERID'] = final_model_data['ENCOUNTERID'].astype(str)\n",
    "\n",
    "        onset = pd.read_parquet('/home/hoyinchan/blue/Data/data2022/'+site+'/p0_onset_'+site+'.parquet')\n",
    "        onset['PATID'] = onset['PATID'].astype(str)\n",
    "        onset['ENCOUNTERID'] = onset['ENCOUNTERID'].astype(str)\n",
    "\n",
    "        onset = onset.merge(final_model_data[['PATID', 'ENCOUNTERID']], on = ['PATID', 'ENCOUNTERID'], how = 'inner')\n",
    "\n",
    "        onset['AKI3'] = onset['AKI_STAGE']==3\n",
    "        onset['AKI2'] = onset['AKI_STAGE']==2\n",
    "        onset['AnyAKI'] = onset['AKI_STAGE']!=0\n",
    "        onset['Non-AKI'] = onset['AKI_STAGE']==0\n",
    "\n",
    "        onset2 = onset.merge(demo, on = ['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "        #First Encounter\n",
    "        if level != \"encounter\":\n",
    "            onset2 = onset2.sort_values('ADMIT_DATE').groupby(['PATID']).first().reset_index()\n",
    "\n",
    "        racial_categories = {\n",
    "            \"01\": \"Native American\",\n",
    "            \"02\": \"Asian\",\n",
    "            \"03\": \"Black\",\n",
    "            \"04\": \"Not specified\",\n",
    "            \"05\": \"White\",\n",
    "            \"OT\": \"Not specified\",\n",
    "            \"NS\": \"Not specified\"\n",
    "        }\n",
    "\n",
    "        hispanic_categories = {\n",
    "            \"Y\" : \"Yes\",\n",
    "            \"N\": \"No\",\n",
    "            \"NI\": \"Unknown\",\n",
    "            \"UN\" : \"Unknown\"\n",
    "        }\n",
    "\n",
    "\n",
    "        sex_categories = {\n",
    "            \"M\" : \"Male\",\n",
    "            \"F\": \"Female\",\n",
    "            \"NI\": None,\n",
    "            \"UN\" : None,\n",
    "            \"A\" : None\n",
    "        }    \n",
    "\n",
    "        df = onset2\n",
    "        df['RACE'] = df['RACE'].map(racial_categories)\n",
    "        df['HISPANIC'] = df['HISPANIC'].map(hispanic_categories)\n",
    "        df['SEX'] = df['SEX'].map(sex_categories)\n",
    "\n",
    "        # Total count\n",
    "        total_count = len(df)\n",
    "        summary['N'] = \"{:,}\".format(total_count)\n",
    "        summaryc['N'] = total_count\n",
    "\n",
    "        # Categorize Age\n",
    "        age_bins = [0, 25, 35, 45, 55, 65, float('inf')]\n",
    "        age_labels = ['18-25', '26-35', '36-45', '46-55', '56-65', '\\u2265 66']\n",
    "        df['Age_Category'] = pd.cut(df['AGE'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "        df = df[['Non-AKI', 'AnyAKI', 'AKI2', 'AKI3', 'Age_Category', 'SEX', 'HISPANIC', 'RACE']]\n",
    "        df.columns = ['Non-AKI', 'Any AKI', 'AKI2', 'AKI3', 'Age', 'Sex', 'Hispanic', 'Race']\n",
    "        # Counting and reshaping\n",
    "        for column in df.columns:\n",
    "            if 'AKI' in column:\n",
    "                count = df[column].sum()\n",
    "                percentage = (count / total_count) * 100\n",
    "                summary[f'AKI:{column}'] = \"{:,}\".format(count)+f\" ({percentage:.1f}%)\"\n",
    "                summaryc[column] = count\n",
    "            else:\n",
    "                counts = df[column].value_counts()\n",
    "                for value in counts.index:\n",
    "                    count = counts[value]\n",
    "                    percentage = (count / total_count) * 100\n",
    "                    summary[f'{column}:{value}'] = \"{:,}\".format(count)+f\" ({percentage:.1f}%)\"\n",
    "                    summaryc[f'{column}:{value}'] = count\n",
    "\n",
    "        # Creating a single-row DataFrame\n",
    "        summary_df = pd.DataFrame([summary])\n",
    "        summary_df.index = [site]\n",
    "        sumeries.append(summary_df)\n",
    "\n",
    "        # Creating a single-row DataFrame\n",
    "        summary_dfx = pd.DataFrame([summaryc])\n",
    "        summary_dfx.index = [site]\n",
    "        sumerisc.append(summary_dfx)    \n",
    "\n",
    "    sumall = pd.concat(sumeries)\n",
    "    sumallx = pd.concat(sumerisc)\n",
    "\n",
    "    return sumall, sumallx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f34b1d-44d8-4067-93b8-65b067a6f58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumall_enc, sumallx_enc = cal_demo_table(level=\"encounter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98499d86-6c2f-4990-9cb4-8a1d61b31f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumall_p, sumallx_p = cal_demo_table(level=\"paitient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b29290-2b68-4b74-a80d-dc0496bcc802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumall = sumall_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c50d2d0-84ff-4d8f-a609-d46c70f2f1bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumall[['AKI:Non-AKI', 'AKI:Any AKI', 'AKI:AKI2', 'AKI:AKI3']] = sumall_enc[['AKI:Non-AKI', 'AKI:Any AKI', 'AKI:AKI2', 'AKI:AKI3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40264c-022c-4808-9f42-cede12be7537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec6905-bce9-4e0a-80c0-129d3235ed65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d5fda-e880-43f3-a4dd-bfe4f0aa12ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumall = sumall[['N', \n",
    "                  'AKI:Non-AKI', 'AKI:Any AKI', 'AKI:AKI2', 'AKI:AKI3', \n",
    "                  'Age:18-25', 'Age:26-35', 'Age:36-45', 'Age:46-55', 'Age:56-65', 'Age:≥ 66',\n",
    "                  'Sex:Male', 'Sex:Female', \n",
    "                  'Hispanic:Yes', 'Hispanic:No', 'Hispanic:Unknown', \n",
    "                  'Race:Asian', 'Race:Black', 'Race:Native American', 'Race:White', 'Race:Not specified']]\n",
    "\n",
    "sumall = sumall.drop(['AKI:AKI2', 'AKI:AKI3'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a774252-2c4a-4b9d-80d5-f357a681ec20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_key = {3:'MCW', 4:'UIOWA', 5:'UMHC', 6:'UNMC', 9:'UofU', 8:'UTHSCSA', 2:'KUMC', 1:'UTSW', 7:'UPITT'}\n",
    "site_keyr = {v: k for k, v in site_key.items()}\n",
    "sumallt = sumall.T\n",
    "sumallt = sumallt.fillna('0 (0.0%)')\n",
    "sumallt = sumallt.rename(columns=site_keyr)\n",
    "sumallt = sumallt.reindex(sorted(sumallt.columns), axis=1)\n",
    "sumallt.to_csv('data_description.csv')\n",
    "\n",
    "# Splitting the index and creating a MultiIndex for hierarchical display\n",
    "new_index = pd.MultiIndex.from_tuples([tuple(idx.split(':')) if ':' in idx else (idx, '') for idx in sumallt.index])\n",
    "sumallt.index = new_index\n",
    "\n",
    "# Expand the DataFrame to insert empty rows for each new category\n",
    "new_rows = []\n",
    "prev_cat = None\n",
    "for idx, row in sumallt.iterrows():\n",
    "    cat, subcat = idx\n",
    "    if cat != prev_cat:\n",
    "        new_rows.append(((cat, ''), pd.Series([None]*len(sumallt.columns), index=sumallt.columns)))\n",
    "        prev_cat = cat\n",
    "    new_rows.append((idx, row))\n",
    "\n",
    "expanded_df = pd.DataFrame(dict(new_rows)).T\n",
    "\n",
    "def remove_cat(val):\n",
    "    if val[1] != '':\n",
    "        val = ('', val[1])\n",
    "    return val\n",
    "\n",
    "expanded_df.index = pd.MultiIndex.from_tuples([remove_cat(x) for x in expanded_df.index])\n",
    "\n",
    "# Function to replace None with empty string\n",
    "def format_none(val):\n",
    "    return '' if val is None else val\n",
    "\n",
    "# Styling the DataFrame with indentation for subcategories\n",
    "styled_df = expanded_df.style.apply(\n",
    "    lambda x: ['padding-left: 20px' if x.name[1] else '' for _ in x], axis=1\n",
    "                ).set_caption('Table 1 Demographic characteristics at different health systems.')\\\n",
    "                .format(format_none)\\\n",
    "                .set_table_styles([\n",
    "                        # Aligning all cells to the left\n",
    "                        {'selector': 'td', 'props': [('text-align', 'left')]},    \n",
    "                        {'selector': 'th', 'props': [('text-align', 'left')]},  # Aligning index to the left\n",
    "\n",
    "                        # Optional: Adding a border under the column headers for consistency\n",
    "                        {'selector': 'thead th', 'props': [('border-bottom', '1px solid black')]},\n",
    "                        \n",
    "                        # Style for the caption\n",
    "                        {'selector': 'caption', \n",
    "                         'props': [('color', 'black'), \n",
    "                                   ('background-color', 'yellow'), \n",
    "                                   ('font-size', '16px'),\n",
    "                                   ('text-align', 'left'),\n",
    "                                   ('font-weight', 'bold')]},                    \n",
    "\n",
    "                ])\n",
    "\n",
    "dataframe_image.export(styled_df,\"styled_table.png\", table_conversion=\"playwright\")\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e7f5e-79cb-4063-a1ad-f8bafcc890fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f3af2-e4f7-4ce5-b094-6fd7d419a763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58d16e-0683-4e05-aa44-e0ec392936b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitdata = pd.read_csv(\"MetaRegression/plotmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b928b2a-12f6-4cef-b237-0d09871dfdb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extra_translate_omop(label):\n",
    "    # first_dixt = {x:plotshapsn.translate_omop(x) for x in plotshapsn.shapdf['Feature'].unique()}\n",
    "    # Then ask ChatGPT to extract \"extract the medical procedure, medication or Lab measurement from...\"\n",
    "    data_dict = {\n",
    "        'AGE': 'AGE',\n",
    "        'PX:09:96.72': 'Invasive Mechanical Ventilation',\n",
    "        'PX:CH:J1940': 'Furosemide Injection',\n",
    "        'SYSTOLIC': 'SYSTOLIC',\n",
    "        'DIASTOLIC': 'DIASTOLIC',\n",
    "        'ORIGINAL_BMI': 'BMI',\n",
    "        'LAB::17861-6(mg/dL)': 'Calcium',\n",
    "        'LAB::1975-2(mg/dL)': 'Total Bilirubin',\n",
    "        'LAB::2075-0(mmol/L)': 'Chloride',\n",
    "        'LAB::2160-0(mg/dL)': 'Creatinine',\n",
    "        'LAB::2345-7(mg/dL)': 'Glucose',\n",
    "        'LAB::2777-1(mg/dL)': 'Phosphate',\n",
    "        'LAB::2823-3(mmol/L)': 'Potassium',\n",
    "        'LAB::2951-2(mmol/L)': 'Sodium',\n",
    "        'LAB::3094-0(mg/dL)': 'Urea Nitrogen',\n",
    "        'LAB::33037-3(mmol/L)': 'Anion Gap',\n",
    "        'LAB::4092-3(ug/mL)': 'Vancomycin',\n",
    "        'LAB::4544-3(%)': 'Hematocrit',\n",
    "        'LAB::6690-2(10*3/uL)': 'Leukocyte',\n",
    "        'LAB::718-7(g/dL)': 'Hemoglobin',\n",
    "        'LAB::777-3(10*3/uL)': 'Platelet',\n",
    "        'LAB::788-0(%)': 'RDW',\n",
    "        'LAB::2028-9(mmol/L)': 'Carbon dioxide'\n",
    "    }\n",
    "    return data_dict.get(label, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3aa7b6-ee8a-4620-8ee2-4d324ec85b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88481a1-8b67-4fb2-a482-e0a187be1252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "\n",
    "df = expanded_df\n",
    "outputname = 'demo'\n",
    "df.index.names = ['First Level', 'Second Level']\n",
    "index = df.index\n",
    "df.columns = [str(i) for i in df.columns]\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09011a58-5b98-40a6-a7c2-213036acaf07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new Document\n",
    "doc = Document()\n",
    "# Adjust for multi-index (+1 for each level of the multi-index)\n",
    "table = doc.add_table(rows=df.shape[0]+1, cols=len(df.columns) + len(index.levels))\n",
    "\n",
    "# Insert the column names for the index levels and then for the data columns\n",
    "hdr_cells = table.rows[0].cells\n",
    "for i, name in enumerate(index.names):\n",
    "    run = hdr_cells[i].paragraphs[0].add_run(name)\n",
    "    run.bold = True\n",
    "    hdr_cells[i].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "for j, column in enumerate(df.columns, start=len(index.names)):\n",
    "    run = hdr_cells[j].paragraphs[0].add_run(column)\n",
    "    run.bold = True\n",
    "    hdr_cells[j].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "# Insert the row data\n",
    "for row_index, row in df.iterrows():\n",
    "    row_cells = table.add_row().cells\n",
    "    # Write the multi-index values and make them bold\n",
    "    for i, index_part in enumerate(row_index):\n",
    "        run = row_cells[i].paragraphs[0].add_run(str(index_part))\n",
    "        run.bold = True\n",
    "        row_cells[i].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    # Write the data columns with appropriate formatting\n",
    "    for j, (column_name, value) in enumerate(row.items(), start=len(index.names)):\n",
    "        text = f\"{value:.6f}\" if isinstance(value, float) else str(value)\n",
    "        run = row_cells[j].paragraphs[0].add_run(text)\n",
    "        row_cells[j].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        \n",
    "doc.save(outputname+'.docx')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa38d07-2052-443e-99a9-d4338e8c5ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_CDM_PY",
   "language": "python",
   "name": "aki_cdm_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
