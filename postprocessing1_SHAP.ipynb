{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e7587-818a-441c-9c3d-0746cc525f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "import pickle\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catboost import Pool, cv\n",
    "import xgboost\n",
    "import catboost\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179cfb84-cbcc-4f48-854d-234c22f05b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collectSHAP_sub(site, year, stg, fs, oversample, model_type, ckd_group=0, returnflag=False):\n",
    "# site = 'MCRI'\n",
    "# year = 3000\n",
    "# stg = 'stg23'\n",
    "# fs = 'onlymed'\n",
    "# oversample = 'raw'\n",
    "# model_type = 'catd'\n",
    "# ckd_group=0\n",
    "# returnflag=False\n",
    "# if True:\n",
    "    print('Running shap '+model_type+' on site '+site+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample, flush = True)\n",
    "    tic = time.perf_counter()     \n",
    "\n",
    "    #load model\n",
    "    print('data/'+site+'/model_'+model_type+'_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl')\n",
    "    print('data/'+site+'/X_train_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl')\n",
    "    \n",
    "    model = pickle.load(open('data/'+site+'/model_'+model_type+'_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl', 'rb'))\n",
    "\n",
    "    #load tables\n",
    "    X_train = pd.read_pickle('data/'+site+'/X_train_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl')\n",
    "    X_test =  pd.read_pickle('data/'+site+'/X_test_' +site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl')\n",
    "    y_train = pd.read_pickle('data/'+site+'/y_train_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl')\n",
    "    y_test =  pd.read_pickle('data/'+site+'/y_test_' +site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl')\n",
    "\n",
    "    X_train_ckdg = pd.read_pickle('data/'+site+'/X_train_ckdg_'+site+'_'+str(year)+'_'+stg+'_nofs_raw.pkl')\n",
    "    X_test_ckdg  = pd.read_pickle( 'data/'+site+'/X_test_ckdg_' +site+'_'+str(year)+'_'+stg+'_nofs_raw.pkl')        \n",
    "        \n",
    "    # Get AUC\n",
    "#    pred = model.get_booster().predict(dtest, pred_contribs=False)\n",
    "#    pred = model.predict(X_test)    \n",
    "#    roc = roc_auc_score(y_test, pred)    \n",
    "\n",
    "    if ckd_group != 0:\n",
    "        X_train = X_train[X_train_ckdg['CKD_group']==ckd_group]\n",
    "        X_test = X_test[X_test_ckdg['CKD_group']==ckd_group]\n",
    "        y_train = y_train[X_train_ckdg['CKD_group']==ckd_group]\n",
    "        y_test = y_test[X_test_ckdg['CKD_group']==ckd_group]\n",
    "\n",
    "    pred = model.predict_proba(X_test)\n",
    "    roc = roc_auc_score(y_test, pred[:,1])       \n",
    "    \n",
    "    shapX = pd.concat([X_train, X_test])\n",
    "    shapy = pd.concat([y_train, y_test])\n",
    "    \n",
    "    # Calculate SHAP value\n",
    "    if type(model) == xgboost.sklearn.XGBClassifier:\n",
    "        dshap  = xgb.DMatrix(shapX, label=shapy)\n",
    "        shap = model.get_booster().predict(dshap, pred_contribs=True)\n",
    "        # Get feature importance\n",
    "        model_data = pd.concat([pd.DataFrame(model.get_booster().get_score(importance_type='cover'), index=['Cover']), \\\n",
    "        pd.DataFrame(model.get_booster().get_score(importance_type='gain'), index=['Gain']), \\\n",
    "        pd.DataFrame(model.get_booster().get_score(importance_type='weight'), index=['Frequency'])]).transpose() >> mutate(Feature = X.index)\n",
    "        model_data['rank'] = model_data['Gain'].rank(method='min', ascending=False)\n",
    "        used_feature = list(model.get_booster().get_score().keys())        \n",
    "    elif type(model) == catboost.core.CatBoostClassifier:\n",
    "        cat_features = model.get_cat_feature_indices()\n",
    "        pshap = Pool(data=shapX, label=shapy, cat_features=cat_features)        \n",
    "        shap = model.get_feature_importance(data=pshap, type='ShapValues')\n",
    "        model_data = model.get_feature_importance(prettified=True)\n",
    "        model_data['Feature'] = model_data['Feature Id']\n",
    "        model_data = model_data >> select('Feature', 'Importances')\n",
    "        model_data['rank'] = model_data['Importances'].rank(method='min', ascending=False)     \n",
    "        used_feature = list((model_data >> mask(X.Importances!=0)).Feature)\n",
    "    else:\n",
    "    #Using shap package example\n",
    "        import shap\n",
    "        explainer = shap.Explainer(model, algorithm='permutation')\n",
    "        shap_valuesX = explainer.shap_values(shapX)\n",
    "        #shap.summary_plot(shap_valuesX, X_test, plot_type=\"bar\")    \n",
    "        shap = shap_valuesX    \n",
    "\n",
    "    \n",
    "    # Collect SHAP value\n",
    "    def CI95(data):\n",
    "        if len(data) == 1:\n",
    "            return (np.nan, np.nan)\n",
    "        return (np.nan, np.nan)            \n",
    "#        return st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)) #95% confidence interval\n",
    "\n",
    "    shap_data = list()\n",
    "    shap_data_raw = list()\n",
    "    for i in range(shapX.columns.shape[0]):\n",
    "        df = pd.DataFrame(list(zip(shapX.iloc[:,i], shap[:, i], abs(shap[:, i]))),columns =['Name', 'val', 'absval'])\n",
    "        # Check confidence interval for one data point\n",
    "        plot_data = df.groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "        df.index = shapX.index\n",
    "\n",
    "        plot_data_all = df.groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "        plot_data_0= df[shapy==0].groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "        plot_data_1= df[shapy==1].groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "\n",
    "        plot_data_all.columns = [''.join(x) for x in plot_data_all.columns]\n",
    "        plot_data_0.columns = [x+'_0' for x in plot_data_all.columns]\n",
    "        plot_data_1.columns = [x+'_1' for x in plot_data_all.columns]\n",
    "        plot_data_all = plot_data_all.drop('absvalsize', axis=1)\n",
    "        plot_data_0   = plot_data_0.drop('absvalsize_0', axis=1)\n",
    "        plot_data_1   = plot_data_1.drop('absvalsize_1', axis=1)\n",
    "        plot_data_0 = plot_data_0.rename({'Name_0':'Name'},axis=1)\n",
    "        plot_data_1 = plot_data_1.rename({'Name_1':'Name'},axis=1)\n",
    "        plot_data = pd.merge(plot_data_all, plot_data_0, left_on='Name', right_on='Name', how='left')\n",
    "        plot_data = pd.merge(plot_data, plot_data_1, left_on='Name', right_on='Name', how='left')        \n",
    "        \n",
    "        plot_data = plot_data >> mutate(Feature=shapX.columns[i])\n",
    "        plot_data.columns = [''.join(x) for x in plot_data.columns]\n",
    "        plot_data[['valCI95down', 'valCI95up']] = pd.DataFrame(plot_data['valCI95'].tolist(), index=plot_data.index)\n",
    "        plot_data[['absvalCI95down', 'absvalCI95up']] = pd.DataFrame(plot_data['absvalCI95'].tolist(), index=plot_data.index)\n",
    "        plot_data = plot_data.drop(['valCI95', 'absvalCI95'],axis=1)\n",
    "        shap_data.append(plot_data.copy())        \n",
    "        plot_data_raw = df >> select(X.Name, X.val) >> mutate(Feature=shapX.columns[i])        \n",
    "        shap_data_raw.append(plot_data_raw.copy())\n",
    "    shap_data = pd.concat(shap_data)\n",
    "    shap_data_raw = pd.concat(shap_data_raw)    \n",
    "#    shap_data= shap_data[shap_data['Feature'].isin(used_feature)]\n",
    "\n",
    "    # create csv for metaregression\n",
    "    shap_data = shap_data >> left_join(model_data, by='Feature')\n",
    "    siteyr = site+'_'+model_type+'_'+fs+'_'+stg+'_'+oversample+'_'+'005'+\"_\"+str(year)    \n",
    "    shap_data = shap_data >> mutate(siteyr=siteyr) >> rename(fval=X.Name) >> rename(mean_val=X.valmean) >> rename(se_val=X.valstd) >> rename(mean_imp = X.absvalmean) >> rename(se_imp = X.absvalstd) >> rename(var_imp = X.absvalvar) >> rename(median_val = X.valmedian) >> rename(median_imp = X.absvalmedian) >> rename(var_val = X.valvar)\n",
    "    shap_data['site'] = site\n",
    "    shap_data['year'] = year\n",
    "    shap_data['stg'] = stg\n",
    "    shap_data['fs'] = fs\n",
    "    shap_data['oversample'] = oversample\n",
    "    shap_data['model'] = model_type\n",
    "    shap_data['rmcol'] = '005'\n",
    "    \n",
    "    # Calculate ranking base on absolute mean value of SHAP\n",
    "    rank_abs_shap_max = (shap_data >> mutate(abs_shap_max = abs(X.mean_val))).loc[:,['Feature', 'abs_shap_max']].groupby(['Feature']).agg(np.max).reset_index()\n",
    "    rank_abs_shap_max['rank_abs_shap_max'] = rank_abs_shap_max['abs_shap_max'].rank(method='min', ascending=False)\n",
    "    shap_data = pd.merge(shap_data, rank_abs_shap_max, left_on=['Feature'], right_on=['Feature'], how='left')\n",
    "\n",
    "    #Calculate ranking base on SHAP min max difference and variance\n",
    "    tdata = shap_data.loc[:,['Feature', 'mean_val']].groupby(['Feature']).agg([np.max,np.min,np.var]).reset_index()\n",
    "    tdata.columns = ['Feature', 'maxSHAP', 'minSHAP', 'varSHAP']\n",
    "    tdata = (tdata >> mutate(minmax_SHAP = X.maxSHAP-X.minSHAP))\n",
    "    tdata['rank_minmax_SHAP'] = tdata['minmax_SHAP'].rank(method='min', ascending=False)\n",
    "    tdata['rank_var_SHAP'] = tdata['varSHAP'].rank(method='min', ascending=False)\n",
    "    shap_data = pd.merge(shap_data, tdata, left_on=['Feature'], right_on=['Feature'], how='left')    \n",
    "\n",
    "    # add auc value\n",
    "    shap_data = shap_data >> mutate(auc=roc)\n",
    "    \n",
    "    #sort\n",
    "    shap_data = shap_data.sort_values(['rank', 'fval'])\n",
    "\n",
    "    #calculate confusion matrix\n",
    "    cdata = pd.concat([pd.concat([X_train, y_train], axis=1), pd.concat([X_test, y_test], axis=1)], axis=0)\n",
    "    cmdata = cdata.melt(id_vars='FLAG', value_vars= list(cdata.columns).remove('FLAG'))\n",
    "    conmat = cmdata.groupby(['FLAG', 'variable','value']).size().reset_index()\n",
    "    conmat2 = conmat.pivot(index=['variable', 'value'], columns='FLAG', values=0).fillna(0).reset_index()\n",
    "    conmat2.columns = ['Feature', 'fval', 'b', 'a']    \n",
    "    conmat3 = cmdata.groupby(['FLAG', 'variable']).size().reset_index()\n",
    "    conmat4 = conmat3.pivot(index=['variable'], columns='FLAG', values=0).fillna(0).reset_index()\n",
    "    conmat4.columns = ['Feature', 'd', 'c'] \n",
    "    conmat5 = pd.merge(conmat2, conmat4, left_on='Feature', right_on='Feature', how='left')\n",
    "    conmat6 = conmat5 >> mutate(d=X.d-X.b) >> mutate(c=X.c-X.a) >> mutate(num=X.a+X.b)\n",
    "    conmat6['fval'] = conmat6['fval'].astype('float64')\n",
    "    shap_data = pd.merge(shap_data, conmat6, left_on=['Feature', 'fval'], right_on=['Feature', 'fval'], how='left')\n",
    "\n",
    "    \n",
    "    #is categorical?\n",
    "    X_test =  pd.read_pickle('data/'+site+ '/X_test_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl')\n",
    "    cat_features = pd.DataFrame(list(X_test.select_dtypes('bool').columns)) >> mutate(isCategorical = True)\n",
    "    cat_features.columns = ['Feature', 'isCategorical']\n",
    "    shap_data = pd.merge(shap_data, cat_features, right_on='Feature', left_on='Feature', how='left')\n",
    "    shap_data.loc[:,'isCategorical'] = shap_data.loc[:,'isCategorical'].fillna(False)    \n",
    "    \n",
    "    #Collect fval range and stats\n",
    "    Xdata = pd.concat([X_train, X_test], axis=0)\n",
    "    try:\n",
    "        filtertable = Xdata.select_dtypes(exclude=bool).agg([np.min, np.max, np.mean,np.std],axis=0).transpose()\n",
    "        filtertable = filtertable.assign(upr=filtertable['mean']+3*filtertable['std']).assign(lwr=filtertable['mean']-3*filtertable['std']).reset_index().rename({'index':'Feature', 'mean':'fval_mean', 'std':'fval_std', 'upr':'fval_upr', 'lwr':'fval_lwr', 'amax':'fval_max', 'amin':'fval_min'},axis=1)\n",
    "        shap_data  = pd.merge(shap_data, filtertable, right_on='Feature', left_on='Feature', how='left')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #Save shap_data \n",
    "    if returnflag:\n",
    "        return shap_data, shap_data_raw\n",
    "    else:\n",
    "        shap_data.to_pickle('data/'+site+'/shapdata_'+model_type+'_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_'+str(ckd_group)+'_005.pkl')\n",
    "        shap_data_raw.to_pickle('data/'+site+'/shapdataraw_'+model_type+'_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_'+str(ckd_group)+'_005.pkl')\n",
    "    #model.to_pickle('data/'+site+'/model_data_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"{site}:{year} finished in {toc - tic:0.4f} seconds\")  \n",
    "    print('Finished shap '+model_type+' on site '+site+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample, flush = True)    \n",
    "\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd104899-334a-4079-8b9b-104c97b1f73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collectSHAP(site, year, stg, fs, oversample, model_type):\n",
    "    shap_data_list = list()\n",
    "    shap_data_raw_list = list()    \n",
    "    for i in range(1,5):\n",
    "        try:\n",
    "            shap_data, shap_data_raw = collectSHAP_sub(site, year, stg, fs, oversample, model_type, ckd_group=i, returnflag=True)\n",
    "        except Exception as error:\n",
    "            print(site+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample+\":\"+model_type+\" raised \" + \"error\" +\"\\n\"+error.traceback)        \n",
    "        shap_data['ckd_group'] = i\n",
    "        shap_data_raw['ckd_group'] = i\n",
    "        shap_data_list.append(shap_data.copy())\n",
    "        shap_data_raw_list.append(shap_data_raw.copy())\n",
    "    shap_data_all = pd.concat(shap_data_list)\n",
    "    shap_data_raw_all = pd.concat(shap_data_raw_list)\n",
    "    shap_data_all.to_pickle('data/'+site+'/shapdata_'+model_type+'_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_005.pkl')\n",
    "    shap_data_raw_all.to_pickle('data/'+site+'/shapdataraw_'+model_type+'_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_005.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83b73b-67c7-4503-9e7b-03949949bbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
