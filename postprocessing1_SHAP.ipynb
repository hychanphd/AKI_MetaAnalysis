{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26d140-a903-4fb9-8909-e1fb67b7ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "depreciated, see postprocessing1_SHAP.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77695d56-26f2-4758-8d59-b67a0ff14656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "#import rpy2.robjects as robjects\n",
    "#from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "import pickle\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catboost import Pool, cv\n",
    "import xgboost\n",
    "import catboost\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179cfb84-cbcc-4f48-854d-234c22f05b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collectSHAP_sub(site, year, stg, fs, oversample, model_type, ckd_group=0, returnflag=False):\n",
    "# site = 'MCRI'\n",
    "# year = 3000\n",
    "# stg = 'stg23'\n",
    "# fs = 'nofs'\n",
    "# oversample = 'raw'\n",
    "# model_type = 'catd'\n",
    "# ckd_group=0\n",
    "# returnflag=False\n",
    "#if True:\n",
    "    print('Running shap '+model_type+' on site '+site+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample, flush = True)\n",
    "    tic = time.perf_counter()     \n",
    "\n",
    "    #load model\n",
    "    print('Running cross_roc '+model_type+' on site '+site_m+'/'+site_d+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample, flush = True)\n",
    "    model = pickle.load(open(datafolder+site_m+'/model_'+model_type+'_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl', 'rb'))\n",
    "\n",
    "    #load tables\n",
    "    X_train = pd.read_pickle(datafolder+site+'/X_train_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    X_test =  pd.read_pickle(datafolder+site+'/X_test_' +site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    y_train = pd.read_pickle(datafolder+site+'/y_train_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    y_test =  pd.read_pickle(datafolder+site+'/y_test_' +site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "\n",
    "#    X_train_ckdg = pd.read_pickle(datafolder+site+'/X_train_ckdg_'+site+'_'+str(year)+'_'+stg+'_nofs_raw.pkl')\n",
    "#    X_test_ckdg  = pd.read_pickle( datafolder+site+'/X_test_ckdg_' +site+'_'+str(year)+'_'+stg+'_nofs_raw.pkl')        \n",
    "        \n",
    "    # Get AUC\n",
    "#    pred = model.get_booster().predict(dtest, pred_contribs=False)\n",
    "#    pred = model.predict(X_test)    \n",
    "#    roc = roc_auc_score(y_test, pred)    \n",
    "\n",
    "    if ckd_group != 0:\n",
    "#        pass\n",
    "        return\n",
    "#        X_train = X_train[X_train_ckdg['CKD_group']==ckd_group]\n",
    "#        X_test = X_test[X_test_ckdg['CKD_group']==ckd_group]\n",
    "#        y_train = y_train[X_train_ckdg['CKD_group']==ckd_group]\n",
    "#        y_test = y_test[X_test_ckdg['CKD_group']==ckd_group]\n",
    "\n",
    "    pred = model.predict_proba(X_test)\n",
    "    roc = roc_auc_score(y_test, pred[:,1])       \n",
    "    \n",
    "    shapX = pd.concat([X_train, X_test])\n",
    "    shapy = pd.concat([y_train, y_test])\n",
    "    \n",
    "    # Calculate SHAP value\n",
    "    if type(model) == xgboost.sklearn.XGBClassifier:\n",
    "        dshap  = xgb.DMatrix(shapX, label=shapy)\n",
    "        shap = model.get_booster().predict(dshap, pred_contribs=True)\n",
    "        # Get feature importance\n",
    "        model_data = pd.concat([pd.DataFrame(model.get_booster().get_score(importance_type='cover'), index=['Cover']), \\\n",
    "        pd.DataFrame(model.get_booster().get_score(importance_type='gain'), index=['Gain']), \\\n",
    "        pd.DataFrame(model.get_booster().get_score(importance_type='weight'), index=['Frequency'])]).transpose() >> mutate(Feature = X.index)\n",
    "        model_data['rank'] = model_data['Gain'].rank(method='min', ascending=False)\n",
    "        used_feature = list(model.get_booster().get_score().keys())        \n",
    "    elif type(model) == catboost.core.CatBoostClassifier:\n",
    "        cat_features = model.get_cat_feature_indices()\n",
    "        pshap = Pool(data=shapX, label=shapy, cat_features=cat_features)        \n",
    "        shap = model.get_feature_importance(data=pshap, type='ShapValues')\n",
    "        model_data = model.get_feature_importance(prettified=True)\n",
    "        model_data['Feature'] = model_data['Feature Id']\n",
    "        model_data = model_data >> select('Feature', 'Importances')\n",
    "        model_data['rank'] = model_data['Importances'].rank(method='min', ascending=False)     \n",
    "        used_feature = list((model_data >> mask(X.Importances!=0)).Feature)\n",
    "    else:\n",
    "    #Using shap package example\n",
    "        import shap\n",
    "        explainer = shap.Explainer(model, algorithm='permutation')\n",
    "        shap_valuesX = explainer.shap_values(shapX)\n",
    "        #shap.summary_plot(shap_valuesX, X_test, plot_type=\"bar\")    \n",
    "        shap = shap_valuesX    \n",
    "\n",
    "    \n",
    "    # Collect SHAP value\n",
    "    def CI95(data):\n",
    "        if len(data) == 1:\n",
    "            return (np.nan, np.nan)\n",
    "        return (np.nan, np.nan)            \n",
    "#        return st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)) #95% confidence interval\n",
    "\n",
    "    shap_data = list()\n",
    "    shap_data_raw = list()\n",
    "    for i in range(shapX.columns.shape[0]):\n",
    "        df = pd.DataFrame(list(zip(shapX.iloc[:,i], shap[:, i], abs(shap[:, i]))),columns =['Name', 'val', 'absval'])\n",
    "        # Check confidence interval for one data point\n",
    "        plot_data = df.groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "        df.index = shapX.index\n",
    "\n",
    "        plot_data_all = df.groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "        plot_data_0= df[shapy==0].groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "        plot_data_1= df[shapy==1].groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "\n",
    "        plot_data_all.columns = [''.join(x) for x in plot_data_all.columns]\n",
    "        plot_data_0.columns = [x+'_0' for x in plot_data_all.columns]\n",
    "        plot_data_1.columns = [x+'_1' for x in plot_data_all.columns]\n",
    "        plot_data_all = plot_data_all.drop('absvalsize', axis=1)\n",
    "        plot_data_0   = plot_data_0.drop('absvalsize_0', axis=1)\n",
    "        plot_data_1   = plot_data_1.drop('absvalsize_1', axis=1)\n",
    "        plot_data_0 = plot_data_0.rename({'Name_0':'Name'},axis=1)\n",
    "        plot_data_1 = plot_data_1.rename({'Name_1':'Name'},axis=1)\n",
    "        plot_data = pd.merge(plot_data_all, plot_data_0, left_on='Name', right_on='Name', how='left')\n",
    "        plot_data = pd.merge(plot_data, plot_data_1, left_on='Name', right_on='Name', how='left')        \n",
    "        \n",
    "        plot_data = plot_data >> mutate(Feature=shapX.columns[i])\n",
    "        plot_data.columns = [''.join(x) for x in plot_data.columns]\n",
    "        plot_data[['valCI95down', 'valCI95up']] = pd.DataFrame(plot_data['valCI95'].tolist(), index=plot_data.index)\n",
    "        plot_data[['absvalCI95down', 'absvalCI95up']] = pd.DataFrame(plot_data['absvalCI95'].tolist(), index=plot_data.index)\n",
    "        plot_data = plot_data.drop(['valCI95', 'absvalCI95'],axis=1)\n",
    "        shap_data.append(plot_data.copy())        \n",
    "        plot_data_raw = df >> select(X.Name, X.val) >> mutate(Feature=shapX.columns[i])        \n",
    "        shap_data_raw.append(plot_data_raw.copy())\n",
    "    shap_data = pd.concat(shap_data)\n",
    "    shap_data_raw = pd.concat(shap_data_raw)    \n",
    "#    shap_data= shap_data[shap_data['Feature'].isin(used_feature)]\n",
    "\n",
    "    # create csv for metaregression\n",
    "    shap_data = shap_data >> left_join(model_data, by='Feature')\n",
    "    siteyr = site+'_'+model_type+'_'+fs+'_'+stg+'_'+oversample+'_'+'005'+\"_\"+str(year)    \n",
    "    shap_data = shap_data >> mutate(siteyr=siteyr) >> rename(fval=X.Name) >> rename(mean_val=X.valmean) >> rename(se_val=X.valstd) >> rename(mean_imp = X.absvalmean) >> rename(se_imp = X.absvalstd) >> rename(var_imp = X.absvalvar) >> rename(median_val = X.valmedian) >> rename(median_imp = X.absvalmedian) >> rename(var_val = X.valvar)\n",
    "    shap_data['site'] = site\n",
    "    shap_data['year'] = year\n",
    "    shap_data['stg'] = stg\n",
    "    shap_data['fs'] = fs\n",
    "    shap_data['oversample'] = oversample\n",
    "    shap_data['model'] = model_type\n",
    "    shap_data['rmcol'] = '005'\n",
    "    \n",
    "    # Calculate ranking base on absolute mean value of SHAP\n",
    "    rank_abs_shap_max = (shap_data >> mutate(abs_shap_max = abs(X.mean_val))).loc[:,['Feature', 'abs_shap_max']].groupby(['Feature']).agg(np.max).reset_index()\n",
    "    rank_abs_shap_max['rank_abs_shap_max'] = rank_abs_shap_max['abs_shap_max'].rank(method='min', ascending=False)\n",
    "    shap_data = pd.merge(shap_data, rank_abs_shap_max, left_on=['Feature'], right_on=['Feature'], how='left')\n",
    "\n",
    "    #Calculate ranking base on SHAP min max difference and variance\n",
    "    tdata = shap_data.loc[:,['Feature', 'mean_val']].groupby(['Feature']).agg([np.max,np.min,np.var]).reset_index()\n",
    "    tdata.columns = ['Feature', 'maxSHAP', 'minSHAP', 'varSHAP']\n",
    "    tdata = (tdata >> mutate(minmax_SHAP = X.maxSHAP-X.minSHAP))\n",
    "    tdata['rank_minmax_SHAP'] = tdata['minmax_SHAP'].rank(method='min', ascending=False)\n",
    "    tdata['rank_var_SHAP'] = tdata['varSHAP'].rank(method='min', ascending=False)\n",
    "    shap_data = pd.merge(shap_data, tdata, left_on=['Feature'], right_on=['Feature'], how='left')    \n",
    "\n",
    "    # add auc value\n",
    "    shap_data = shap_data >> mutate(auc=roc)\n",
    "    \n",
    "    #sort\n",
    "    shap_data = shap_data.sort_values(['rank', 'fval'])\n",
    "\n",
    "    #calculate confusion matrix\n",
    "    cdata = pd.concat([pd.concat([X_train, y_train], axis=1), pd.concat([X_test, y_test], axis=1)], axis=0)\n",
    "    cmdata = cdata.melt(id_vars='FLAG', value_vars= list(cdata.columns).remove('FLAG'))\n",
    "    conmat = cmdata.groupby(['FLAG', 'variable','value']).size().reset_index()\n",
    "    conmat2 = conmat.pivot(index=['variable', 'value'], columns='FLAG', values=0).fillna(0).reset_index()\n",
    "    conmat2.columns = ['Feature', 'fval', 'b', 'a']    \n",
    "    conmat3 = cmdata.groupby(['FLAG', 'variable']).size().reset_index()\n",
    "    conmat4 = conmat3.pivot(index=['variable'], columns='FLAG', values=0).fillna(0).reset_index()\n",
    "    conmat4.columns = ['Feature', 'd', 'c'] \n",
    "    conmat5 = pd.merge(conmat2, conmat4, left_on='Feature', right_on='Feature', how='left')\n",
    "    conmat6 = conmat5 >> mutate(d=X.d-X.b) >> mutate(c=X.c-X.a) >> mutate(num=X.a+X.b)\n",
    "    conmat6['fval'] = conmat6['fval'].astype('float64')\n",
    "    shap_data = pd.merge(shap_data, conmat6, left_on=['Feature', 'fval'], right_on=['Feature', 'fval'], how='left')\n",
    "    \n",
    "    #is categorical?\n",
    "    X_test =  pd.read_pickle(datafolder+site+ '/X_test_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    cat_features = pd.DataFrame(list(X_test.select_dtypes('bool').columns)) >> mutate(isCategorical = True)\n",
    "    cat_features.columns = ['Feature', 'isCategorical']\n",
    "    shap_data = pd.merge(shap_data, cat_features, right_on='Feature', left_on='Feature', how='left')\n",
    "    shap_data.loc[:,'isCategorical'] = shap_data.loc[:,'isCategorical'].fillna(False)    \n",
    "    \n",
    "    #Collect fval range and stats\n",
    "    Xdata = pd.concat([X_train, X_test], axis=0)\n",
    "    try:\n",
    "        filtertable = Xdata.select_dtypes(exclude=bool).agg([np.min, np.max, np.mean,np.std],axis=0).transpose()\n",
    "        filtertable = filtertable.assign(upr=filtertable['mean']+3*filtertable['std']).assign(lwr=filtertable['mean']-3*filtertable['std']).reset_index().rename({'index':'Feature', 'mean':'fval_mean', 'std':'fval_std', 'upr':'fval_upr', 'lwr':'fval_lwr', 'amax':'fval_max', 'amin':'fval_min'},axis=1)\n",
    "        shap_data  = pd.merge(shap_data, filtertable, right_on='Feature', left_on='Feature', how='left')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #Save shap_data \n",
    "    if returnflag:\n",
    "#        pass\n",
    "        return shap_data, shap_data_raw\n",
    "    else:\n",
    "        shap_data.to_pickle(datafolder+site+'/shapdata_'+model_type+'_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_'+str(ckd_group)+'_005.pkl')\n",
    "        shap_data_raw.to_pickle(datafolder+site+'/shapdataraw_'+model_type+'_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_'+str(ckd_group)+'_005.pkl')\n",
    "    #model.to_pickle(datafolder+site+'/model_data_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"{site}:{year} finished in {toc - tic:0.4f} seconds\")  \n",
    "    print('Finished shap '+model_type+' on site '+site+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample, flush = True)    \n",
    "\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd104899-334a-4079-8b9b-104c97b1f73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collectSHAP(site, year, stg, fs, oversample, model_type):\n",
    "    shap_data_list = list()\n",
    "    shap_data_raw_list = list()    \n",
    "    for i in range(1,5):\n",
    "        try:\n",
    "            shap_data, shap_data_raw = collectSHAP_sub(site, year, stg, fs, oversample, model_type, ckd_group=i, returnflag=True)\n",
    "        except Exception as error:\n",
    "            print(site+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample+\":\"+model_type+\" raised \" + \"error\" +\"\\n\"+error.traceback)        \n",
    "        shap_data['ckd_group'] = i\n",
    "        shap_data_raw['ckd_group'] = i\n",
    "        shap_data_list.append(shap_data.copy())\n",
    "        shap_data_raw_list.append(shap_data_raw.copy())\n",
    "    shap_data_all = pd.concat(shap_data_list)\n",
    "    shap_data_raw_all = pd.concat(shap_data_raw_list)\n",
    "    shap_data_all.to_pickle(datafolder+site+'/shapdata_'+model_type+'_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_005.pkl')\n",
    "    shap_data_raw_all.to_pickle(datafolder+site+'/shapdataraw_'+model_type+'_'+site+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_005.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836413ef-8ded-4e8c-9152-20a71e28d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_roc(site_m, site_d, year, stg, fs, oversample, model_type, ckd_group=0, returnflag=False):\n",
    "\n",
    "    model = pickle.load(open(datafolder+site_m+'/model_'+model_type+'_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl', 'rb'))\n",
    "\n",
    "    #load tables\n",
    "    X_train_m = pd.read_pickle(datafolder+site_m+'/X_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    y_train_m = pd.read_pickle(datafolder+site_m+'/y_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    X_test_m = pd.read_pickle(datafolder+site_m+'/X_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    y_test_m = pd.read_pickle(datafolder+site_m+'/y_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "\n",
    "    X_train_d = pd.read_pickle(datafolder+site_d+'/X_train_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    y_train_d = pd.read_pickle(datafolder+site_d+'/y_train_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    X_test_d =  pd.read_pickle(datafolder+site_d+'/X_test_' +site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    y_test_d =  pd.read_pickle(datafolder+site_d+'/y_test_' +site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "\n",
    "    common_features = [x for x in X_test_d.columns if x in X_train_m.columns]\n",
    "\n",
    "    X_train2_d = X_train_d[common_features]\n",
    "    X_test2_d = X_test_d[common_features]\n",
    "\n",
    "    X_train2_m = X_train_m.iloc[0:1]\n",
    "    X_test2_m = X_test_m.iloc[0:1]\n",
    "\n",
    "    X_train3_d = pd.concat([X_train2_m, X_train2_d]).iloc[1:]\n",
    "    X_test3_d = pd.concat([X_test2_m, X_test2_d]).iloc[1:]\n",
    "\n",
    "    X_train3_d.loc[:,X_train2_m.dtypes==bool] = X_train3_d.loc[:,X_train2_m.dtypes==bool].fillna(False)\n",
    "    X_test3_d.loc[:,X_test2_m.dtypes==bool] = X_test3_d.loc[:,X_test2_m.dtypes==bool].fillna(False)\n",
    "\n",
    "    pred = model.predict_proba(X_test3_d)\n",
    "\n",
    "    roc = roc_auc_score(y_test_d, pred[:,1])\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852747b8-2923-44d1-a04b-437544e955fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectSHAP_cross_sub(site_m, site_d, ckd_group=0, returnflag=False):\n",
    "'''\n",
    "This function apply internal and external data to each cross validate model of site_m\n",
    "if site_d = site_m, the validaton set is used\n",
    "else all data from site_d is used\n",
    "'''\n",
    "\n",
    "    year=3000\n",
    "    configs_variables = utils_function.read_config(site_m)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    stg = configs_variables['stg']\n",
    "    fs = configs_variables['fs']\n",
    "    oversample = configs_variables['oversample']\n",
    "    model_type = configs_variables['model_type']   \n",
    "    \n",
    "    \n",
    "    print('Running collectSHAP_cross_sub '+model_type+' on site '+site_m+'/'+site_d+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample, flush = True)\n",
    "    tic = time.perf_counter()     \n",
    "\n",
    "    #load model\n",
    "    model = pickle.load(open(datafolder+site_m+'/model_'+model_type+'_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'.pkl', 'rb'))\n",
    "\n",
    "    #load tables\n",
    "    X_train_m = pd.read_pickle(datafolder+site_m+'/X_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    y_train_m = pd.read_pickle(datafolder+site_m+'/y_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    X_test_m = pd.read_pickle(datafolder+site_m+'/X_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    y_test_m = pd.read_pickle(datafolder+site_m+'/y_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "\n",
    "    X_train_d = pd.read_pickle(datafolder+site_d+'/X_train_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    y_train_d = pd.read_pickle(datafolder+site_d+'/y_train_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    X_test_d =  pd.read_pickle(datafolder+site_d+'/X_test_' +site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    y_test_d =  pd.read_pickle(datafolder+site_d+'/y_test_' +site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "\n",
    "   #Generate cross site data     \n",
    "    common_features = [x for x in X_test_d.columns if x in X_train_m.columns]\n",
    "\n",
    "    X_train2_d = X_train_d[common_features]\n",
    "    X_test2_d = X_test_d[common_features]\n",
    "\n",
    "    X_train2_m = X_train_m.iloc[0:1]\n",
    "    X_test2_m = X_test_m.iloc[0:1]\n",
    "\n",
    "    X_train3_d = pd.concat([X_train2_m, X_train2_d]).iloc[1:]\n",
    "    X_test3_d = pd.concat([X_test2_m, X_test2_d]).iloc[1:]\n",
    "\n",
    "    X_train3_d.loc[:,X_train2_m.dtypes==bool] = X_train3_d.loc[:,X_train2_m.dtypes==bool].fillna(False)\n",
    "    X_test3_d.loc[:,X_test2_m.dtypes==bool] = X_test3_d.loc[:,X_test2_m.dtypes==bool].fillna(False)        \n",
    "\n",
    "    X_train = X_train3_d\n",
    "    X_test = X_test3_d    \n",
    "    y_train = y_train_d\n",
    "    y_test = y_test_d    \n",
    "    \n",
    "    # Get AUC\n",
    "#    pred = model.get_booster().predict(dtest, pred_contribs=False)\n",
    "#    pred = model.predict(X_test)    \n",
    "#    roc = roc_auc_score(y_test, pred)    \n",
    "\n",
    "    pred = model.predict_proba(X_test)\n",
    "    roc = roc_auc_score(y_test, pred[:,1])       \n",
    "    \n",
    "    shapX = pd.concat([X_train, X_test])\n",
    "    shapy = pd.concat([y_train, y_test])\n",
    "    \n",
    "    # Calculate SHAP value\n",
    "    if type(model) == xgboost.sklearn.XGBClassifier:\n",
    "        dshap  = xgb.DMatrix(shapX, label=shapy)\n",
    "        shap = model.get_booster().predict(dshap, pred_contribs=True)\n",
    "        # Get feature importance\n",
    "        model_data = pd.concat([pd.DataFrame(model.get_booster().get_score(importance_type='cover'), index=['Cover']), \\\n",
    "        pd.DataFrame(model.get_booster().get_score(importance_type='gain'), index=['Gain']), \\\n",
    "        pd.DataFrame(model.get_booster().get_score(importance_type='weight'), index=['Frequency'])]).transpose() >> mutate(Feature = X.index)\n",
    "        model_data['rank'] = model_data['Gain'].rank(method='min', ascending=False)\n",
    "        used_feature = list(model.get_booster().get_score().keys())        \n",
    "    elif type(model) == catboost.core.CatBoostClassifier:\n",
    "        cat_features = model.get_cat_feature_indices()\n",
    "        pshap = Pool(data=shapX, label=shapy, cat_features=cat_features)        \n",
    "        shap = model.get_feature_importance(data=pshap, type='ShapValues')\n",
    "        model_data = model.get_feature_importance(prettified=True)\n",
    "        model_data['Feature'] = model_data['Feature Id']\n",
    "        model_data = model_data >> select('Feature', 'Importances')\n",
    "        model_data['rank'] = model_data['Importances'].rank(method='min', ascending=False)     \n",
    "        used_feature = list((model_data >> mask(X.Importances!=0)).Feature)\n",
    "    else:\n",
    "    #Using shap package example\n",
    "        import shap\n",
    "        explainer = shap.Explainer(model, algorithm='permutation')\n",
    "        shap_valuesX = explainer.shap_values(shapX)\n",
    "        #shap.summary_plot(shap_valuesX, X_test, plot_type=\"bar\")    \n",
    "        shap = shap_valuesX    \n",
    "    \n",
    "    # Collect SHAP value\n",
    "    def CI95(data):\n",
    "        if len(data) == 1:\n",
    "            return (np.nan, np.nan)\n",
    "        return (np.nan, np.nan)            \n",
    "#        return st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)) #95% confidence interval\n",
    "\n",
    "    shap_data = list()\n",
    "    shap_data_raw = list()\n",
    "    for i in range(shapX.columns.shape[0]):\n",
    "        df = pd.DataFrame(list(zip(shapX.iloc[:,i], shap[:, i], abs(shap[:, i]))),columns =['Name', 'val', 'absval'])\n",
    "        # Check confidence interval for one data point\n",
    "        plot_data = df.groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "        df.index = shapX.index\n",
    "\n",
    "        plot_data_all = df.groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "        plot_data_0= df[shapy==0].groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "        plot_data_1= df[shapy==1].groupby(\"Name\").agg([np.mean, np.var, np.std, np.median, CI95, 'size']).reset_index()\n",
    "\n",
    "        plot_data_all.columns = [''.join(x) for x in plot_data_all.columns]\n",
    "        plot_data_0.columns = [x+'_0' for x in plot_data_all.columns]\n",
    "        plot_data_1.columns = [x+'_1' for x in plot_data_all.columns]\n",
    "        plot_data_all = plot_data_all.drop('absvalsize', axis=1)\n",
    "        plot_data_0   = plot_data_0.drop('absvalsize_0', axis=1)\n",
    "        plot_data_1   = plot_data_1.drop('absvalsize_1', axis=1)\n",
    "        plot_data_0 = plot_data_0.rename({'Name_0':'Name'},axis=1)\n",
    "        plot_data_1 = plot_data_1.rename({'Name_1':'Name'},axis=1)\n",
    "        plot_data = pd.merge(plot_data_all, plot_data_0, left_on='Name', right_on='Name', how='left')\n",
    "        plot_data = pd.merge(plot_data, plot_data_1, left_on='Name', right_on='Name', how='left')        \n",
    "        \n",
    "        plot_data = plot_data >> mutate(Feature=shapX.columns[i])\n",
    "        plot_data.columns = [''.join(x) for x in plot_data.columns]\n",
    "        plot_data[['valCI95down', 'valCI95up']] = pd.DataFrame(plot_data['valCI95'].tolist(), index=plot_data.index)\n",
    "        plot_data[['absvalCI95down', 'absvalCI95up']] = pd.DataFrame(plot_data['absvalCI95'].tolist(), index=plot_data.index)\n",
    "        plot_data = plot_data.drop(['valCI95', 'absvalCI95'],axis=1)\n",
    "        shap_data.append(plot_data.copy())        \n",
    "        plot_data_raw = df >> select(X.Name, X.val) >> mutate(Feature=shapX.columns[i])        \n",
    "        shap_data_raw.append(plot_data_raw.copy())\n",
    "    shap_data = pd.concat(shap_data)\n",
    "    shap_data_raw = pd.concat(shap_data_raw)    \n",
    "#    shap_data= shap_data[shap_data['Feature'].isin(used_feature)]\n",
    "\n",
    "    # create csv for metaregression\n",
    "    shap_data = shap_data >> left_join(model_data, by='Feature')\n",
    "    siteyr = site_m+'_'+site_d+'_'+model_type+'_'+fs+'_'+stg+'_'+oversample+'_'+'005'+\"_\"+str(year)    \n",
    "    shap_data = shap_data >> mutate(siteyr=siteyr) >> rename(fval=X.Name) >> rename(mean_val=X.valmean) >> rename(se_val=X.valstd) >> rename(mean_imp = X.absvalmean) >> rename(se_imp = X.absvalstd) >> rename(var_imp = X.absvalvar) >> rename(median_val = X.valmedian) >> rename(median_imp = X.absvalmedian) >> rename(var_val = X.valvar)\n",
    "    shap_data['site_m'] = site_m\n",
    "    shap_data['site_d'] = site_d    \n",
    "    shap_data['year'] = year\n",
    "    shap_data['stg'] = stg\n",
    "    shap_data['fs'] = fs\n",
    "    shap_data['oversample'] = oversample\n",
    "    shap_data['model'] = model_type\n",
    "    shap_data['rmcol'] = '005'\n",
    "    \n",
    "    # Calculate ranking base on absolute mean value of SHAP\n",
    "    rank_abs_shap_max = (shap_data >> mutate(abs_shap_max = abs(X.mean_val))).loc[:,['Feature', 'abs_shap_max']].groupby(['Feature']).agg(np.max).reset_index()\n",
    "    rank_abs_shap_max['rank_abs_shap_max'] = rank_abs_shap_max['abs_shap_max'].rank(method='min', ascending=False)\n",
    "    shap_data = pd.merge(shap_data, rank_abs_shap_max, left_on=['Feature'], right_on=['Feature'], how='left')\n",
    "\n",
    "    #Calculate ranking base on SHAP min max difference and variance\n",
    "    tdata = shap_data.loc[:,['Feature', 'mean_val']].groupby(['Feature']).agg([np.max,np.min,np.var]).reset_index()\n",
    "    tdata.columns = ['Feature', 'maxSHAP', 'minSHAP', 'varSHAP']\n",
    "    tdata = (tdata >> mutate(minmax_SHAP = X.maxSHAP-X.minSHAP))\n",
    "    tdata['rank_minmax_SHAP'] = tdata['minmax_SHAP'].rank(method='min', ascending=False)\n",
    "    tdata['rank_var_SHAP'] = tdata['varSHAP'].rank(method='min', ascending=False)\n",
    "    shap_data = pd.merge(shap_data, tdata, left_on=['Feature'], right_on=['Feature'], how='left')    \n",
    "\n",
    "    # add auc value\n",
    "    shap_data = shap_data >> mutate(auc=roc)\n",
    "    \n",
    "    #sort\n",
    "    shap_data = shap_data.sort_values(['rank', 'fval'])\n",
    "\n",
    "    #calculate confusion matrix\n",
    "    cdata = pd.concat([pd.concat([X_train, y_train], axis=1), pd.concat([X_test, y_test], axis=1)], axis=0)\n",
    "    cmdata = cdata.melt(id_vars='FLAG', value_vars= list(cdata.columns).remove('FLAG'))\n",
    "    conmat = cmdata.groupby(['FLAG', 'variable','value']).size().reset_index()\n",
    "    conmat2 = conmat.pivot(index=['variable', 'value'], columns='FLAG', values=0).fillna(0).reset_index()\n",
    "    conmat2.columns = ['Feature', 'fval', 'b', 'a']    \n",
    "    conmat3 = cmdata.groupby(['FLAG', 'variable']).size().reset_index()\n",
    "    conmat4 = conmat3.pivot(index=['variable'], columns='FLAG', values=0).fillna(0).reset_index()\n",
    "    conmat4.columns = ['Feature', 'd', 'c'] \n",
    "    conmat5 = pd.merge(conmat2, conmat4, left_on='Feature', right_on='Feature', how='left')\n",
    "    conmat6 = conmat5 >> mutate(d=X.d-X.b) >> mutate(c=X.c-X.a) >> mutate(num=X.a+X.b)\n",
    "    conmat6['fval'] = conmat6['fval'].astype('float64')\n",
    "    shap_data = pd.merge(shap_data, conmat6, left_on=['Feature', 'fval'], right_on=['Feature', 'fval'], how='left')\n",
    "\n",
    "    \n",
    "    #is categorical?\n",
    "    X_test =  pd.read_pickle(datafolder+site_m+ '/X_test_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "    cat_features = pd.DataFrame(list(X_test.select_dtypes('bool').columns)) >> mutate(isCategorical = True)\n",
    "    cat_features.columns = ['Feature', 'isCategorical']\n",
    "    shap_data = pd.merge(shap_data, cat_features, right_on='Feature', left_on='Feature', how='left')\n",
    "    shap_data.loc[:,'isCategorical'] = shap_data.loc[:,'isCategorical'].fillna(False)    \n",
    "    \n",
    "    #Collect fval range and stats\n",
    "    Xdata = pd.concat([X_train, X_test], axis=0)\n",
    "    try:\n",
    "        filtertable = Xdata.select_dtypes(exclude=bool).agg([np.min, np.max, np.mean,np.std],axis=0).transpose()\n",
    "        filtertable = filtertable.assign(upr=filtertable['mean']+3*filtertable['std']).assign(lwr=filtertable['mean']-3*filtertable['std']).reset_index().rename({'index':'Feature', 'mean':'fval_mean', 'std':'fval_std', 'upr':'fval_upr', 'lwr':'fval_lwr', 'amax':'fval_max', 'amin':'fval_min'},axis=1)\n",
    "        shap_data  = pd.merge(shap_data, filtertable, right_on='Feature', left_on='Feature', how='left')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #Save shap_data \n",
    "    if returnflag:\n",
    "#        pass\n",
    "        return shap_data, shap_data_raw\n",
    "    else:\n",
    "        shap_data.to_pickle(datafolder+site_m+'/shapdata_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_'+str(ckd_group)+'_005.pkl')\n",
    "        shap_data_raw.to_pickle(datafolder+site_m+'/shapdataraw_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_'+str(ckd_group)+'_005.pkl')\n",
    "    #model.to_pickle(datafolder+site+'/model_data_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"{site_m}/{site_d}:{year} finished in {toc - tic:0.4f} seconds\")  \n",
    "    print('Finished collectSHAP_cross_sub '+model_type+' on site '+site_m+'/'+site_d+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample, flush = True)\n",
    "\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58990d-4d49-4d9f-b925-70765da0dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectSHAP_cross_sub_validate(site_m, site_d, fold, returnflag=False):\n",
    "'''\n",
    "This function apply internal and external data to each cross validate model of site_m\n",
    "if site_d = site_m, the validaton set is used\n",
    "else all data from site_d is used\n",
    "'''\n",
    "\n",
    "    year=3000\n",
    "    configs_variables = utils_function.read_config(site_m)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    stg = configs_variables['stg']\n",
    "    fs = configs_variables['fs']\n",
    "    oversample = configs_variables['oversample']\n",
    "    model_type = configs_variables['model_type']\n",
    "    \n",
    "    \n",
    "    tic = time.perf_counter()     \n",
    "    print('Running collectSHAP_cross_sub '+model_type+' on site '+site_m+'/'+site_d+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample+':'+str(fold), flush = True)\n",
    "\n",
    "    #load model\n",
    "    model = pickle.load(open(datafolder+site_m+'/boosttrap_'+model_type+'_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_'+str(fold)+'.pkl', 'rb'))\n",
    "\n",
    "    if site_m == site_d:\n",
    "        #load tables\n",
    "        X_test_m = pd.read_pickle(datafolder+site_m+'/X_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+str(fold)+'pos.pkl')\n",
    "        y_test_m = pd.read_pickle(datafolder+site_m+'/y_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+str(fold)+'pos.pkl')\n",
    "\n",
    "        X_test_d =  pd.read_pickle(datafolder+site_d+'/X_test_' +site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+str(fold)+'pos.pkl')\n",
    "        y_test_d =  pd.read_pickle(datafolder+site_d+'/y_test_' +site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+str(fold)+'pos.pkl')\n",
    "\n",
    "    else:\n",
    "        #load tables\n",
    "        X_test_m = pd.read_pickle(datafolder+site_m+'/X_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+str(fold)+'pos.pkl')\n",
    "        y_test_m = pd.read_pickle(datafolder+site_m+'/y_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+str(fold)+'pos.pkl')\n",
    "\n",
    "        X_test_d =  pd.read_pickle(datafolder+site_d+'/X_test_' +site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "        y_test_d =  pd.read_pickle(datafolder+site_d+'/y_test_' +site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "        \n",
    "    model = model[-3]\n",
    "\n",
    "    #Generate cross site data     \n",
    "    common_features = [x for x in X_test_d.columns if x in X_test_m.columns]\n",
    "\n",
    "    X_test2_d = X_test_d[common_features]\n",
    "    X_test2_m = X_test_m.iloc[0:1]\n",
    "    X_test3_d = pd.concat([X_test2_m, X_test2_d]).iloc[1:]\n",
    "    X_test3_d.loc[:,X_test2_m.dtypes==bool] = X_test3_d.loc[:,X_test2_m.dtypes==bool].fillna(False)        \n",
    "\n",
    "    X_test = X_test3_d    \n",
    "    y_test = y_test_d    \n",
    "\n",
    "    for unmatch in X_test.dtypes[X_test.dtypes != X_test_m.dtypes].keys():\n",
    "        if X_test_m.dtypes[unmatch] == bool:\n",
    "            X_test[unmatch] = False\n",
    "        else:\n",
    "            X_test[unmatch] = np.nan        \n",
    "\n",
    "    pred = model.predict_proba(X_test)\n",
    "    roc = roc_auc_score(y_test, pred[:,1]) \n",
    "\n",
    "    shap_data = dict()\n",
    "    shap_data['site_m'] = site_m\n",
    "    shap_data['site_d'] = site_d    \n",
    "    shap_data['year'] = year\n",
    "    shap_data['stg'] = stg\n",
    "    shap_data['fs'] = fs\n",
    "    shap_data['oversample'] = oversample\n",
    "    shap_data['model'] = model_type\n",
    "    shap_data['rmcol'] = '005'\n",
    "    shap_data['fold'] = fold\n",
    "    shap_data['roc'] = roc\n",
    "    \n",
    "    shap_data['y_test'] = [np.array(y_test)]\n",
    "    shap_data['pred'] = [np.array(pred[:,1])]    \n",
    "\n",
    "    shap_data = pd.DataFrame(shap_data,index=[1])\n",
    "    shap_data.to_pickle(datafolder+site_m+'/shapdata_cv_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_'+str(fold)+'.pkl')\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"{site_m}/{site_d}:{year} finished in {toc - tic:0.4f} seconds\")  \n",
    "    print('Finished collectSHAP_cross_sub '+model_type+' on site '+site_m+'/'+site_d+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample, flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077cfbf-a3dc-4417-8fee-b87062ae2ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# site_m = 'KUMC'\n",
    "# site_d = 'KUMC'\n",
    "# year = 3000\n",
    "# stg = 'stg01'\n",
    "# fs = 'nofs'\n",
    "# oversample = 'raw'\n",
    "# model_type = 'catd'\n",
    "# flist = None\n",
    "# fold=1\n",
    "# returnflag=False\n",
    "\n",
    "# if True:\n",
    "# #def collectSHAP_cross_sub_validate_SHAP(site_m, site_d, year, stg, fs, oversample, model_type, flist, fold=10, returnflag=False):\n",
    "\n",
    "#     tic = time.perf_counter()     \n",
    "\n",
    "#     #load tables\n",
    "#     X_train_m = pd.read_pickle(datafolder+site_m+'/X_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "#     y_train_m = pd.read_pickle(datafolder+site_m+'/y_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "#     X_test_m = pd.read_pickle(datafolder+site_m+'/X_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "#     y_test_m = pd.read_pickle(datafolder+site_m+'/y_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "\n",
    "#     X_train_d = pd.read_pickle(datafolder+site_d+'/X_train_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "#     y_train_d = pd.read_pickle(datafolder+site_d+'/y_train_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "#     X_test_d =  pd.read_pickle(datafolder+site_d+'/X_test_' +site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')\n",
    "#     y_test_d =  pd.read_pickle(datafolder+site_d+'/y_test_' +site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'pos.pkl')    \n",
    "    \n",
    "#     #load model\n",
    "#     print('Running collectSHAP_cross_sub '+model_type+' on site '+site_m+'/'+site_d+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample+':'+str(fold), flush = True)\n",
    "#     model = pickle.load(open(datafolder+site_m+'/boosttrap_'+model_type+'_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_'+str(fold)+'.pkl', 'rb'))\n",
    "\n",
    "#     model = model[-3]\n",
    "\n",
    "#     #Generate cross site data     \n",
    "#     common_features = [x for x in X_test_d.columns if x in X_train_m.columns]\n",
    "\n",
    "#     X_train2_d = X_train_d[common_features].copy()\n",
    "#     X_test2_d = X_test_d[common_features].copy()\n",
    "\n",
    "#     X_train2_m = X_train_m.iloc[0:1].copy()\n",
    "#     X_test2_m = X_test_m.iloc[0:1].copy()\n",
    "\n",
    "#     X_train3_d = pd.concat([X_train2_m, X_train2_d]).iloc[1:]\n",
    "#     X_test3_d = pd.concat([X_test2_m, X_test2_d]).iloc[1:]\n",
    "\n",
    "#     X_train3_d.loc[:,X_train2_m.dtypes==bool] = X_train3_d.loc[:,X_train2_m.dtypes==bool].fillna(False)\n",
    "#     X_test3_d.loc[:,X_test2_m.dtypes==bool] = X_test3_d.loc[:,X_test2_m.dtypes==bool].fillna(False)        \n",
    "\n",
    "#     X_train = X_train3_d\n",
    "#     X_test = X_test3_d    \n",
    "#     y_train = y_train_d\n",
    "#     y_test = y_test_d   \n",
    "\n",
    "#     for unmatch in X_test.dtypes[X_test.dtypes != X_test_m.dtypes].keys():\n",
    "#         if X_test_m.dtypes[unmatch] == bool:\n",
    "#             X_test[unmatch] = False\n",
    "#             X_train[unmatch] = False                \n",
    "#         else:\n",
    "#             X_test[unmatch] = np.nan        \n",
    "#             X_train[unmatch] = np.nan        \n",
    "\n",
    "\n",
    "#     shapX = pd.concat([X_train, X_test])\n",
    "#     shapy = pd.concat([y_train, y_test])\n",
    "\n",
    "#     # Calculate SHAP value\n",
    "#     if type(model) == xgboost.sklearn.XGBClassifier:\n",
    "#         dshap  = xgb.DMatrix(shapX, label=shapy)\n",
    "#         shap = model.get_booster().predict(dshap, pred_contribs=True)\n",
    "#     elif type(model) == catboost.core.CatBoostClassifier:\n",
    "#         cat_features = model.get_cat_feature_indices()\n",
    "#         pshap = Pool(data=shapX, label=shapy, cat_features=cat_features)        \n",
    "#         shap = model.get_feature_importance(data=pshap, type='ShapValues')\n",
    "#     else:\n",
    "#     #Using shap package example\n",
    "#         import shap\n",
    "#         explainer = shap.Explainer(model, algorithm='permutation')\n",
    "#         shap_valuesX = explainer.shap_values(shapX)\n",
    "#         #shap.summary_plot(shap_valuesX, X_test, plot_type=\"bar\")    \n",
    "#         shap = shap_valuesX    \n",
    "\n",
    "#     toc = time.perf_counter()\n",
    "#     print(f\"{site_m}/{site_d}:{year} finished in {toc - tic:0.4f} seconds\")  \n",
    "#     print('Finished collectSHAP_cross_sub '+model_type+' on site '+site_m+'/'+site_d+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample+':'+str(fold), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bea04a-63da-45cd-99c0-f5c255b27d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_data = model.get_feature_importance(prettified=True)\n",
    "# model_data['Feature'] = model_data['Feature Id']\n",
    "# model_data = model_data >> select('Feature', 'Importances')\n",
    "# model_data['rank'] = model_data['Importances'].rank(method='min', ascending=False)     \n",
    "# used_feature = list((model_data >> mask(X.Importances!=0)).Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d9bf4-21cd-42b1-9b99-746e408c8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = pickle.load(open(datafolder+site_m+'/boosttrap_'+model_type+'_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+'_'+str(fold)+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40700c95-e2a3-4d06-b616-e29911d5835d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_CDM_PY",
   "language": "python",
   "name": "aki_cdm_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
