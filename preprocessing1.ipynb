{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78aab9-cd69-43fb-a75c-b8eb0770ad31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "depreciated. see preprocessing1.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13bf8e-6305-477c-92ff-8ae7a84a0b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This module contains a set of function that process different PCORNET table \n",
    "Split processing into per year\n",
    "Long table into wide format\n",
    "One hot for boolean values\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa9025-8c48-4422-b94e-f392f852a4c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637c98b-99c0-4dd1-b2ef-a58caa4bff17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73408152-e69e-4f95-9917-6172455f9745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "#import rpy2.robjects as robjects\n",
    "#from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234beb21-03eb-4f16-99f6-7d1e309f7e05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "#import rpy2.robjects as robjects\n",
    "#from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import logging\n",
    "from sys import getsizeof\n",
    "import utils_function\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac3eb8-cdac-4e0e-8f8f-374313600913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfed57b-b7c6-4a7c-9e2a-cc2362f239d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def onset_old(site, year):                \n",
    "'''\n",
    "The module process the onset table\n",
    "1) Split iinto years\n",
    "2) Define the onset time as the last stage\n",
    "\n",
    "Input:\n",
    "p0_onset_{site}.pkl - Long format PCORNET table\n",
    "\n",
    "Output:\n",
    "onset_{site}_{str(year)}.pkl - vital table (cont)\n",
    "'''\n",
    "\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    # load tables    \n",
    "    onset = pd.read_pickle(datafolder+site+'/p0_onset_'+site+'.pkl')        \n",
    "\n",
    "    print('Running onset on site '+site+\":\"+str(year), flush = True)            \n",
    "    #get paitient by year\n",
    "    onset.loc[:,'ADMIT_DATE'] = pd.to_datetime(onset['ADMIT_DATE'])\n",
    "    onset_yr = onset.query(\"ADMIT_DATE >= '\"+str(year)+\"/01/01' and ADMIT_DATE <= '\"+str(year)+\"/12/31'\")\n",
    "\n",
    "    # get non-AKI paitients\n",
    "    onset_yr_aki0 = onset_yr[onset_yr[\"NONAKI_SINCE_ADMIT\"].notnull()]\n",
    "    onset_yr_aki0_select = onset_yr_aki0[[\"PATID\", \"ENCOUNTERID\", \"NONAKI_SINCE_ADMIT\"]]\n",
    "    onset_yr_aki0_select = onset_yr_aki0_select.assign(FLAG = 0)\n",
    "    onset_yr_aki0_select = onset_yr_aki0_select >> rename(SINCE_ADMIT=X.NONAKI_SINCE_ADMIT)\n",
    "\n",
    "    # Get AKI1 paitients    \n",
    "    onset_yr_aki1 = onset_yr[np.logical_and(onset_yr[\"AKI1_SINCE_ADMIT\"].notnull(), np.logical_and(onset_yr[\"AKI2_SINCE_ADMIT\"].isnull(), onset_yr[\"AKI3_SINCE_ADMIT\"].isnull()))]\n",
    "    onset_yr_aki1_select = onset_yr_aki1[[\"PATID\", \"ENCOUNTERID\", \"AKI1_SINCE_ADMIT\"]]\n",
    "    onset_yr_aki1_select = onset_yr_aki1_select.assign(FLAG = 1)\n",
    "    onset_yr_aki1_select = onset_yr_aki1_select >> rename(SINCE_ADMIT=X.AKI1_SINCE_ADMIT)\n",
    "    \n",
    "    # Get AKI2 paitients    \n",
    "    onset_yr_aki2 = onset_yr[np.logical_and(onset_yr[\"AKI2_SINCE_ADMIT\"].notnull(), onset_yr[\"AKI3_SINCE_ADMIT\"].isnull())]\n",
    "    onset_yr_aki2_select = onset_yr_aki2[[\"PATID\", \"ENCOUNTERID\", \"AKI2_SINCE_ADMIT\"]]\n",
    "    onset_yr_aki2_select = onset_yr_aki2_select.assign(FLAG = 2)\n",
    "    onset_yr_aki2_select = onset_yr_aki2_select >> rename(SINCE_ADMIT=X.AKI2_SINCE_ADMIT)    \n",
    "\n",
    "    # Get AKI3 paitients    \n",
    "    onset_yr_aki3 = onset_yr[onset_yr[\"AKI3_SINCE_ADMIT\"].notnull()]\n",
    "    onset_yr_aki3_select = onset_yr_aki3[[\"PATID\", \"ENCOUNTERID\", \"AKI3_SINCE_ADMIT\"]]\n",
    "    onset_yr_aki3_select = onset_yr_aki3_select.assign(FLAG = 3)\n",
    "    onset_yr_aki3_select = onset_yr_aki3_select >> rename(SINCE_ADMIT=X.AKI3_SINCE_ADMIT)        \n",
    "    \n",
    "    newdf = pd.concat([onset_yr_aki1_select, onset_yr_aki0_select, onset_yr_aki2_select, onset_yr_aki3_select], axis=0, sort=False).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    # This Table has to be straightly nonnull, (sometime the SINCE_ADMIT is null)\n",
    "    # onset_yr_aki0_select = onset_yr_aki0_select.dropna()\n",
    "    # onset_yr_aki1_select = onset_yr_aki1_select.dropna()\n",
    "    # onset_yr_aki2_select = onset_yr_aki2_select.dropna()\n",
    "    # onset_yr_aki3_select = onset_yr_aki3_select.dropna()\n",
    "    \n",
    "    newdf = newdf.dropna()\n",
    "    #Save table   \n",
    "    newdf.to_pickle(datafolder+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    print('Finished onset on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d72fc3-4d61-45c6-816b-51b9effefdd3",
   "metadata": {
    "tags": [
     "virat"
    ]
   },
   "outputs": [],
   "source": [
    "def onset(site, year):                \n",
    "'''\n",
    "The module process the onset table\n",
    "1) Split iinto years\n",
    "2) Define the onset time as the last stage\n",
    "\n",
    "Input:\n",
    "p0_onset_{site}.pkl - Long format PCORNET table\n",
    "\n",
    "Output:\n",
    "onset_{site}_{str(year)}.pkl - vital table (cont)\n",
    "'''\n",
    "\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "    \n",
    "    # load tables    \n",
    "    onset = pd.read_pickle(datafolder+site+'/p0_onset_'+site+'.pkl')        \n",
    "\n",
    "    print('Running onset on site '+site+\":\"+str(year), flush = True)            \n",
    "    #get paitient by year\n",
    "    onset.loc[:,'ADMIT_DATE'] = pd.to_datetime(onset['ADMIT_DATE'])\n",
    "    onset_yr = onset.query(\"ADMIT_DATE >= '\"+str(year)+\"/01/01' and ADMIT_DATE <= '\"+str(year)+\"/12/31'\")\n",
    "    \n",
    "    onset_yr = onset_yr[[\"PATID\", \"ENCOUNTERID\", \"SINCE_ADMIT\", \"AKI_STAGE\"]]\n",
    "    onset_yr.columns = [[\"PATID\", \"ENCOUNTERID\", \"SINCE_ADMIT\", \"FLAG\"]]\n",
    "    \n",
    "    \n",
    "    newdf.to_pickle(datafolder+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    print('Finished onset on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc98db5-5743-4ded-8dcf-fe370e2a7c4f",
   "metadata": {
    "tags": [
     "virat"
    ]
   },
   "outputs": [],
   "source": [
    "def vital(site, year):\n",
    "'''\n",
    "The module process the vital table to get the last avaliable data 24 hour before onset\n",
    "1) Include only all data 1 day before onset\n",
    "2) Calculate Daily Average\n",
    "3) Drop all data before admit\n",
    "4) Collect last avaliable data\n",
    "\n",
    "Input:\n",
    "p0_vital_{site}.pkl - Long format PCORNET table\n",
    "\n",
    "Output:\n",
    "vital_{site}_{str(year)}.pkl - vital table (cont)\n",
    "'''\n",
    "\n",
    "    print('Running vital on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']\n",
    "\n",
    "    # load tables\n",
    "#    vital = pd.read_pickle(datafolder+site+'/p0_vital_'+site+'.pkl')\n",
    "    vital = pd.read_pickle(datafolder+site+'/p0_vital_'+site+'_drop.pkl')\n",
    "    \n",
    "    # Get the patient records in onset\n",
    "    # Calculate 'FUTURE' column as ONSET_DAY-MEASURE_DAY\n",
    "    newdfX = pd.read_pickle(datafolder+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID', 'SINCE_ADMIT') >> mutate(dummy = True)\n",
    "    vital = (pd.merge(vital, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy) >> mutate(FUTURE=X.SINCE_ADMIT-X.DAYS_SINCE_ADMIT)).reset_index(drop=True)\n",
    "    \n",
    "    #24 hours prediction \n",
    "    vital = vital[vital['FUTURE']>0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "    #Only include in-hosipital record\n",
    "    vital = vital[vital['DAYS_SINCE_ADMIT']>=0]\n",
    "    \n",
    "    #Calculate daily average\n",
    "    vital_mean = vital.groupby(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).mean().reset_index()\n",
    "\n",
    "    #Transform vital Table (Row over the previous value if unknown) (Continuous)\n",
    "    vital_list = []\n",
    "    #Vital table drop data before admit\n",
    "    vital_sys = vital_mean >> select('PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT', 'SYSTOLIC')\n",
    "    vital_dia = vital_mean >> select('PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT', 'DIASTOLIC')\n",
    "    vital_bmi = vital_mean >> select('PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT', 'ORIGINAL_BMI')\n",
    "    vital_wt = vital_mean >> select('PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT', 'WT')\n",
    "   \n",
    "    #get the last avaliable value                                \n",
    "    vital_sys_p = vital_sys.dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID']).agg({'SYSTOLIC':'last'}).reset_index()    \n",
    "    vital_dia_p = vital_dia.dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID']).agg({'DIASTOLIC':'last'}).reset_index()    \n",
    "    vital_bmi_p = vital_bmi.dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID']).agg({'ORIGINAL_BMI':'last'}).reset_index()    \n",
    "    vital_wt_p  =  vital_wt.dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID']).agg({'WT':'last'}).reset_index()    \n",
    "\n",
    "    \n",
    "    #Combine back into one vital table\n",
    "    vital_t = pd.merge(vital_sys_p, vital_dia_p, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    vital_t = pd.merge(vital_t, vital_bmi_p, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    vital_t = pd.merge(vital_t, vital_wt_p, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='outer')        \n",
    "\n",
    "    #Save table\n",
    "    vital_t.to_pickle(datafolder+site+'/vital_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    #consistency check\n",
    "    if vital_t.empty:\n",
    "        logging.basicConfig(filename='vital.log', filemode='a')    \n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('vital: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "\n",
    "    print('Finished vital on site '+site+\":\"+str(year), flush = True)\n",
    "#    return vital_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09fad5-20e4-4aca-a692-542e2b202a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def demo(site, year):\n",
    "'''\n",
    "The module process the demographic table\n",
    "1) Extract demographic record f patients in onset\n",
    "2) Transform 'SEX', 'RACE', 'HISPANIC' into onehot vector\n",
    "\n",
    "Input:\n",
    "p0_demo_{site}.pkl - Long format PCORNET table\n",
    "\n",
    "Output:\n",
    "demo_{site}_{str(year)}.pkl - Demo table (AGE+boolean)\n",
    "'''\n",
    "    print('Running demo on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']    \n",
    "    \n",
    "    # load tables\n",
    "    demo = pd.read_pickle(datafolder+site+'/p0_demo_'+site+'.pkl')\n",
    "    \n",
    "    # Get the patient records in onset\n",
    "    newdfX = pd.read_pickle(datafolder+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID') >> mutate(dummy = True) >> distinct()\n",
    "    demo = (pd.merge(demo, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy)).reset_index(drop=True)\n",
    "\n",
    "    #onehot transform demo \n",
    "    var = ['SEX', 'RACE', 'HISPANIC']\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(demo[var])\n",
    "    demo_onehot_cat = pd.DataFrame(enc.transform(demo[var]).toarray(), columns=enc.get_feature_names(var)).astype('bool')\n",
    "    demo_one = pd.concat([demo[['PATID', 'ENCOUNTERID', 'AGE']].reset_index(), demo_onehot_cat], axis=1).drop('index',axis=1)    \n",
    "    \n",
    "    #Save table\n",
    "    demo_one.to_pickle(datafolder+site+'/demo_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    #consistency check\n",
    "    if demo_one.empty:\n",
    "        logging.basicConfig(filename='demo.log', filemode='a')    \n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('demo: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "\n",
    "    print('Finished demo on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8695a77-1e05-4a88-81e6-92e0e195b766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dx(site, year):\n",
    "'''\n",
    "The module process the diagnosis table for commorbidity (records before admission) \n",
    "1) Include only all data before admission\n",
    "2) Translate ICD10 to ICD9 if possible\n",
    "3) Roll icd code to 3 digit\n",
    "4) Seperate Comorbidity into >6 months and <6 months before admission\n",
    "\n",
    "Input:\n",
    "p0_dx_{site}.pkl - Long format PCORNET table\n",
    "\n",
    "Output:\n",
    "dx_{site}_{str(year)}.pkl - One hot dx table (boolean)\n",
    "'''    \n",
    "\n",
    "    # dx\n",
    "    print('Running dx on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory2 = configs_variables['home_directory']      \n",
    "    \n",
    "    # load table\n",
    "    dx = pd.read_pickle(datafolder+site+'/p0_dx_'+site+'.pkl')\n",
    "    \n",
    "    # Get the patient records in onset\n",
    "    newdfX = pd.read_pickle(datafolder+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID', 'SINCE_ADMIT') >> mutate(dummy = True)\n",
    "    dx = (pd.merge(dx, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy)d).reset_index(drop=True)\n",
    "\n",
    "    #Only include historical records\n",
    "    dx = dx[dx['DAYS_SINCE_ADMIT']<0].drop(['SINCE_ADMIT'],axis=1)\n",
    "        \n",
    "    #Some site use 9 some site use 09\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].where(dx['DX_TYPE'] != '9', '09')\n",
    "    \n",
    "    # ICD10 -> ICD09\n",
    "    icd10toicd09 = pd.read_csv(home_directory+'2018_I10gem.csv',sep=',')\n",
    "    \n",
    "    icd10toicd09.columns = ['DX', 'DX09']\n",
    "    dx4 = dx >> mask(X.DX_TYPE == '10')\n",
    "    dx4['DX'] = dx4['DX'].map(lambda x: x.replace('.',''))\n",
    "    dx4 = dx4 >> left_join(icd10toicd09, by='DX')\n",
    "\n",
    "    #Keep icd10 if no match\n",
    "    dx4['DX_TYPE'] = dx4['DX_TYPE'].where(dx4['DX09'].isnull(), '09')\n",
    "    dx4['DX'] = dx4['DX'].where(dx4['DX09'].isnull(), dx4['DX09'])\n",
    "    dx4 = dx4.drop('DX09', axis=1)\n",
    "    dx = pd.concat([dx >> mask(X.DX_TYPE != '10'), dx4], axis=0)\n",
    "\n",
    "    # Roll icd 09 code up native\n",
    "    dx['DX'] = dx['DX'].where(dx['DX_TYPE'] != '09', dx['DX'].map(lambda x: x[0:3]))\n",
    "\n",
    "    # Roll icd 10 code up native\n",
    "    dx['DX'] = dx['DX'].where(dx['DX_TYPE'] != '10', dx['DX'].map(lambda x: x[0:3]))\n",
    "    \n",
    "    # Transform dx table (Historical data: Yes if any diagnoasis show up)  (Boolean)\n",
    "    # Fillna if no record\n",
    "    dx['sixmonth'] = '<6'\n",
    "    dx['sixmonth'] = dx['sixmonth'].where(dx['DAYS_SINCE_ADMIT']<-365/2, '>6') # becareful negative number\n",
    "    dx_t = dx >> mutate(DX='DX:'+X.DX_TYPE+\":\"+X.DX+X.sixmonth) >> drop('DX_TYPE') >> drop('sixmonth')\n",
    "    dx_t = (dx_t >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='DX', values='dummy').fillna(False).reset_index()\n",
    "\n",
    "    #Save table\n",
    "    dx_t.to_pickle(datafolder+site+'/dx_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    #consistency check\n",
    "    if dx_t.empty:\n",
    "        logging.basicConfig(filename='dx.log', filemode='a')\n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('dx: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "    \n",
    "    #consistency check2\n",
    "        \n",
    "    print('Finished dx on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad611cd5-bd0b-4212-bad9-48479593f82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def px(site, year):\n",
    "'''\n",
    "The module process the procedure table to get the last avaliable data 24 hour before onset\n",
    "1) Include only all data before admission\n",
    "2) Drop all data before admit\n",
    "\n",
    "Input:\n",
    "p0_px_{site}.pkl - Long format PCORNET table\n",
    "\n",
    "Output:\n",
    "px_{site}_{str(year)}.pkl - One hot px table (boolean)\n",
    "'''    \n",
    "\n",
    "    print('Running px on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory']        \n",
    "    \n",
    "    #load table\n",
    "    px = pd.read_pickle(datafolder+site+'/p0_px_'+site+'.pkl')\n",
    "    \n",
    "    # Get the patient records in onset\n",
    "    # Calculate 'FUTURE' column as ONSET_DAY-MEASURE_DAY\n",
    "    newdfX = pd.read_pickle(datafolder+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID', 'SINCE_ADMIT') >> mutate(dummy = True)\n",
    "    px = (pd.merge(px, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy) >> mutate(FUTURE=X.SINCE_ADMIT-X.DAYS_SINCE_ADMIT)).reset_index(drop=True)\n",
    "\n",
    "    #24 hours prediction\n",
    "    px = px[px['FUTURE']>0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "    #Only include in-hosipital record\n",
    "    px = px[px['DAYS_SINCE_ADMIT']>=0]\n",
    "    \n",
    "    #Some site use 9 some site use 09\n",
    "    px['PX_TYPE'] = px['PX_TYPE'].where(px['PX_TYPE'] != '9', '09')\n",
    "    \n",
    "    # drop unused column\n",
    "    px = px >> mutate(PX='PX:'+X.PX_TYPE+\":\"+X.PX) >> drop('PX_TYPE')\n",
    "\n",
    "    # Transform px table (Boolean)\n",
    "    # Fillna if no record\n",
    "    px_t = (px >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='PX', values='dummy').fillna(False).reset_index()\n",
    "\n",
    "    #Save table\n",
    "    px_t.to_pickle(datafolder+site+'/px_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    if px_t.empty:\n",
    "        logging.basicConfig(filename='px.log', filemode='a')\n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('px: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "\n",
    "\n",
    "    print('Finished px on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd021f9-ff5b-4847-9284-24bef8ce2dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lab(site, year):\n",
    "'''\n",
    "The module process the lab table to get the last avaliable data 24 hour before onset\n",
    "1) Include only all data 1 day before onset\n",
    "2) Drop all data before admit\n",
    "3) Calculate Daily Average\n",
    "4) Seperate into numeric lab and categoricl lab\n",
    "5) (Numeric) Calucalte Daily average\n",
    "6) Collect last avaliable data\n",
    "\n",
    "Input:\n",
    "p0_lab_g_{site}.pkl - Long format PCORNET table (unit unified)\n",
    "\n",
    "Output:\n",
    "lab_{site}_{str(year)}.pkl - One hot px table\n",
    "'''    \n",
    "\n",
    "    print('Running lab on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory'] \n",
    "    \n",
    "    #load table\n",
    "    lab = pd.read_pickle(datafolder+site+'/p0_lab_g_'+site+'.pkl')\n",
    "    \n",
    "    # Get the patient records in onset\n",
    "    # Calculate 'FUTURE' column as ONSET_DAY-MEASURE_DAY\n",
    "    newdfX = pd.read_pickle(datafolder+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID', 'SINCE_ADMIT') >> mutate(dummy = True)\n",
    "    lab = (pd.merge(lab, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy) >> mutate(FUTURE=X.SINCE_ADMIT-X.DAYS_SINCE_ADMIT)).reset_index(drop=True)\n",
    "\n",
    "    #24 hours prediciton\n",
    "    lab = lab[lab['FUTURE']>0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "    #Only include in-hosipital record\n",
    "    lab = lab[lab['DAYS_SINCE_ADMIT']>=0]\n",
    "\n",
    "    #seperate into numberic lab and pos/neg lab\n",
    "    #calculate categorical first\n",
    "    lab_cat = lab.loc[lab['RESULT_NUM'].isnull()]\n",
    "    if lab_cat.empty:\n",
    "        labcat_t = lab >> select('PATID','ENCOUNTERID')\n",
    "        labcat_t.to_pickle(datafolder+site+'/labcat_'+site+'_'+str(year)+'.pkl')\n",
    "    else:\n",
    "        lab_mode = lab_cat.loc[:, ['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT', 'RESULT_QUAL']].groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT']).agg(pd.Series.mode).reset_index()\n",
    "        lab_mode_nnd = lab_mode.loc[lab_mode['RESULT_QUAL'].apply(type) == str].copy()\n",
    "        lab_mode_nd = lab_mode.loc[lab_mode['RESULT_QUAL'].apply(type) != str].copy()\n",
    "        pattern = '[\\[\\]\\']'\n",
    "        lab_mode_nd.loc[:,'RESULT_QUAL'] = lab_mode_nd['RESULT_QUAL'].apply(lambda x: re.sub(pattern, \"\", np.array2string(x,separator='-')))\n",
    "        lab_mode = pd.concat([lab_mode_nd, lab_mode_nnd], ignore_index=True)\n",
    "\n",
    "        labcat_t = lab_mode.sort_values(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC']).agg({'RESULT_QUAL':'last'}).reset_index()\n",
    "        labcat_t = labcat_t >> mutate(LAB_LOINC='LAB:'+\":\"+X.LAB_LOINC+\"(\"+X.RESULT_QUAL+\")\") >> mutate(dummy = True) >> select('PATID', 'ENCOUNTERID', 'LAB_LOINC', 'dummy')\n",
    "        labcat_t = labcat_t.pivot(index=['PATID', 'ENCOUNTERID'], columns='LAB_LOINC', values='dummy').fillna(False).reset_index()        \n",
    "        #Save table    \n",
    "        labcat_t.to_pickle(datafolder+site+'/labcat_'+site+'_'+str(year)+'.pkl') \n",
    "    \n",
    "    #calculate numerica    \n",
    "    lab_num = lab.loc[lab['RESULT_NUM'].notnull()]  \n",
    "    if lab_num.empty:\n",
    "        labnum_t = lab >> select('PATID','ENCOUNTERID')\n",
    "        labnum_t.to_pickle(datafolder+site+'/labnum_'+site+'_'+str(year)+'.pkl')        \n",
    "    else:    \n",
    "        #Calculate daily average\n",
    "        lab_mean = lab_num.groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'RESULT_UNIT', 'DAYS_SINCE_ADMIT']).agg({'RESULT_NUM':'mean'}).reset_index()\n",
    "        lab_mean = lab_mean >> mutate(LAB_LOINC='LAB:'+\":\"+X.LAB_LOINC+\"(\"+X.RESULT_UNIT+\")\")\n",
    "        labnum_t = lab_mean.sort_values(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC']).agg({'RESULT_NUM':'last'}).reset_index()\n",
    "        labnum_t = labnum_t.pivot(index=['PATID', 'ENCOUNTERID'], columns='LAB_LOINC', values='RESULT_NUM').reset_index()\n",
    "        #Save table\n",
    "        labnum_t.to_pickle(datafolder+site+'/labnum_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "\n",
    "    if labnum_t.empty or labcat_t.empty:\n",
    "        logging.basicConfig(filename='lab.log', filemode='a')\n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('lab: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()   \n",
    "\n",
    "    print('Finished lab on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719d7ce-469b-4c46-be2f-24050dc3f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amed(site, year):\n",
    "'''\n",
    "The module process the amed table for medication to get the last avaliable data 24 hour before onset\n",
    "1) Include only all data 1 day before onset\n",
    "2) Drop all data before admit\n",
    "3) Convert rxnorm to atc code\n",
    "4) Convert ndc -> atc code\n",
    "\n",
    "Input:\n",
    "p0_amed_{site}.pkl - Long format PCORNET table (unit unified)\n",
    "\n",
    "Output:\n",
    "amed_{site}_{str(year)}.pkl - One hot px table\n",
    "'''    \n",
    "\n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory'] \n",
    "    \n",
    "    print('Running amed on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    #load table\n",
    "    amed = pd.read_pickle(datafolder+site+'/p0_amed_'+site+'.pkl')\n",
    "    \n",
    "    # Get the patient records in onset\n",
    "    # Calculate 'FUTURE' column as ONSET_DAY-MEASURE_DAY\n",
    "    newdfX = pd.read_pickle(datafolder+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID', 'SINCE_ADMIT') >> mutate(dummy = True)\n",
    "    amed = (pd.merge(amed, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy) >> mutate(FUTURE=X.SINCE_ADMIT-X.DAYS_SINCE_ADMIT)).reset_index(drop=True)\n",
    "\n",
    "    #24 hours prediction\n",
    "    amed = amed[amed['FUTURE']>0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "    #Only include in-hosipital record\n",
    "    amed = amed[amed['DAYS_SINCE_ADMIT']>=0]\n",
    "    \n",
    "    # rxnorm -> atc\n",
    "    amed_rx = amed.loc[amed['MEDADMIN_TYPE'] == \"RX\"]\n",
    "    if not amed_rx.empty:\n",
    "        # pd.DataFrame(amed['MEDADMIN_CODE'].unique()).to_csv('/home/hchan2/AKI/AKI_Python/data/'+site+'/rxnormtmp.csv', sep=',', index=False, header = False)\n",
    "        # Go to run rxnorm2atcR.ipynb NOW\n",
    "        rxcui2atc_dtypes =  {\"Rxcui\": 'object', \"ATC4th\": 'object'}    \n",
    "        rxcui2atc = pd.read_csv(datafolder+site+'/rxnorm_out_'+site+'.csv',sep=',', dtype=(rxcui2atc_dtypes)) >> rename(MEDADMIN_CODE=X.Rxcui)\n",
    "        amed_rx = amed_rx >> left_join(rxcui2atc, by='MEDADMIN_CODE')\n",
    "        amed_rx['MEDADMIN_TYPE'] = amed_rx['MEDADMIN_TYPE'].where(amed_rx['ATC4th'].isnull(), 'ATC')\n",
    "        amed_rx['MEDADMIN_CODE'] = amed_rx['MEDADMIN_CODE'].where(amed_rx['ATC4th'].isnull(), amed_rx['ATC4th'])\n",
    "        amed_rx = amed_rx >> mutate(MEDADMIN_CODE='MED:'+X.MEDADMIN_TYPE+':'+X.MEDADMIN_CODE)\n",
    "        amed_rx = amed_rx >> select('PATID', 'ENCOUNTERID', 'MEDADMIN_CODE', 'DAYS_SINCE_ADMIT')\n",
    "    else:\n",
    "        amed_rx = amed_rx >> select('PATID', 'ENCOUNTERID', 'MEDADMIN_CODE', 'DAYS_SINCE_ADMIT')\n",
    "    \n",
    "    # ndc -> atc\n",
    "    amed_ndc = amed.loc[amed['MEDADMIN_TYPE'] == \"ND\"]    \n",
    "    if not amed_ndc.empty:    \n",
    "        # pd.DataFrame(amed['MEDADMIN_CODE'].unique()).to_csv('/home/hchan2/AKI/AKI_Python/data/'+site+'/rxnormtmp.csv', sep=',', index=False, header = False)\n",
    "        # Go to run rxnorm2atcR.ipynb NOW\n",
    "        ndc2atc_dtypes =  {\"ndc\": 'object', \"ATC4th\": 'object'}    \n",
    "        ndc2atc = pd.read_csv(datafolder+site+'/ndc_out_'+site+'.csv',sep=',', dtype=(ndc2atc_dtypes)) >> rename(MEDADMIN_CODE=X.ndc)\n",
    "        amed_ndc = amed_ndc >> left_join(ndc2atc, by='MEDADMIN_CODE')\n",
    "        amed_ndc['MEDADMIN_TYPE'] = amed_ndc['MEDADMIN_TYPE'].where(amed_ndc['ATC4th'].isnull(), 'ATC')\n",
    "        amed_ndc['MEDADMIN_CODE'] = amed_ndc['MEDADMIN_CODE'].where(amed_ndc['ATC4th'].isnull(), amed_ndc['ATC4th'])\n",
    "        amed_ndc = amed_ndc >> mutate(MEDADMIN_CODE='MED:'+X.MEDADMIN_TYPE+':'+X.MEDADMIN_CODE)\n",
    "        amed_ndc = amed_ndc >> select('PATID', 'ENCOUNTERID', 'MEDADMIN_CODE', 'DAYS_SINCE_ADMIT')\n",
    "    else:\n",
    "        amed_ndc = amed_ndc >> select('PATID', 'ENCOUNTERID', 'MEDADMIN_CODE', 'DAYS_SINCE_ADMIT')\n",
    "       \n",
    "    amed = pd.concat([amed_rx, amed_ndc], axis=0, ignore_index=True)   \n",
    "\n",
    "    # Transform amed table (Boolean)\n",
    "    # Fillna if no record    \n",
    "    amed_t = (amed >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='MEDADMIN_CODE', values='dummy').fillna(False).reset_index()\n",
    "        \n",
    "    #Save table\n",
    "    amed_t.to_pickle(datafolder+site+'/amed_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    if amed_t.empty:\n",
    "        logging.basicConfig(filename='amed.log', filemode='a')\n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('amed: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()   \n",
    "\n",
    "    print('Finished amed on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476bcea6-e5b7-4f3b-83a1-ad6c41b680f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_lab(site):\n",
    "'''\n",
    "The module process convert lab units and attempt to group LONIC into groups into common units if possible\n",
    "\n",
    "Input:\n",
    "p0_lab_{site}.pkl - Long format PCORNET table (unit unified)\n",
    "\n",
    "Output:\n",
    "p0_lab_g_{site}}.pkl - One hot px table\n",
    "'''        \n",
    "    print('Running unify lab on site '+site, flush = True)\n",
    "    \n",
    "    configs_variables = utils_function.read_config(site)\n",
    "    datafolder = configs_variables['datafolder']\n",
    "    home_directory = configs_variables['home_directory'] \n",
    "    \n",
    "    UCUMunitX = pd.read_csv('UCUMunitX.csv')\n",
    "    local_custom_convert =  pd.read_csv('local_custom_convert.csv')\n",
    "    UCUMqualX = pd.read_csv('UCUMqualX.csv')\n",
    "    loincmap3 =pd.read_csv(home_directory+'loinc/AccessoryFiles/GroupFile/GroupLoincTerms.csv') \n",
    "    \n",
    "    labtest = pd.read_pickle(datafolder+site+'/p0_lab_'+site+'_drop.pkl')\n",
    "    labtest['site']=site\n",
    "\n",
    "    labtest2 = labtest.merge(local_custom_convert, on = ['LAB_LOINC', 'site'], how='left')\n",
    "    labtest2['NEW_UNIT'] = np.where(labtest2['TARGET_UNIT'].notnull(), labtest2['TARGET_UNIT'], labtest2['RESULT_UNIT'])\n",
    "    labtest2['NEW_RESULT_NUM'] = np.where(labtest2['TARGET_UNIT'].notnull(), labtest2['Multipliyer']*labtest2['RESULT_NUM'], labtest2['RESULT_NUM'])\n",
    "\n",
    "    labtest3 = labtest2.copy()\n",
    "    labtest3['RESULT_UNIT'] = labtest3['NEW_UNIT']\n",
    "    labtest3['RESULT_NUM'] = labtest3['NEW_RESULT_NUM']\n",
    "    labtest3 = labtest3.drop(['NEW_UNIT', 'NEW_RESULT_NUM', 'SOURCE_UNIT', 'TARGET_UNIT', 'LONG_COMMON_NAME', 'Multipliyer'], axis=1)\n",
    "\n",
    "    labtest4 = labtest3.merge(UCUMunitX, on = ['LAB_LOINC', 'RESULT_UNIT'], how='left').copy()\n",
    "    labtest4['NEW_UNIT'] = np.where(labtest4['FINAL_UNIT'].notnull(), labtest4['FINAL_UNIT'], labtest4['RESULT_UNIT'])\n",
    "    labtest4['NEW_RESULT_NUM'] = np.where(labtest4['FINAL_UNIT'].notnull(), labtest4['factor_final']*labtest4['RESULT_NUM'], labtest4['RESULT_NUM'])\n",
    "    labtest4['NEW_LAB_LOINC'] = np.where(labtest4['FINAL_UNIT'].notnull(), labtest4['GroupId'], labtest4['LAB_LOINC'])\n",
    "    labtest4['RESULT_UNIT'] = labtest4['NEW_UNIT']\n",
    "    labtest4['RESULT_NUM'] = labtest4['NEW_RESULT_NUM']\n",
    "    labtest4['LAB_LOINC'] = labtest4['NEW_LAB_LOINC']\n",
    "    labtest4 = labtest4.drop(['GroupId', 'EXAMPLE_UCUM_UNITS',\n",
    "           'EXAMPLE_UCUM_UNITS_FINAL', 'RESULT_UNIT_CONSENSUS', 'FINAL_UNIT',\n",
    "           'FINAL_Multiplyer', 'RESULT_UNIT_API', 'FINAL_UNIT_API', 'factor_final',\n",
    "           'NEW_UNIT', 'NEW_RESULT_NUM', 'NEW_LAB_LOINC'], axis=1)\n",
    "\n",
    "    mmc = loincmap3[loincmap3['Category']=='Mass-Molar conversion'][['GroupId']]\n",
    "    labtest4 = labtest4.merge(mmc, left_on = 'LAB_LOINC', right_on='GroupId', how='left', indicator=True)\n",
    "    labtest4 = labtest4[labtest4['_merge']=='left_only']\n",
    "    labtest4 = labtest4.drop(['GroupId', '_merge'],axis=1)\n",
    "\n",
    "    labtest5 = labtest4.copy()\n",
    "    labtest5 = labtest5.merge(UCUMqualX[['LAB_LOINC', 'GroupId']].drop_duplicates(), on='LAB_LOINC', how='left')\n",
    "    labtest5['NEW_LAB_LOINC'] = np.where(labtest5['GroupId'].notnull(), labtest5['GroupId'], labtest5['LAB_LOINC'])\n",
    "    labtest5['LAB_LOINC'] = labtest5['NEW_LAB_LOINC']\n",
    "    labtest5 = labtest5.drop(['GroupId','NEW_LAB_LOINC'],axis=1)\n",
    "    labtest5 = labtest5.drop('site',axis=1)\n",
    "    labtest5 = labtest5.drop_duplicates()\n",
    "    labtest5.to_pickle(datafolder+site+'/p0_lab_g_'+site+'.pkl')\n",
    "    \n",
    "    print('Finished unify lab on site '+site, flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c622a-0a92-4ae9-87ce-08bbf2e8f25f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_CDM_PY",
   "language": "python",
   "name": "aki_cdm_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
