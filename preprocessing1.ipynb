{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d08178-e835-46e5-ac89-5ef92ae014f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "#import rpy2.robjects as robjects\n",
    "#from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "import logging\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28621c5f-17be-4aa8-8f7a-89e740a9ef83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#onset\n",
    "def onset(site, year):                \n",
    "    datafolder = '/home/hoyinchan/blue/Data/data2021/data2021/'\n",
    "    home_directory = \"/home/hoyinchan/code/AKI_CDM_PY/\"\n",
    "    pred_end = 7    \n",
    "\n",
    "    # load tables    \n",
    "#   onset = pd.read_csv(datafolder+site+'/raw/'+'AKI_ONSETS'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "    onset = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/p0_onset_'+site+'.pkl')        \n",
    "#    onset = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/p0_onsetpy_'+site+'.pkl')        \n",
    "\n",
    "#    years = list(pd.to_datetime(onset['ADMIT_DATE']).dt.year.unique())\n",
    "\n",
    "    print('Running onset on site '+site+\":\"+str(year), flush = True)            \n",
    "    #get paitient by year\n",
    "    onset.loc[:,'ADMIT_DATE'] = pd.to_datetime(onset['ADMIT_DATE'])\n",
    "    onset_yr = onset.query(\"ADMIT_DATE >= '\"+str(year)+\"/01/01' and ADMIT_DATE <= '\"+str(year)+\"/12/31'\")\n",
    "#    onset_yr = onset    #Still need to seperate by year if file too big\n",
    "\n",
    "    # get non-AKI paitients\n",
    "    onset_yr_aki0 = onset_yr[onset_yr[\"NONAKI_SINCE_ADMIT\"].notnull()]\n",
    "    onset_yr_aki0_select = onset_yr_aki0[[\"PATID\", \"ENCOUNTERID\", \"NONAKI_SINCE_ADMIT\"]]\n",
    "    onset_yr_aki0_select = onset_yr_aki0_select.assign(FLAG = 0)\n",
    "    onset_yr_aki0_select = onset_yr_aki0_select >> rename(SINCE_ADMIT=X.NONAKI_SINCE_ADMIT)\n",
    "\n",
    "    # Get AKI1 paitients    \n",
    "    onset_yr_aki1 = onset_yr[np.logical_and(onset_yr[\"AKI1_SINCE_ADMIT\"].notnull(), np.logical_and(onset_yr[\"AKI2_SINCE_ADMIT\"].isnull(), onset_yr[\"AKI3_SINCE_ADMIT\"].isnull()))]\n",
    "    onset_yr_aki1_select = onset_yr_aki1[[\"PATID\", \"ENCOUNTERID\", \"AKI1_SINCE_ADMIT\"]]\n",
    "    onset_yr_aki1_select = onset_yr_aki1_select.assign(FLAG = 1)\n",
    "    onset_yr_aki1_select = onset_yr_aki1_select >> rename(SINCE_ADMIT=X.AKI1_SINCE_ADMIT)\n",
    "    \n",
    "    # Get AKI2 paitients    \n",
    "    onset_yr_aki2 = onset_yr[np.logical_and(onset_yr[\"AKI2_SINCE_ADMIT\"].notnull(), onset_yr[\"AKI3_SINCE_ADMIT\"].isnull())]\n",
    "    onset_yr_aki2_select = onset_yr_aki2[[\"PATID\", \"ENCOUNTERID\", \"AKI2_SINCE_ADMIT\"]]\n",
    "    onset_yr_aki2_select = onset_yr_aki2_select.assign(FLAG = 2)\n",
    "    onset_yr_aki2_select = onset_yr_aki2_select >> rename(SINCE_ADMIT=X.AKI2_SINCE_ADMIT)    \n",
    "\n",
    "    # Get AKI3 paitients    \n",
    "    onset_yr_aki3 = onset_yr[onset_yr[\"AKI3_SINCE_ADMIT\"].notnull()]\n",
    "    onset_yr_aki3_select = onset_yr_aki3[[\"PATID\", \"ENCOUNTERID\", \"AKI3_SINCE_ADMIT\"]]\n",
    "    onset_yr_aki3_select = onset_yr_aki3_select.assign(FLAG = 3)\n",
    "    onset_yr_aki3_select = onset_yr_aki3_select >> rename(SINCE_ADMIT=X.AKI3_SINCE_ADMIT)        \n",
    "    \n",
    "    newdf = pd.concat([onset_yr_aki1_select, onset_yr_aki0_select, onset_yr_aki2_select, onset_yr_aki3_select], axis=0, sort=False).reset_index(drop=True)\n",
    "#    newdf = pd.concat([onset_yr_aki1_select, onset_yr_aki0_select], axis=0, sort=False).reset_index(drop=True)\n",
    "#    newdf = pd.concat([onset_yr_aki2_select, onset_yr_aki1_select], axis=0, sort=False).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    # This Table has to be straightly nonnull, (sometime the SINCE_ADMIT is null)\n",
    "    # onset_yr_aki0_select = onset_yr_aki0_select.dropna()\n",
    "    # onset_yr_aki1_select = onset_yr_aki1_select.dropna()\n",
    "    # onset_yr_aki2_select = onset_yr_aki2_select.dropna()\n",
    "    # onset_yr_aki3_select = onset_yr_aki3_select.dropna()\n",
    "    \n",
    "    newdf = newdf.dropna()\n",
    "    #Save table   \n",
    "    newdf.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    print('Finished onset on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "   # no need, just use that day\n",
    "#   onset_yr_aki1_select.loc[onset_yr_aki1_select.AKI1_SINCE_ADMIT == 0, 'AKI1_SINCE_ADMIT'] = 1    \n",
    "    \n",
    "    #PATID = onset_yr_aki1_select['PATID'].tolist()\n",
    "    #ENCOUNTERID = onset_yr_aki1_select['ENCOUNTERID'].tolist()\n",
    "    #SINCE_ADMIT = onset_yr_aki1_select['AKI1_SINCE_ADMIT'].tolist()\n",
    "\n",
    "    #Expand data (Create row for each day for each encounter until pred end)\n",
    "    #newdf = pd.DataFrame(np.repeat(onset_yr_aki1_select.values,7,axis=0))\n",
    "    #newdf.columns = onset_yr_aki1_select.columns\n",
    "    #since_admit = [*range(7)]*onset_yr_aki1_select.shape[0]\n",
    "    #newdf['SINCE_ADMIT'] = since_admit\n",
    "    #conditions = [(newdf['SINCE_ADMIT'] < newdf['AKI1_SINCE_ADMIT']-1), (newdf['SINCE_ADMIT'] == newdf['AKI1_SINCE_ADMIT']-1), (newdf['SINCE_ADMIT'] > newdf['AKI1_SINCE_ADMIT']-1)]\n",
    "    #values = [0, 1, -1]\n",
    "    #newdf['FLAG'] = np.select(conditions, values)\n",
    "    #newdf = newdf.drop(newdf[newdf['FLAG']==-1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d68caf-9efd-40d0-9067-ed946e087762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #onset\n",
    "    sites = ['IUR', 'MCRI', 'MCW', 'UIOWA', 'UMHC', 'UNMC', 'UofU', 'UPITT', 'UTHSCSA', 'UTSW']\n",
    "    #site = ['UTSW', 'MCW', 'UofU']\n",
    "\n",
    "    for site in sites:\n",
    "        onset(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7c058-9313-46e4-96a8-9757ed1d8dbd",
   "metadata": {
    "tags": [
     "virat"
    ]
   },
   "outputs": [],
   "source": [
    "# vital\n",
    "def vital(site, year):\n",
    "    print('Running vital on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    # load tables\n",
    "#    vital = pd.read_csv(datafolder+site+'/raw/'+'AKI_VITAL'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "#    vital = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/p0_vital_'+site+'.pkl')\n",
    "    vital = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/p0_vital_'+site+'.pkl')\n",
    "    \n",
    "    #use only relevent data\n",
    "#    newdfX = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    newdfX = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "#    newdfX = newdfX >> select('PATID', 'ENCOUNTERID') >> mutate(dummy = True) >> distinct()\n",
    "#    vital = (pd.merge(vital, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy)).reset_index(drop=True)\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID', 'SINCE_ADMIT') >> mutate(dummy = True)\n",
    "    vital = (pd.merge(vital, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy) >> mutate(FUTURE=X.SINCE_ADMIT-X.DAYS_SINCE_ADMIT)).reset_index(drop=True)\n",
    "\n",
    "    #0 hours prediction\n",
    "#    vital = vital[vital['FUTURE']>=0].drop(['SINCE_ADMIT','FUTURE'],axis=1)    \n",
    "    \n",
    "    #24 hours prediction\n",
    "    vital = vital[vital['FUTURE']>0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "        \n",
    "    #Rethink if I need it TODO (no need since fill last data)\n",
    "    #vital.loc[vital.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1\n",
    "\n",
    "    # drop unused column\n",
    "#    vital = vital.drop(['Row #','MEASURE_DATE_TIME','SMOKING', 'TOBACCO', 'TOBACCO_TYPE', 'WT'],axis=1, errors='ignore')\n",
    "\n",
    "    #Calculate daily average\n",
    "    vital_mean = vital.groupby(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).mean().reset_index()\n",
    "\n",
    "    #Transform vital Table (Row over the previous value if unknown) (Continuous)\n",
    "    vital_list = []\n",
    "    #depreciated vital table only collect data until onset\n",
    "    #vital_p = (vital_mean >> mask(X.DAYS_SINCE_ADMIT < 0) >> drop('DAYS_SINCE_ADMIT') >> distinct())\n",
    "    vital_sys = vital_mean >> select('PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT', 'SYSTOLIC')\n",
    "    vital_dia = vital_mean >> select('PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT', 'DIASTOLIC')\n",
    "    vital_bmi = vital_mean >> select('PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT', 'ORIGINAL_BMI')\n",
    "    vital_wt = vital_mean >> select('PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT', 'WT')\n",
    "\n",
    "    # deprecitaed only collect at onset\n",
    "#    for i in range(pred_end+1):\n",
    "#        vital_sys_p = vital_sys.loc[((vital_sys >> mask(X.DAYS_SINCE_ADMIT <= i)).dropna()).groupby(['PATID', 'ENCOUNTERID']).DAYS_SINCE_ADMIT.idxmax()] >> select(~X.DAYS_SINCE_ADMIT)\n",
    "#        vital_dia_p = vital_dia.loc[((vital_dia >> mask(X.DAYS_SINCE_ADMIT <= i)).dropna()).groupby(['PATID', 'ENCOUNTERID']).DAYS_SINCE_ADMIT.idxmax()] >> select(~X.DAYS_SINCE_ADMIT)\n",
    "#        vital_bmi_p = vital_bmi.loc[((vital_bmi >> mask(X.DAYS_SINCE_ADMIT <= i)).dropna()).groupby(['PATID', 'ENCOUNTERID']).DAYS_SINCE_ADMIT.idxmax()] >> select(~X.DAYS_SINCE_ADMIT)\n",
    "#        vital_wt_p = vital_wt.loc[((vital_wt >> mask(X.DAYS_SINCE_ADMIT <= i)).dropna()).groupby(['PATID', 'ENCOUNTERID']).DAYS_SINCE_ADMIT.idxmax()] >> select(~X.DAYS_SINCE_ADMIT)        \n",
    "        # fill in previous day value (row over if unknown)\n",
    "#        vital_p = pd.merge(vital_sys_p, vital_dia_p, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "#        vital_p = pd.merge(vital_p, vital_bmi_p, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "#        vital_p = pd.merge(vital_p, vital_wt_p, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='outer')        \n",
    "#        vital_p = vital_p >> mutate(SINCE_ADMIT = i)\n",
    "#        vital_list.append(vital_p.copy())\n",
    "#    vital_t = pd.concat(vital_list, axis=0, ignore_index=True) \n",
    "    #Save table\n",
    "#    vital_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/vital_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    # idxmax is slow, use sort and last\n",
    "    # vital_sys_p = vital_sys.loc[vital_sys.dropna().groupby(['PATID', 'ENCOUNTERID']).DAYS_SINCE_ADMIT.idxmax()] >> select(~X.DAYS_SINCE_ADMIT)\n",
    "    # vital_dia_p = vital_dia.loc[vital_dia.dropna().groupby(['PATID', 'ENCOUNTERID']).DAYS_SINCE_ADMIT.idxmax()] >> select(~X.DAYS_SINCE_ADMIT)\n",
    "    # vital_bmi_p = vital_bmi.loc[vital_bmi.dropna().groupby(['PATID', 'ENCOUNTERID']).DAYS_SINCE_ADMIT.idxmax()] >> select(~X.DAYS_SINCE_ADMIT)\n",
    "    # vital_wt_p  =  vital_wt.loc[ vital_wt.dropna().groupby(['PATID', 'ENCOUNTERID']).DAYS_SINCE_ADMIT.idxmax()] >> select(~X.DAYS_SINCE_ADMIT)        \n",
    "\n",
    "    vital_sys_p = vital_sys.dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID']).agg({'SYSTOLIC':'last'}).reset_index()    \n",
    "    vital_dia_p = vital_dia.dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID']).agg({'DIASTOLIC':'last'}).reset_index()    \n",
    "    vital_bmi_p = vital_bmi.dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID']).agg({'ORIGINAL_BMI':'last'}).reset_index()    \n",
    "    vital_wt_p  =  vital_wt.dropna().sort_values(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID']).agg({'WT':'last'}).reset_index()    \n",
    "\n",
    "    \n",
    "# fill in previous day value (row over if unknown)\n",
    "    vital_t = pd.merge(vital_sys_p, vital_dia_p, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    vital_t = pd.merge(vital_t, vital_bmi_p, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    vital_t = pd.merge(vital_t, vital_wt_p, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='outer')        \n",
    "\n",
    "    #Save table\n",
    "#    vital_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/vital_'+site+'_'+str(year)+'.pkl')    \n",
    "    vital_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/vital_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    #consistency check\n",
    "    if vital_t.empty:\n",
    "        logging.basicConfig(filename='vital.log', filemode='a')    \n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('vital: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "\n",
    "    print('Finished vital on site '+site+\":\"+str(year), flush = True)\n",
    "#    return vital_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09fad5-20e4-4aca-a692-542e2b202a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def demo(site, year):\n",
    "    # demo\n",
    "    print('Running demo on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    # load tables\n",
    "#    demo = pd.read_csv(datafolder+site+'/raw/'+'AKI_DEMO'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "    demo = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/p0_demo_'+site+'.pkl')\n",
    "    \n",
    "    #use only relevent data\n",
    "    newdfX = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID') >> mutate(dummy = True) >> distinct()\n",
    "    demo = (pd.merge(demo, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy)).reset_index(drop=True)\n",
    "\n",
    "    # drop unused column\n",
    "#    demo = demo.drop(['Row #','DEATH_DATE','BIRTH_DATE','DDAYS_SINCE_ENC','DEATH_DATE_IMPUTE','DEATH_SOURCE'],axis=1, errors='ignore')\n",
    "\n",
    "    #onehot transform demo \n",
    "    var = ['SEX', 'RACE', 'HISPANIC']\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(demo[var])\n",
    "    demo_onehot_cat = pd.DataFrame(enc.transform(demo[var]).toarray(), columns=enc.get_feature_names(var)).astype('bool')\n",
    "    demo_one = pd.concat([demo[['PATID', 'ENCOUNTERID', 'AGE']].reset_index(), demo_onehot_cat], axis=1).drop('index',axis=1)    \n",
    "    \n",
    "    #Save table\n",
    "    demo_one.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/demo_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    #consistency check\n",
    "    if demo_one.empty:\n",
    "        logging.basicConfig(filename='demo.log', filemode='a')    \n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('demo: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "\n",
    "    print('Finished demo on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8695a77-1e05-4a88-81e6-92e0e195b766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dx(site, year):\n",
    "    home_directory = \"/home/hoyinchan/code/AKI_CDM_PY/\"\n",
    "    \n",
    "    # dx\n",
    "    print('Running dx on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    # load table\n",
    "#    dx = pd.read_csv(datafolder+site+'/raw/'+'AKI_DX'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object', 'DX_TYPE': 'object', 'DX': 'object'}))\n",
    "    dx = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/p0_dx_'+site+'.pkl')\n",
    "    \n",
    "    #use only relevent data\n",
    "    newdfX = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    #newdfX = newdfX >> select('PATID', 'ENCOUNTERID') >> mutate(dummy = True) >> distinct()\n",
    "    #dx = (pd.merge(dx, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy)).reset_index(drop=True)\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID', 'SINCE_ADMIT') >> mutate(dummy = True)\n",
    "    dx = (pd.merge(dx, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy) >> mutate(FUTURE=X.SINCE_ADMIT-X.DAYS_SINCE_ADMIT)).reset_index(drop=True)\n",
    "\n",
    "    #0 hours prediction\n",
    "#    dx = dx[dx['FUTURE']>=0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "\n",
    "    #24 hours prediction\n",
    "    dx = dx[dx['FUTURE']>0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "    \n",
    "    \n",
    "    #Some site use 9 some site use 09\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].where(dx['DX_TYPE'] != '9', '09')\n",
    "    \n",
    "    # ICD10 -> ICD09\n",
    "    icd10toicd09 = pd.read_csv(home_directory+'2018_I10gem.csv',sep=',')\n",
    "    \n",
    "    icd10toicd09.columns = ['DX', 'DX09']\n",
    "    #add fillna for unmatch ICD10 (DONE)\n",
    "    #dx3 = dx >> mask(X.DX_TYPE == '10') >> left_join(icd10toicd09, by='DX') >> mutate(DX_TYPE = '09', DX = X.DX09) >> select(~X.DX09)\n",
    "#    dx4 = dx >> mask(X.DX_TYPE == '10') >> left_join(icd10toicd09, by='DX')\n",
    "    dx4 = dx >> mask(X.DX_TYPE == '10')\n",
    "    dx4['DX'] = dx4['DX'].map(lambda x: x.replace('.',''))\n",
    "    dx4 = dx4 >> left_join(icd10toicd09, by='DX')\n",
    "    #Keep icd10 if no match\n",
    "    dx4['DX_TYPE'] = dx4['DX_TYPE'].where(dx4['DX09'].isnull(), '09')\n",
    "    dx4['DX'] = dx4['DX'].where(dx4['DX09'].isnull(), dx4['DX09'])\n",
    "    dx4 = dx4.drop('DX09', axis=1)\n",
    "    dx = pd.concat([dx >> mask(X.DX_TYPE != '10'), dx4], axis=0)\n",
    "\n",
    "    # Roll icd 09 code up native\n",
    "    dx['DX'] = dx['DX'].where(dx['DX_TYPE'] != '09', dx['DX'].map(lambda x: x[0:3]))\n",
    "\n",
    "    # Roll icd 10 code up native\n",
    "    dx['DX'] = dx['DX'].where(dx['DX_TYPE'] != '10', dx['DX'].map(lambda x: x[0:3]))\n",
    "    \n",
    "    #Rethink if I need it TODO (no need since historic)\n",
    "    #dx.loc[dx.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1\n",
    "\n",
    "    # drop unused column\n",
    "#    dx = dx.drop(['DX_SOURCE','DX_ORIGIN','PDX','DX_DATE'],axis=1, errors='ignore')\n",
    "\n",
    "    #Transform dx table (Historical data: Yes if any diagnoasis show up) (Assuming all DAYS_SINCE_ADMIT < 0) (Boolean)\n",
    "    \n",
    "    dx['sixmonth'] = '<6'\n",
    "    dx['sixmonth'] = dx['sixmonth'].where(dx['DAYS_SINCE_ADMIT']<-365/2, '>6') # becareful negative number\n",
    "    dx_t = dx >> mutate(DX='DX:'+X.DX_TYPE+\":\"+X.DX+X.sixmonth) >> drop('DX_TYPE') >> drop('sixmonth')\n",
    "    dx_t = (dx_t >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='DX', values='dummy').fillna(False).reset_index()\n",
    "\n",
    "    #Save table\n",
    "    dx_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/dx_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    #consistency check\n",
    "    if dx_t.empty:\n",
    "        logging.basicConfig(filename='dx.log', filemode='a')\n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('dx: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "    \n",
    "    #consistency check2\n",
    "        \n",
    "    print('Finished dx on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad611cd5-bd0b-4212-bad9-48479593f82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def px(site, year):\n",
    "    # px\n",
    "    print('Running px on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    #load table\n",
    "#    px = pd.read_csv(datafolder+site+'/raw/'+'AKI_PX'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "    px = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/p0_px_'+site+'.pkl')\n",
    "    \n",
    "    #use only relevent data\n",
    "    newdfX = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    #newdfX = newdfX >> select('PATID', 'ENCOUNTERID') >> mutate(dummy = True) >> distinct()\n",
    "    #px = (pd.merge(px, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy)).reset_index(drop=True)\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID', 'SINCE_ADMIT') >> mutate(dummy = True)\n",
    "    px = (pd.merge(px, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy) >> mutate(FUTURE=X.SINCE_ADMIT-X.DAYS_SINCE_ADMIT)).reset_index(drop=True)\n",
    "\n",
    "    #0 hours prediction\n",
    "#    px = px[px['FUTURE']>=0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "\n",
    "    #24 hours prediction\n",
    "    px = px[px['FUTURE']>0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "    \n",
    "    \n",
    "    #Some site use 9 some site use 09\n",
    "    px['PX_TYPE'] = px['PX_TYPE'].where(px['PX_TYPE'] != '9', '09')\n",
    "    \n",
    "    #Rethink if I need it TODO (exact date, so follow onset) (actually no, because day 0 is still day0, onset just shift theoneset to next day)\n",
    "#    px.loc[px.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1\n",
    "\n",
    "    # drop unused column\n",
    "    px = px >> mutate(PX='PX:'+X.PX_TYPE+\":\"+X.PX) >> drop('PX_TYPE')\n",
    "#    px = px.drop(['Row #', 'PX_SOURCE','DX_ORIGIN','PDX','PX_DATE', 'PX_TYPE'],axis=1, errors='ignore')\n",
    "\n",
    "    #Depreciate -> Chnage back to historic\n",
    "    #Transform PX Table (Exact Date) (Boolean)\n",
    "#    px_list = []\n",
    "#    for i in range(pred_end+1):\n",
    "#        px_p = (px >> mask(X.DAYS_SINCE_ADMIT == i) >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='PX', values='dummy').fillna(False).reset_index()\n",
    "#        px_p = px_p >> mutate(SINCE_ADMIT = i)\n",
    "#        px_list.append(px_p.copy())\n",
    "#    px_t = pd.concat(px_list, axis=0, ignore_index=True).fillna(False) \n",
    "\n",
    "    px_t = (px >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='PX', values='dummy').fillna(False).reset_index()\n",
    "\n",
    "    #Save table\n",
    "    px_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/px_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    if px_t.empty:\n",
    "        logging.basicConfig(filename='px.log', filemode='a')\n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('px: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()\n",
    "\n",
    "\n",
    "    print('Finished px on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd021f9-ff5b-4847-9284-24bef8ce2dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lab(site, year):\n",
    "#site = 'UPITT'\n",
    "#year=2018\n",
    "#if True:\n",
    "    # lab\n",
    "    print('Running lab on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    #load table\n",
    "#    lab = pd.read_csv(datafolder+site+'/raw/'+'AKI_LAB'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "#    lab = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/p0_lab_'+site+'.pkl')\n",
    "    lab = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/p0_lab_'+site+'.pkl')\n",
    "    #Rethink if I need it TODO ( no need since row over)\n",
    "    #lab.loc[lab.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1\n",
    "   \n",
    "    #use only relevent data\n",
    "    newdfX = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    #newdfX = newdfX >> select('PATID', 'ENCOUNTERID') >> mutate(dummy = True) >> distinct()\n",
    "    #lab = (pd.merge(lab, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy)).reset_index(drop=True)\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID', 'SINCE_ADMIT') >> mutate(dummy = True)\n",
    "    lab = (pd.merge(lab, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy) >> mutate(FUTURE=X.SINCE_ADMIT-X.DAYS_SINCE_ADMIT)).reset_index(drop=True)\n",
    "\n",
    "    #0 hours prediciton\n",
    "#    lab = lab[lab['FUTURE']>=0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "\n",
    "    #24 hours prediciton\n",
    "    lab = lab[lab['FUTURE']>0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "\n",
    "\n",
    "    #seperate into numberic lab and pos/neg lab\n",
    "    #calculate categorical first\n",
    "    lab_cat = lab.loc[lab['RESULT_NUM'].isnull()]\n",
    "    if lab_cat.empty:\n",
    "        labcat_t = lab >> select('PATID','ENCOUNTERID')\n",
    "        labcat_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/labcat_'+site+'_'+str(year)+'.pkl')\n",
    "    else:\n",
    "        #Calculate daily average    \n",
    "#        lab_mode = lab_cat.loc[:, ['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT', 'RESULT_QUAL']].groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT']).agg(mymode).reset_index()\n",
    "        \n",
    "        lab_mode = lab_cat.loc[:, ['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT', 'RESULT_QUAL']].groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT']).agg(pd.Series.mode).reset_index()\n",
    "        lab_mode_nnd = lab_mode.loc[lab_mode['RESULT_QUAL'].apply(type) == str].copy()\n",
    "        lab_mode_nd = lab_mode.loc[lab_mode['RESULT_QUAL'].apply(type) != str].copy()\n",
    "        pattern = '[\\[\\]\\']'\n",
    "        lab_mode_nd.loc[:,'RESULT_QUAL'] = lab_mode_nd['RESULT_QUAL'].apply(lambda x: re.sub(pattern, \"\", np.array2string(x,separator='-')))\n",
    "        lab_mode = pd.concat([lab_mode_nd, lab_mode_nnd], ignore_index=True)\n",
    "\n",
    "        labcat_t = lab_mode.sort_values(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC']).agg({'RESULT_QUAL':'last'}).reset_index()\n",
    "        labcat_t = labcat_t >> mutate(LAB_LOINC='LAB:'+\":\"+X.LAB_LOINC+\"(\"+X.RESULT_QUAL+\")\") >> mutate(dummy = True) >> select('PATID', 'ENCOUNTERID', 'LAB_LOINC', 'dummy')\n",
    "        labcat_t = labcat_t.pivot(index=['PATID', 'ENCOUNTERID'], columns='LAB_LOINC', values='dummy').fillna(False).reset_index()        \n",
    "        #Save table    \n",
    "        labcat_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/labcat_'+site+'_'+str(year)+'.pkl') \n",
    "    \n",
    "    #calculate numerica    \n",
    "    lab_num = lab.loc[lab['RESULT_NUM'].notnull()]  \n",
    "    if lab_num.empty:\n",
    "        labnum_t = lab >> select('PATID','ENCOUNTERID')\n",
    "#        labnum_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/labnum_'+site+'_'+str(year)+'.pkl')\n",
    "        labnum_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/labnum_'+site+'_'+str(year)+'.pkl')        \n",
    "    else:    \n",
    "        #Calculate daily average\n",
    "        lab_mean = lab_num.groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'RESULT_UNIT', 'DAYS_SINCE_ADMIT']).agg({'RESULT_NUM':'mean'}).reset_index()\n",
    "        lab_mean = lab_mean >> mutate(LAB_LOINC='LAB:'+\":\"+X.LAB_LOINC+\"(\"+X.RESULT_UNIT+\")\")\n",
    "        labnum_t = lab_mean.sort_values(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT']).groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC']).agg({'RESULT_NUM':'last'}).reset_index()\n",
    "        labnum_t = labnum_t.pivot(index=['PATID', 'ENCOUNTERID'], columns='LAB_LOINC', values='RESULT_NUM').reset_index()\n",
    "        #Save table\n",
    "#        labnum_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/labnum_'+site+'_'+str(year)+'.pkl')        \n",
    "        labnum_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/labnum_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    #Depreciate -> Chnage back to historic\n",
    "    #Transform LAB Table (Row over the previous value if unknown) (Continuous)\n",
    "#    lab_list = []\n",
    "    # use 0 as beginning #lab.loc[lab.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1 disabled\n",
    "#    lab_p = (lab_mean >> mask(X.DAYS_SINCE_ADMIT == 0) >> drop('DAYS_SINCE_ADMIT') >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='LAB_LOINC', values='RESULT_NUM').reset_index()\n",
    "#    lab_p = lab_p >> mutate(SINCE_ADMIT = 1)   \n",
    "#    lab_list.append(lab_p.copy())\n",
    "    # use 1 as beginning #lab.loc[lab.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1 disabled\n",
    "#    for i in range(1,pred_end+1):\n",
    "#        lab_p = (lab_mean >> mask(X.DAYS_SINCE_ADMIT == i) >> drop('DAYS_SINCE_ADMIT') >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='LAB_LOINC', values='RESULT_NUM').reset_index()\n",
    "        # fill in previous day value (row over if unknown)\n",
    "#        lab_p = lab_p.join(lab_list[-1], lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "#        lab_p = lab_p.combine_first(lab_list[-1])\n",
    "#        lab_p = lab_p >> mutate(SINCE_ADMIT = i)    \n",
    "#        lab_list.append(lab_p.copy())\n",
    "#    lab_t = pd.concat(lab_list, axis=0, ignore_index=True) \n",
    "\n",
    "    if labnum_t.empty or labcat_t.empty:\n",
    "        logging.basicConfig(filename='lab.log', filemode='a')\n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('lab: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()   \n",
    "\n",
    "    print('Finished lab on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719d7ce-469b-4c46-be2f-24050dc3f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amed(site, year):\n",
    "    # amed\n",
    "    home_directory = \"/home/hoyinchan/code/AKI_CDM_PY/\"\n",
    "    print('Running amed on site '+site+\":\"+str(year), flush = True)\n",
    "\n",
    "    #load table\n",
    "#    amed = pd.read_csv(datafolder+site+'/raw/'+'AKI_AMED'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "    amed = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/p0_amed_'+site+'.pkl')\n",
    "    \n",
    "    #use only relevent data\n",
    "    newdfX = pd.read_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/onset_'+site+'_'+str(year)+'.pkl')\n",
    "    #newdfX = newdfX >> select('PATID', 'ENCOUNTERID') >> mutate(dummy = True) >> distinct()\n",
    "    #amed = (pd.merge(amed, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy)).reset_index(drop=True)\n",
    "    newdfX = newdfX >> select('PATID', 'ENCOUNTERID', 'SINCE_ADMIT') >> mutate(dummy = True)\n",
    "    amed = (pd.merge(amed, newdfX, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left').fillna({'dummy': False}) >> mask(X.dummy) >> select(~X.dummy) >> mutate(FUTURE=X.SINCE_ADMIT-X.DAYS_SINCE_ADMIT)).reset_index(drop=True)\n",
    "\n",
    "    #0 hours prediction\n",
    "#    amed = amed[amed['FUTURE']>=0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "\n",
    "    #24 hours prediction\n",
    "    amed = amed[amed['FUTURE']>0].drop(['SINCE_ADMIT','FUTURE'],axis=1)\n",
    "\n",
    "    # rxnorm -> atc\n",
    "    amed_rx = amed.loc[amed['MEDADMIN_TYPE'] == \"RX\"]\n",
    "    if not amed_rx.empty:\n",
    "        # pd.DataFrame(amed['MEDADMIN_CODE'].unique()).to_csv('/home/hchan2/AKI/AKI_Python/data/'+site+'/rxnormtmp.csv', sep=',', index=False, header = False)\n",
    "        # Go to run rxnorm2atcR.ipynb NOW\n",
    "        rxcui2atc_dtypes =  {\"Rxcui\": 'object', \"ATC4th\": 'object'}    \n",
    "        rxcui2atc = pd.read_csv('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/rxnorm_out_'+site+'.csv',sep=',', dtype=(rxcui2atc_dtypes)) >> rename(MEDADMIN_CODE=X.Rxcui)\n",
    "        amed_rx = amed_rx >> left_join(rxcui2atc, by='MEDADMIN_CODE')\n",
    "        amed_rx['MEDADMIN_TYPE'] = amed_rx['MEDADMIN_TYPE'].where(amed_rx['ATC4th'].isnull(), 'ATC')\n",
    "        amed_rx['MEDADMIN_CODE'] = amed_rx['MEDADMIN_CODE'].where(amed_rx['ATC4th'].isnull(), amed_rx['ATC4th'])\n",
    "        amed_rx = amed_rx >> mutate(MEDADMIN_CODE='MED:'+X.MEDADMIN_TYPE+':'+X.MEDADMIN_CODE)\n",
    "        amed_rx = amed_rx >> select('PATID', 'ENCOUNTERID', 'MEDADMIN_CODE', 'DAYS_SINCE_ADMIT')\n",
    "    else:\n",
    "        amed_rx = amed_rx >> select('PATID', 'ENCOUNTERID', 'MEDADMIN_CODE', 'DAYS_SINCE_ADMIT')\n",
    "    \n",
    "    # ndc -> atc\n",
    "    amed_ndc = amed.loc[amed['MEDADMIN_TYPE'] == \"ND\"]    \n",
    "    if not amed_ndc.empty:    \n",
    "        # pd.DataFrame(amed['MEDADMIN_CODE'].unique()).to_csv('/home/hchan2/AKI/AKI_Python/data/'+site+'/rxnormtmp.csv', sep=',', index=False, header = False)\n",
    "        # Go to run rxnorm2atcR.ipynb NOW\n",
    "        ndc2atc_dtypes =  {\"ndc\": 'object', \"ATC4th\": 'object'}    \n",
    "        ndc2atc = pd.read_csv('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/ndc_out_'+site+'.csv',sep=',', dtype=(ndc2atc_dtypes)) >> rename(MEDADMIN_CODE=X.ndc)\n",
    "        amed_ndc = amed_ndc >> left_join(ndc2atc, by='MEDADMIN_CODE')\n",
    "        amed_ndc['MEDADMIN_TYPE'] = amed_ndc['MEDADMIN_TYPE'].where(amed_ndc['ATC4th'].isnull(), 'ATC')\n",
    "        amed_ndc['MEDADMIN_CODE'] = amed_ndc['MEDADMIN_CODE'].where(amed_ndc['ATC4th'].isnull(), amed_ndc['ATC4th'])\n",
    "        amed_ndc = amed_ndc >> mutate(MEDADMIN_CODE='MED:'+X.MEDADMIN_TYPE+':'+X.MEDADMIN_CODE)\n",
    "        amed_ndc = amed_ndc >> select('PATID', 'ENCOUNTERID', 'MEDADMIN_CODE', 'DAYS_SINCE_ADMIT')\n",
    "    else:\n",
    "        amed_ndc = amed_ndc >> select('PATID', 'ENCOUNTERID', 'MEDADMIN_CODE', 'DAYS_SINCE_ADMIT')\n",
    "       \n",
    "    amed = pd.concat([amed_rx, amed_ndc], axis=0, ignore_index=True)   \n",
    "\n",
    "    #Rethink if I need it TODO (yes since exact date) (actually no, because day 0 is still day0, onset just shift theoneset to next day)\n",
    "#    amed.loc[amed.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1\n",
    "\n",
    "    #Depreciate -> Chnage back to historic    \n",
    "    #Transform AMED Table (Exact date) (Boolean)\n",
    "#    amed_list = []\n",
    "#    for i in range(1,pred_end+1):\n",
    "#        amed_p = (amed >> mask(X.DAYS_SINCE_ADMIT == i) >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).fillna('NI').pivot(index=['PATID', 'ENCOUNTERID'], columns='ATC4th', values='dummy').fillna(False).reset_index()\n",
    "#        amed_p = amed_p >> mutate(SINCE_ADMIT = i)\n",
    "#        amed_list.append(amed_p.copy())\n",
    "#    amed_t = pd.concat(amed_list, axis=0, ignore_index=True).fillna(False) \n",
    "    #TODO test if not amed data in onset paitient\n",
    "    # [x for x in newdf['PATID'] if x in amed_t['PATID']]  \n",
    "\n",
    "    amed_t = (amed >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='MEDADMIN_CODE', values='dummy').fillna(False).reset_index()\n",
    "        \n",
    "    #Save table\n",
    "    amed_t.to_pickle('/home/hoyinchan/blue/Data/data2021/data2021/'+site+'/amed_'+site+'_'+str(year)+'.pkl')\n",
    "\n",
    "    if amed_t.empty:\n",
    "        logging.basicConfig(filename='amed.log', filemode='a')\n",
    "        print('DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year), flush = True)\n",
    "        logging.error('amed: DATAFRAME EMPTY!!!!!! '+site+\":\"+str(year))\n",
    "        logging.shutdown()   \n",
    "\n",
    "    print('Finished amed on site '+site+\":\"+str(year), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476bcea6-e5b7-4f3b-83a1-ad6c41b680f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
