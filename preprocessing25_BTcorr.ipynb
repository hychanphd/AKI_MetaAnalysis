{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f485a-31dd-482c-b8ea-e3ad018edc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "#import rpy2.robjects as robjects\n",
    "#from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "import logging\n",
    "from glob import glob\n",
    "import pickle\n",
    "import utils_function\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f4fe8-de9b-440b-9074-5ec478bb8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corr(configs_variables):\n",
    "    \n",
    "    site, datafolder, home_directory = utils_function.get_commons(configs_variables)\n",
    "    stg = configs_variables['stg']\n",
    "    print('Running bt3corr on site '+site, flush = True)\n",
    "\n",
    "    if not configs_variables['rerun_flag'] and os.path.exists(datafolder+site+'/bt3corr_'+site+'_'+stg+'_3000.pkl'):\n",
    "        print('Existed: bt3corr_'+site+'_'+stg+'_3000.pkl')\n",
    "        return         \n",
    "    \n",
    "    bt = pd.read_pickle(datafolder+site+'/bt3pos_'+site+'_'+stg+'_3000.pkl')\n",
    "    bt = bt.drop(['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'],axis=1)\n",
    "    corr = bt.corr()\n",
    "    corr.to_pickle(datafolder+site+'/bt3corr_'+site+'_'+stg+'_3000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6645121-704b-43e8-b4fa-9cb7fef24754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corr_occurence2():\n",
    "    files = []\n",
    "    start_dir = os.getcwd()\n",
    "    pattern   = \"btcorr_*\"\n",
    "    for dir,_,_ in os.walk(start_dir):\n",
    "        files.extend(glob(os.path.join(dir,pattern))) \n",
    "\n",
    "    meltcorrlist = list() \n",
    "\n",
    "    for file in files:\n",
    "        site = file.split('/')[-1].split('.')[0].split('_')[1]\n",
    "        year = file.split('/')[-1].split('.')[0].split('_')[2]\n",
    "        corr = pd.read_pickle(file)\n",
    "        corr = abs(corr)\n",
    "        meltcorr = pd.melt(corr.reset_index(), id_vars=['index'])\n",
    "        meltcorr = meltcorr[meltcorr['index'] != meltcorr['variable']]\n",
    "        meltcorr['site'] = site\n",
    "        meltcorr['year'] = year    \n",
    "        meltcorrlist.append(meltcorr)\n",
    "\n",
    "        \n",
    "    meltcorrall = pd.concat(meltcorrlist)\n",
    "    meltcorrall.columns = ['v1', 'v2', 'corr', 'site', 'year']\n",
    "    meltcorrall.to_pickle('data/meltcorrall.pkl')\n",
    "                              \n",
    "#if __name__ == \"__main__\":\n",
    "#    calculate_corr_occurence2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca4933-7018-4744-b3df-3b661d3f1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_plot():\n",
    "    files = []\n",
    "    start_dir = os.getcwd()\n",
    "    pattern   = \"btcorr_*\"\n",
    "    for dir,_,_ in os.walk(start_dir):\n",
    "        files.extend(glob(os.path.join(dir,pattern))) \n",
    "\n",
    "    meltcorrlist = list() \n",
    "\n",
    "    for file in files:\n",
    "        site = file.split('/')[-1].split('.')[0].split('_')[1]\n",
    "        year = file.split('/')[-1].split('.')[0].split('_')[2]\n",
    "        corr = pd.read_pickle(file)\n",
    "        meltcorr = pd.melt(corr.reset_index(), id_vars=['index'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e8284-843d-4011-94e5-25787aa7af78",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_corr_occurence(threshold = 0.5):\n",
    "    files = []\n",
    "    start_dir = os.getcwd()\n",
    "    pattern   = \"btcorr_*\"\n",
    "    for dir,_,_ in os.walk(start_dir):\n",
    "        files.extend(glob(os.path.join(dir,pattern))) \n",
    "\n",
    "    meltcorrlist = list() \n",
    "\n",
    "    for file in files:\n",
    "        site = file.split('/')[-1].split('.')[0].split('_')[1]\n",
    "        year = file.split('/')[-1].split('.')[0].split('_')[2]\n",
    "        corr = pd.read_pickle(file)\n",
    "        meltcorr = pd.melt(corr.reset_index(), id_vars=['index'])\n",
    "        meltcorr = meltcorr[meltcorr['value'] >= threshold]\n",
    "        meltcorr = meltcorr[meltcorr['index'] != meltcorr['variable']]\n",
    "        meltcorr['site'] = site\n",
    "        meltcorr['year'] = year    \n",
    "        meltcorrlist.append(meltcorr)\n",
    "\n",
    "    meltcorrall = pd.concat(meltcorrlist)\n",
    "    meltcorrall.columns = ['v1', 'v2', 'corr', 'site', 'year']\n",
    "    meltcorrallcount = pd.melt(meltcorrall.drop('corr', axis=1), id_vars=['site', 'year']).drop('variable', axis=1).drop_duplicates().groupby('value').count().drop('site',axis=1).reset_index()\n",
    "    meltcorrallcount.columns = ['value', 'count']\n",
    "    meltcorrallcount.to_pickle('data/meltcorrallcount_'+str(threshold)+'.pkl')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    calculate_corr_occurence(threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ad30c-f23b-4a61-864c-4d842a3124fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_corr_bt(site, year, threshold=0.5):   \n",
    "\n",
    "    meltcorrallcount = pd.read_pickle('data/meltcorrallcount_'+str(threshold)+'.pkl')\n",
    "    bt = pd.read_pickle('data/'+site+'/bt_'+site+'_'+str(year)+'.pkl')\n",
    "    corr = bt.corr()\n",
    "    corr.to_pickle('data/'+site+'/btcorr_'+site+'_'+str(year)+'.pkl')\n",
    "    meltcorr = pd.melt(corr.reset_index(), id_vars=['index'])\n",
    "    meltcorr = meltcorr[meltcorr['value'] >= threshold]\n",
    "    meltcorr = meltcorr[meltcorr['index'] != meltcorr['variable']]\n",
    "       \n",
    "    correlated_group = list()\n",
    "    correlated_group.append([meltcorr.iloc[0,0], meltcorr.iloc[0,1]])\n",
    "    for i in range(meltcorr.shape[0]):\n",
    "        newgroup = True\n",
    "        for j in range(len(correlated_group)):\n",
    "            if (meltcorr.iloc[i,0] in correlated_group[j] or meltcorr.iloc[i,1] in correlated_group[j]):\n",
    "                correlated_group[j].append(meltcorr.iloc[i,0])\n",
    "                correlated_group[j].append(meltcorr.iloc[i,1])\n",
    "                newgroup = False\n",
    "                break\n",
    "        if newgroup:\n",
    "            correlated_group.append([meltcorr.iloc[i,0], meltcorr.iloc[i,1]])\n",
    "\n",
    "    keep_list = list()\n",
    "    drop_list = list()\n",
    "\n",
    "    for i in range(len(correlated_group)):\n",
    "        correlated_groupX = list(set(correlated_group[i]))\n",
    "        correlated_groupX.sort(reverse=True)\n",
    "        maxcount = -1\n",
    "        for feature in correlated_groupX:\n",
    "            count = meltcorrallcount[meltcorrallcount['value'] == feature].iloc[0,1]\n",
    "            if maxcount < count:\n",
    "                maxfeature = feature\n",
    "                maxcount = count\n",
    "        keep_list.append(maxfeature)\n",
    "        correlated_groupX.remove(maxfeature)\n",
    "        drop_list.extend(correlated_groupX)\n",
    "        \n",
    "    with open('data/'+site+'/corrdropadd_'+site+'_'+str(year)+'.pkl', 'wb') as f:\n",
    "        pickle.dump(drop_list, f)            \n",
    "        pickle.dump(keep_list, f)            \n",
    "    bt = bt.drop(drop_list, axis=1)\n",
    "    bt.to_pickle('data/'+site+'/bt2_'+site+'_'+str(year)+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d28f57-d94d-4e3f-b090-035e02338bf6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_corr_occurence_new(configs_variables):\n",
    "#if True:    \n",
    "    site, datafolder, home_directory = utils_function.get_commons(configs_variables[0])\n",
    "    stg = configs_variables[0]['stg']\n",
    "    print('Running bt3corr on site '+site, flush = True)\n",
    "    threshold = float(configs_variables[0]['threshold_correlation'])\n",
    "    \n",
    "    files = []\n",
    "    for configs_variable in configs_variables:\n",
    "        site, datafolder, home_directory = utils_function.get_commons(configs_variable)\n",
    "        files.append(datafolder+site+'/bt3corr_'+site+'_'+stg+'_3000.pkl')\n",
    "    \n",
    "    meltcorrlist = list() \n",
    "    flag_corr = list()\n",
    "    \n",
    "    for file in files:\n",
    "        site = file.split('/')[-1].split('.')[0].split('_')[1]\n",
    "        corr = pd.read_pickle(file)\n",
    "        flag_corr_t = corr[['FLAG']]\n",
    "        flag_corr_t['site'] = site\n",
    "        flag_corr.append(flag_corr_t)\n",
    "        \n",
    "        corr = corr.drop('FLAG').drop('FLAG',axis=1)\n",
    "        meltcorr = pd.melt(corr.reset_index(), id_vars=['index'])\n",
    "        meltcorr = meltcorr[meltcorr['index'] != meltcorr['variable']]\n",
    "        meltcorr['site'] = site\n",
    "        meltcorrlist.append(meltcorr)\n",
    "\n",
    "    flag_corr_all = pd.concat(flag_corr)\n",
    "    flag_corr_median = flag_corr_all.groupby(flag_corr_all.index).median().sort_values('FLAG',ascending=False)\n",
    "    flag_corr_dict = flag_corr_median.to_dict()['FLAG']\n",
    "    meltcorrall = pd.concat(meltcorrlist)\n",
    "    meltcorrall.columns = ['v1', 'v2', 'corr', 'site']\n",
    "\n",
    "    feature_site_count = meltcorrall[['v1', 'site']].drop_duplicates().groupby('v1').count()\n",
    "    feature_site_dict = feature_site_count.to_dict()['site']    \n",
    "    \n",
    "    removal_order = meltcorrall[['v1','v2','corr']].groupby(['v1','v2']).median().reset_index().sort_values('corr',ascending=False).reset_index(drop=True).dropna()\n",
    "    removal_order['abs_corr'] = abs(removal_order['corr'] )\n",
    "    removal_order = removal_order.sort_values('abs_corr',ascending=False)\n",
    "    removal_order = removal_order[removal_order['abs_corr']>=threshold].reset_index(drop=True)\n",
    "\n",
    "    removal_list = []\n",
    "\n",
    "    for i in range(removal_order.shape[0]):\n",
    "        row = removal_order.iloc[i]\n",
    "        if row['v1'] in removal_list or row['v2'] in removal_list:\n",
    "            if row['v1'] in removal_list:\n",
    "                print(row['v1'])\n",
    "            else:\n",
    "                print(row['v2'])            \n",
    "            continue\n",
    "\n",
    "    #    if feature_site_dict[row['v1']] > feature_site_dict[row['v2']]: # preserve the most common variable\n",
    "    #        removal_list.append(row['v1'])\n",
    "    #    elif feature_site_dict[row['v1']] < feature_site_dict[row['v2']]:\n",
    "    #        removal_list.append(row['v2'])\n",
    "    #    else:\n",
    "\n",
    "        # Only remove if one of them is avaliable in all site        \n",
    "        if feature_site_dict[row['v1']] == len(configs_variables) and  feature_site_dict[row['v2']] == len(configs_variables):            \n",
    "#            print(row['v1'], feature_site_dict[row['v1']], row['v2'], feature_site_dict[row['v2']])\n",
    "#            print(row['v1'], flag_corr_dict[row['v1']], row['v2'], flag_corr_dict[row['v2']])\n",
    "            \n",
    "            if abs(flag_corr_dict[row['v1']]) > abs(flag_corr_dict[row['v2']]): # preserve the variable more correlated top FLAG\n",
    "#                print(row['v2'])\n",
    "                removal_list.append(row['v2'])\n",
    "                \n",
    "            else:\n",
    "#                print(row['v1'])\n",
    "                removal_list.append(row['v1']) \n",
    "        elif feature_site_dict[row['v1']] == len(configs_variables):\n",
    "#            print(row['v1'], feature_site_dict[row['v1']], row['v2'], feature_site_dict[row['v2']])\n",
    "            \n",
    "#            print(row['v2'])\n",
    "            removal_list.append(row['v2'])\n",
    "        elif feature_site_dict[row['v2']] == len(configs_variables):\n",
    "#            print(row['v1'], feature_site_dict[row['v1']], row['v2'], feature_site_dict[row['v2']])\n",
    "            \n",
    "#            print(row['v1'])\n",
    "            removal_list.append(row['v1'])        \n",
    "            \n",
    "    removal_list = ['SEX_M' if x == 'SEX_F' else x for x in removal_list]\n",
    "#    removal_list = ['HISPANIC_N' if x == 'HISPANIC_Y' else x for x in removal_list]\n",
    "            \n",
    "    removal_list = pd.DataFrame(removal_list, columns=['features'])        \n",
    "    removal_list.to_pickle(datafolder+'/'+'meltcorrallcount_'+str(threshold)+'.pkl')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    site_list = ['KUMC', 'UTSW', 'MCW', 'UofU', 'UIOWA', 'UMHC', 'UPITT', 'UTHSCSA', 'UNMC']    \n",
    "    configs_variables = [utils_function.read_config(site) for site in site_list]\n",
    "    calculate_corr_occurence_new(configs_variables)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a1702-6b44-4e7d-b408-863f78a19d12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_correlated_features(configs_variables):\n",
    "    site, datafolder, home_directory = utils_function.get_commons(configs_variables)    \n",
    "    threshold = float(configs_variables['threshold_correlation'])\n",
    "    stg = configs_variables['stg']\n",
    "    removal_list =pd.read_pickle(datafolder+'/'+'meltcorrallcount_'+str(threshold)+'.pkl')\n",
    "    removal_list = removal_list['features'].to_list()\n",
    "    \n",
    "    print(f\"Removing correlation {site}\")\n",
    "    \n",
    "    bt = pd.read_pickle(datafolder+site+'/bt3pos_'+site+'_'+stg+'_3000.pkl')\n",
    "    filtered_columns = [x for x in bt.columns if x not in removal_list]\n",
    "    bt = bt[filtered_columns]\n",
    "    \n",
    "    bt.to_pickle(datafolder+site+'/bt3posnc_'+site+'_'+stg+'_3000.pkl')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    site_list = ['KUMC', 'UTSW', 'MCW', 'UofU', 'UIOWA', 'UMHC', 'UPITT', 'UTHSCSA', 'UNMC']    \n",
    "    configs_variables = [utils_function.read_config(site) for site in site_list]\n",
    "    for configs_variable in configs_variables:\n",
    "        remove_correlated_features(configs_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b90a3-ed86-45a8-9d6c-532649d3387c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_CDM_PY",
   "language": "python",
   "name": "aki_cdm_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
