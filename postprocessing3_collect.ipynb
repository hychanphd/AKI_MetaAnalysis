{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618a70f-a558-41f6-a0fc-669b07eab6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "#import rpy2.robjects as robjects\n",
    "#from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import plot_utils\n",
    "import utils_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47225a1-5b90-4285-84f0-d10efc989caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect(stg='', model='',site = '', year='', oversample='', fs='', rmcol='005', return_result=False):\n",
    "    print('Running collect on all site', flush = True)\n",
    "    \n",
    "    #Get all shap result files\n",
    "    files = []\n",
    "    start_dir = \"/home/hoyinchan/blue/Data/data2021/data2021/\"\n",
    "    pattern   = \"shapdata_*\"\n",
    "    for dir,_,_ in os.walk(start_dir):\n",
    "        files.extend(glob(os.path.join(dir,pattern))) \n",
    "\n",
    "    files = [x for x in files if model in x and stg in x and site in x and str(year) in x and oversample in x and fs in x and rmcol in x and not ('drop' in x) and not ('BACKUP' in x)]\n",
    "    \n",
    "    #Read all into dataframe\n",
    "    shap_tables = []\n",
    "    for file in files:\n",
    "        print(file)        \n",
    "        shap_tables.append(pd.read_pickle(file))\n",
    "\n",
    "    #concatanate\n",
    "    result = pd.concat(shap_tables, ignore_index=True)\n",
    "\n",
    "    if return_result:\n",
    "        return result\n",
    "    \n",
    "    #save to file\n",
    "    result.to_pickle('result.pkl')\n",
    "    result.to_csv('result.csv', index=False)\n",
    "\n",
    "    print('Finished collect on all site', flush = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb7243f-793b-453a-87cc-409c421cee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_strp(filename, cat=True):\n",
    "    result = pd.read_pickle(filename)\n",
    "    if cat:\n",
    "        result = result.loc[:,['fval', 'mean_val', 'se_val', 'Feature', 'Importances', 'rank', 'site', 'year', 'stg', 'fs', 'oversample', 'model', 'rmcol', 'auc', 'isCategorical']]\n",
    "    else:\n",
    "#        result = result.loc[:,['fval', 'mean_val', 'se_val', 'Feature', 'Importances', 'rank', 'site', 'year', 'stg', 'fs', 'oversample', 'model', 'rmcol', 'auc', 'isCategorical']]        \n",
    "        pass\n",
    "    result.to_pickle('result_strp.pkl')\n",
    "    result = result.drop(['rank', 'stg', 'rmcol', 'model'],axis=1)\n",
    "    result.to_pickle('result_strp3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48c742-ecab-45a2-9c90-a266a0c0fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_split(model, stg='stg01', site = '', year='', oversample='raw', fs='rmscrbun', rmcol='005', return_result=False):\n",
    "    t = collect(stg=stg, model=model,site =site, year=year, oversample=oversample, fs=fs, rmcol='005', return_result=True)\n",
    "    if return_result:\n",
    "        return t\n",
    "    else:\n",
    "        filestring = ('resultsplit_'+model+\"_\"+stg+\"_\"+site+\"_\"+year+\"_\"+fs+\"_\"+oversample+\"_\"+rmcol+'.pkl').replace(\"_.pkl\", \".pkl\").replace(\"__\", \"_\").replace(\"__\", \"_\").replace(\"__\", \"_\").replace(\"__\", \"_\").replace(\"__\", \"_\").replace(\"__\", \"_\").replace(\"__\", \"_\")   \n",
    "        print(filestring)\n",
    "        t.to_pickle(\"/home/hoyinchan/blue/Data/data2021/data2021/\"+filestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c349ae-f289-4ec1-9e65-3921ff82e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEID(model, stg='stg01', site = '', year='', oversample='raw', fs='rmscrbun', rmcol='005', return_result=False):\n",
    "#    shap_data = pd.read_pickle('result.pkl')\n",
    "    result_split(model, stg, site, year, oversample, fs, rmcol, return_result=False)\n",
    "    filestring = ('resultsplit_'+model+\"_\"+stg+\"_\"+site+\"_\"+year+\"_\"+fs+\"_\"+oversample+\"_\"+rmcol+'.pkl').replace(\"_.pkl\", \".pkl\").replace(\"__\", \"_\").replace(\"__\", \"_\").replace(\"__\", \"_\").replace(\"__\", \"_\").replace(\"__\", \"_\").replace(\"__\", \"_\").replace(\"__\", \"_\")   \n",
    "    shap_data = pd.read_pickle(\"/home/hoyinchan/blue/Data/data2021/data2021/\"+filestring)\n",
    "    mask1 = shap_data['Feature'] != 'AGE'\n",
    "    mask2 = shap_data['fval'] < 90\n",
    "    shap_data = shap_data.loc[mask1 | mask2]\n",
    "    #save to file\n",
    "    shap_data.to_pickle(\"/home/hoyinchan/blue/Data/data2021/data2021/\"+'DEID_'+filestring)\n",
    "#    shap_data.to_pickle('DEID_result.pkl')\n",
    "#    shap_data.to_csv('DEID_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fdc5e-1615-4452-ad90-5ba2f3f1bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_bt(stg='', fs='', oversample='', model_type='', numberbt=10, suffix='', return_result=False, site = '', year = '', rmcol=''):\n",
    "\n",
    "    files = []\n",
    "    start_dir = os.getcwd()\n",
    "    pattern   = \"boosttrap_*\"\n",
    "    for dir,_,_ in os.walk(start_dir):\n",
    "        files.extend(glob(os.path.join(dir,pattern))) \n",
    "\n",
    "    files = [x for x in files if model_type in x and stg in x and site in x and str(year) in x and oversample in x and fs in x and rmcol in x and not ('drop' in x) and not ('BACKUP' in x)]\n",
    "    \n",
    "    shap_tables = []\n",
    "    for file in files:\n",
    "        print(file)        \n",
    "        with open(file, 'rb') as f:\n",
    "            x = pickle.load(f)\n",
    "            shap_tables.append(pd.read_pickle(file))\n",
    "        \n",
    "    #concatanate\n",
    "    result = pd.DataFrame(shap_tables, columns =['site', 'year', 'stg', 'fs', 'oversample', 'model', 'numberbt', 'modelobj', 'roc', 'cm'])\n",
    "\n",
    "    if return_result:\n",
    "        return result\n",
    "    \n",
    "    result.to_pickle('result_boosttrap.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5262078-50f3-44d0-8e18-c20181e685ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_collectSHAPraw_cross_sub(configs_variables):   \n",
    "    \n",
    "    if not configs_variables[0]['rerun_flag'] and os.path.exists(datafolder+'/shapalltmp.parquet'):\n",
    "        print('Existed: shapalltmp.parquet')\n",
    "        return\n",
    "\n",
    "    shap_data_raws = list()\n",
    "    for configs_variable_m in configs_variables:\n",
    "         for configs_variable_d in configs_variables:\n",
    "            datafolder = configs_variable_m['datafolder']\n",
    "            stg = configs_variable_m['stg']\n",
    "            fs = configs_variable_m['fs']\n",
    "            oversample = configs_variable_m['oversample']\n",
    "            model_type = configs_variable_m['model_type']   \n",
    "\n",
    "            drop_correlation_catboost = configs_variable_m['drop_correlation_catboost']\n",
    "            if drop_correlation_catboost:\n",
    "                suffix = 'nc'\n",
    "            else:\n",
    "                suffix = ''            \n",
    "            tmpdf = pd.read_parquet(datafolder+site_m+'/shapdataraw1d_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+suffix+'.parquet')\n",
    "            tmpdf['site_m'] = 'site_m'\n",
    "            tmpdf['site_d'] = 'site_d'\n",
    "            shap_data_raws.append(tmpdf)\n",
    "\n",
    "    shap_data_raws = pd.concat(shap_data_raws)\n",
    "    shap_data_raws.to_parquet(datafolder+'/shapdataraw1d.parquet')            \n",
    "    \n",
    "    shap_data_raws = list()\n",
    "    for configs_variable_m in configs_variables:\n",
    "         for configs_variable_d in configs_variables:\n",
    "            datafolder = configs_variable_m['datafolder']\n",
    "            stg = configs_variable_m['stg']\n",
    "            fs = configs_variable_m['fs']\n",
    "            oversample = configs_variable_m['oversample']\n",
    "            model_type = configs_variable_m['model_type']   \n",
    "\n",
    "            drop_correlation_catboost = configs_variable_m['drop_correlation_catboost']\n",
    "            if drop_correlation_catboost:\n",
    "                suffix = 'nc'\n",
    "            else:\n",
    "                suffix = ''            \n",
    "            tmpdf = pd.read_parquet(datafolder+site_m+'/shapdataraw2d_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+suffix+'.parquet')\n",
    "            tmpdf['site_m'] = 'site_m'\n",
    "            tmpdf['site_d'] = 'site_d'\n",
    "            shap_data_raws.append(tmpdf)\n",
    "\n",
    "    shap_data_raws = pd.concat(shap_data_raws)\n",
    "    shap_data_raws.to_parquet(datafolder+'/shapdataraw2d.parquet')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9e4b3-e410-4659-81fe-f2fb092a9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_collectSHAPraw_cross_sub_pre(configs_variables, top0=30):\n",
    "    \n",
    "    # get top features\n",
    "    df_importances, df_importances_stat = plot_utils.get_importances_features_stat(configs_variables)\n",
    "    df = df_importances.sort_values('rank', ascending=False).reset_index().groupby('site').head(top0)\n",
    "    top3030 = df[['site', 'Feature Id']].groupby('Feature Id').count().sort_values('site',ascending=False).head(top0)  \n",
    "    top3030 =top3030.index\n",
    "\n",
    "    shap_finals = list()\n",
    "\n",
    "    for configs_variable_m in configs_variables:\n",
    "        for configs_variable_d in configs_variables:\n",
    "            # read datas\n",
    "            year=3000\n",
    "            site_m, datafolder, home_directory = utils_function.get_commons(configs_variable_m)\n",
    "            site_d, datafolder, home_directory = utils_function.get_commons(configs_variable_d)\n",
    "\n",
    "            datafolder = configs_variable_m['datafolder']\n",
    "            stg = configs_variable_m['stg']\n",
    "            fs = configs_variable_m['fs']\n",
    "            oversample = configs_variable_m['oversample']\n",
    "            model_type = configs_variable_m['model_type']   \n",
    "\n",
    "            drop_correlation_catboost = configs_variable_m['drop_correlation_catboost']\n",
    "            if drop_correlation_catboost:\n",
    "                suffix = 'nc'\n",
    "            else:\n",
    "                suffix = ''     \n",
    "\n",
    "            if not configs_variable_m['rerun_flag'] and os.path.exists(datafolder+'/shapalltmp.parquet'):\n",
    "                print('Existed: shapalltmp.parquet')\n",
    "\n",
    "            print('Running collectSHAPraw_cross_sub '+model_type+' on site '+site_m+'/'+site_d+\":\"+str(year)+\":\"+stg+\":\"+fs+\":\"+oversample, flush = True)\n",
    "            tic = time.perf_counter()     \n",
    "\n",
    "            try:\n",
    "                columc_df = pd.read_pickle(datafolder+site_m+'/X_train_'+site_m+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+suffix+'.pkl')\n",
    "                feature_exists = list(set(columc_df.columns) & set(top3030))\n",
    "                \n",
    "                shapX = pd.read_parquet(datafolder+site_m+'/shapdatarawX_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+suffix+'.parquet', columns=feature_exists)\n",
    "                shap = pd.read_parquet(datafolder+site_m+'/shapdataraw_'+model_type+'_'+site_m+'_'+site_d+'_'+str(year)+'_'+stg+'_'+fs+'_'+oversample+suffix+'.parquet', columns=feature_exists)            \n",
    "                \n",
    "                shapX = shapX[feature_exists]\n",
    "                shap = shap[feature_exists]\n",
    "\n",
    "#                 # Reset index to convert the index to a column\n",
    "#                 shapX_reset = shapX.reset_index()\n",
    "#                 shapX_long = pd.melt(shapX_reset, id_vars=['index'], var_name='feature', value_name='value')\n",
    "#                 shapX_long = shapX_long.rename(columns={'index': 'ID'})\n",
    "#                 shapX_long.columns = ['ID', 'feature', 'Name']\n",
    "\n",
    "#                 # Reset index to convert the index to a column\n",
    "#                 shap_reset = shap.reset_index()\n",
    "#                 shap_long = pd.melt(shap_reset, id_vars=['index'], var_name='feature', value_name='value')\n",
    "#                 shap_long = shap_long.rename(columns={'index': 'ID'})\n",
    "\n",
    "#                 shap_final = shap_long.merge(shapX_long, on = ['ID', 'feature'], how='inner')\n",
    "                \n",
    "                shapX.columns = shapX.columns+'_Names'\n",
    "                shap.columns = shap.columns+'_vals'\n",
    "                shap_final = pd.concat([shapX, shap],axis=1)    \n",
    "    \n",
    "                shap_final['site_m'] = site_m\n",
    "                shap_final['site_d'] = site_d\n",
    "\n",
    "                shap_finals.append(shap_final)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    shap_finalX = pd.concat(shap_finals)\n",
    "    shap_finalX.to_parquet(datafolder+'/shapalltmp.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_CDM_PY",
   "language": "python",
   "name": "aki_cdm_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
