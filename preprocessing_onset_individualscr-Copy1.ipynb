{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936897e1-0e30-4bc7-a43a-32b8e1870b85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:37:41.318338Z",
     "start_time": "2023-12-03T23:37:39.324512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import python packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import importlib\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5828b-fe13-4187-b261-8daacd5bb6ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_list = ['KUMC', 'UTSW', 'MCW', 'UofU', 'UIOWA', 'UMHC', 'UPITT', 'MCRI', 'UTHSCSA', 'UNMC']\n",
    "ext_list = ['csv','dsv', 'dsv', 'csv', 'csv', 'csv', 'csv', 'csv', 'csv']\n",
    "sep_list = [',','|', '|', '|', ',', ',', ',', ',', '|']\n",
    "encoding_list = ['utf-8','utf-8','utf-8','utf-8','utf-8','utf-8', 'windows-1252', 'utf-8','utf-16'] \n",
    "ct = 1\n",
    "\n",
    "daily_average = True\n",
    "\n",
    "\n",
    "site = site_list[ct]\n",
    "ext = ext_list[ct]\n",
    "sep = sep_list[ct]\n",
    "encoding = encoding_list[ct]\n",
    "path = []\n",
    "\n",
    "if site != 'KUMC':\n",
    "    rawpath = '/blue/yonghui.wu/hoyinchan/Data/data2022raw/' + site + '/raw/'\n",
    "else: \n",
    "    rawpath = '/blue/yonghui.wu/hoyinchan/Data/data2022raw/' + site + '_ORCALE/raw/'\n",
    "path.append(rawpath)\n",
    "path.append('/blue/yonghui.wu/qixu/aki/data2022/' + site + '/')\n",
    "pdata = '/blue/yonghui.wu/hoyinchan/Data/data2022/'+ site \n",
    "site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e08053-c887-4392-b136-43c756e8ec6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_load_csv(filename, ext, sep, path):\n",
    "    if 'UMHC' in path[0]:\n",
    "        filename = 'DEID_'+filename    \n",
    "    try:\n",
    "        # Try to load the file from the first path\n",
    "        df = pd.read_csv( path[0] +  filename + '.' + ext, engine=\"pyarrow\", sep=sep, encoding=encoding)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load from {path[0]}: {e}. Loading converted csv...\")\n",
    "        try:\n",
    "            # If the first attempt fails, try to load the file from the second path\n",
    "            #df = pd.read_csv( path[0] +  filename + '.' + ext, engine=\"pyarrow\", sep=sep)\n",
    "            df = pd.read_csv(path[1] +  filename + '.csv', engine=\"pyarrow\", sep=',')\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                # If the first attempt fails, try to load the file from the second path\n",
    "                df = pd.read_csv(path[1] +  filename + '.csv', sep=',', on_bad_lines = 'skip')\n",
    "            except Exception as e:\n",
    "                # If the second attempt also fails, handle or re-raise the exception\n",
    "                print(f\"Failed to load from {path[1]} as well: {e}\")\n",
    "                raise Exception(f\"Could not load the file from either path.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a31ac8-6e98-45ff-9220-aa70eec5d2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pat_count(df, var = 'PATID'):\n",
    "    return df[var].nunique()\n",
    "\n",
    "def null_count(data):\n",
    "    return data.isnull().sum()/data.shape[0]\n",
    "\n",
    "def enc_count(df):\n",
    "    return df['ENCOUNTERID'].nunique()\n",
    "\n",
    "def aki_count(df):\n",
    "    return ( [pat_count(df), enc_count(df)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696b011-3605-4f9a-bf0b-7b05846a273f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_onset_data(path, ext, sep):\n",
    "    ## AKI staging\n",
    "    xxx = try_load_csv('AKI_LAB_SCR', ext, sep, path) #pd.read_csv(path+\"AKI_LAB_SCR.\"+ext, engine=\"pyarrow\", sep=sep)\n",
    "    yyy = try_load_csv('AKI_ONSETS', ext, sep, path) #pd.read_csv(path+\"AKI_ONSETS.\"+ext, engine=\"pyarrow\", sep=sep)\n",
    "    xxx.columns = xxx.columns.str.upper()\n",
    "    yyy.columns = yyy.columns.str.upper()\n",
    "    xxx.rename(columns = {'SPECIMEN_DATE\"+PD.DATE_SHIFT\"': 'SPECIMEN_DATE'}, inplace = True)\n",
    "    yyy.rename(columns = {'ADMIT_DATE\"+PD.DATE_SHIFT\"': 'ADMIT_DATE'}, inplace = True)\n",
    "    yyy = yyy[['ENCOUNTERID', 'PATID', 'ADMIT_DATE']]\n",
    "    yyy.columns = ['ENCOUNTERID', 'PATID', 'ADMIT_DATE']\n",
    "    \n",
    "#    xxx = xxx[['ONSETS_ENCOUNTERID', 'PATID', 'SPECIMEN_DATE', 'SPECIMEN_TIME', 'RESULT_NUM', 'DAYS_SINCE_ADMIT']]\n",
    "#    xxx.columns = ['ENCOUNTERID', 'PATID', 'SPECIMEN_DATE', 'SPECIMEN_TIME', 'RESULT_NUM', 'DAYS_SINCE_ADMIT']\n",
    "\n",
    "    xxx = xxx[['ONSETS_ENCOUNTERID', 'PATID', 'SPECIMEN_DATE', 'RESULT_NUM', 'DAYS_SINCE_ADMIT']]\n",
    "    xxx.columns = ['ENCOUNTERID', 'PATID', 'SPECIMEN_DATE', 'RESULT_NUM', 'DAYS_SINCE_ADMIT']\n",
    "\n",
    "    xxx = xxx.merge(yyy, on = ['ENCOUNTERID', 'PATID'], how='left')\n",
    "    #xxx['SPECIMEN_TIME'] = xxx['SPECIMEN_TIME'].fillna('00:00:00')\n",
    "    xxx = xxx.dropna()\n",
    "    xxx['SPECIMEN_DATE'] = pd.to_datetime(pd.to_datetime(xxx['SPECIMEN_DATE']).dt.date)\n",
    "    # xxx['SPECIMEN_DATETIME'] = xxx['SPECIMEN_DATE'].astype(str)+' '+xxx['SPECIMEN_TIME'].astype(str)\n",
    "    # xxx['SPECIMEN_DATETIME'] = pd.to_datetime(xxx['SPECIMEN_DATETIME'])\n",
    "    \n",
    "    xxx['ADMIT_DATE'] = pd.to_datetime(pd.to_datetime(xxx['ADMIT_DATE']).dt.date)\n",
    "    xxx['DAYS_SINCE_ADMIT'] = (xxx['SPECIMEN_DATE']-xxx['ADMIT_DATE']).dt.days\n",
    "    # xxx = xxx.sort_values('SPECIMEN_DATE')\n",
    "    # xxx = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATETIME', 'DAYS_SINCE_ADMIT', 'RESULT_NUM']].groupby(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATETIME', 'DAYS_SINCE_ADMIT']).mean()\n",
    "    xxx = xxx.sort_values(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'])\n",
    "    xxx = xxx.reset_index()\n",
    "    return xxx.drop_duplicates(), yyy.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d53b2-34ef-4a01-88bf-3633c0f39360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onset, df_y = read_onset_data(path, ext, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad864f6f-b4b7-4efc-a552-3fbac315ff5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_onset = df_onset[['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'RESULT_NUM', 'DAYS_SINCE_ADMIT']]\n",
    "# df_onset = df_onset.groupby(['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'DAYS_SINCE_ADMIT']).mean().reset_index()\n",
    "# df_y\n",
    "if daily_average:\n",
    "    df_onset = df_onset.groupby(['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'DAYS_SINCE_ADMIT']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99ba2a-5750-4ab4-a972-90e467e0e76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_onset.to_pickle(pdata+'/scr00.pkl')\n",
    "df_y.to_pickle(pdata+'/admit00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34600e89-8fe4-4c50-8852-9f34ef7df6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inverse_MDRD(row, KDIGO_baseline):\n",
    "    age = row[\"AGE\"]\n",
    "    is_male = True if row[\"MALE\"]  else False\n",
    "    is_black = True if row[\"RACE_BLACK\"] else False\n",
    "        \n",
    "    if is_male and is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Black males\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Black males\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Black males\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Black males\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Black males\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Black males\"]\n",
    "    \n",
    "    if is_male and not is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Other males\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Other males\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Other males\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Other males\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Other males\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Other males\"]\n",
    "\n",
    "    if not is_male and is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Black females\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Black females\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Black females\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Black females\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Black females\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Black females\"]\n",
    "    \n",
    "    if not is_male and not is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Other females\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Other females\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Other females\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Other females\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Other females\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Other females\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd329b9-e49b-4480-b257-87abcfccb8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scr_baseline(path, df_onset, df_y, ext, sep):\n",
    "\n",
    "    # process dx data\n",
    "    dx = try_load_csv('AKI_DX', ext, sep, path) # pd.read_csv(path+\"AKI_DX.\"+ext, sep= sep, engine=\"pyarrow\")\n",
    "    dx.columns = dx.columns.str.upper()\n",
    "    dx.rename(columns = {'DX_DATE\"+PD.DATE_SHIFT\"': 'DX_DATE'}, inplace = True)\n",
    "    dx[\"ENCOUNTERID\"] = dx[\"ONSETS_ENCOUNTERID\"]\n",
    "    # dx['ENCOUNTERID'] = dx['ENCOUNTERID'].astype(str)\n",
    "    # dx['PATID'] = dx['PATID'].astype(str)\n",
    "    dx = dx[[\"PATID\", \"ENCOUNTERID\", \"DX\", \"DAYS_SINCE_ADMIT\", \"DX_DATE\", \"DX_TYPE\"]]\n",
    "    dx = df_y[[\"PATID\",\"ENCOUNTERID\", 'ADMIT_DATE']].merge(dx, on = [\"PATID\",\"ENCOUNTERID\"], how = \"inner\")\n",
    "    dx['ADMIT_DATE'] = pd.to_datetime(pd.to_datetime(dx['ADMIT_DATE']).dt.date)\n",
    "    dx['DX_DATE'] = pd.to_datetime(pd.to_datetime(dx['DX_DATE']).dt.date)\n",
    "    dx['DAYS_SINCE_ADMIT'] = (dx['DX_DATE']-dx['ADMIT_DATE']).dt.days\n",
    "\n",
    "    # process demo data\n",
    "    demo = try_load_csv('AKI_DEMO', ext, sep, path) # pd.read_csv(path+'AKI_DEMO.'+ext, sep=sep,  engine=\"pyarrow\")\n",
    "    demo.columns = demo.columns.str.upper()\n",
    "    demo['ENCOUNTERID'] = demo['ONSETS_ENCOUNTERID']\n",
    "    # demo['ENCOUNTERID'] = demo['ENCOUNTERID'].astype(str)\n",
    "    # demo['PATID'] = demo['PATID'].astype(str)\n",
    "    demo['MALE'] = demo['SEX'] == 'M'\n",
    "    demo['HISPANIC'] = demo['HISPANIC'] == 'Y'\n",
    "    demo['RACE_WHITE'] = demo['RACE'] == '05'\n",
    "    demo['RACE_BLACK'] = demo['RACE'] == '03'\n",
    "    demo = demo[['PATID', 'ENCOUNTERID', 'AGE', 'MALE', 'RACE_WHITE', 'RACE_BLACK', 'HISPANIC']]\n",
    "    demo = demo.drop_duplicates()\n",
    "    \n",
    "    return dx, demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6acac-e1d7-4069-a6f2-e67bab66479c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scr_baseline2(path, df_onset, df_y, ext, sep, dx, demo):\n",
    "    \n",
    "    # Estimate SCr Baseline\n",
    "    pat_id_cols = [\"PATID\",  \"ENCOUNTERID\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    complete_df = df_onset[['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    " \n",
    "    complete_df['ADMIT_DATE'] =  pd.to_datetime(pd.to_datetime(complete_df['ADMIT_DATE']).dt.date)\n",
    "\n",
    "    # 1. min between the min of 1-week prior admission SCr and within 24 hour after admission SCr\n",
    "    #SCr within 24 hour after admission, that is admission day and one day after, get mean\n",
    "    admission_SCr = complete_df[(complete_df.SPECIMEN_DATE >= complete_df.ADMIT_DATE) & \\\n",
    "                                (complete_df.SPECIMEN_DATE <= (complete_df.ADMIT_DATE + pd.Timedelta(days=1)))].copy()\n",
    "\n",
    "    # Admission SCr is the mean of all the SCr within 24h admission\n",
    "    admission_SCr = admission_SCr.groupby(pat_id_cols)['RESULT_NUM'].mean().reset_index()\n",
    "\n",
    "    admission_SCr.rename(columns = {\"RESULT_NUM\": \"ADMISSION_SCR\"}, inplace = True)\n",
    "\n",
    "    #merge the ADMISSION_SCR back to the main frame\n",
    "    complete_df = complete_df.merge(admission_SCr, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = \"left\")\n",
    "\n",
    "    #SCr within 7 days prior to admission\n",
    "    one_week_prior_admission = complete_df[(complete_df.SPECIMEN_DATE >= complete_df.ADMIT_DATE - pd.Timedelta(days=7)) & \\\n",
    "                                     (complete_df.SPECIMEN_DATE < complete_df.ADMIT_DATE)].copy()\n",
    "    one_week_prior_admission = one_week_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].min().reset_index()\n",
    "    one_week_prior_admission.rename(columns = {\"RESULT_NUM\": \"ONE_WEEK_SCR\"}, inplace = True)\n",
    "\n",
    "    complete_df = complete_df.merge(one_week_prior_admission, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = \"left\")\n",
    "\n",
    "    #take the min between one week SCr and admission SCr\n",
    "    complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), \"BASELINE_EST_1\"] = \\\n",
    "    np.min(complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), [\"ONE_WEEK_SCR\",\"ADMISSION_SCR\"]], axis = 1)\n",
    "\n",
    "    ori_num_unique_combinations = df_onset.groupby(['PATID', 'ENCOUNTERID']).ngroups\n",
    "    #get the percentage of encounters that do not have past 7-day records\n",
    "    criterion1_no_missing = complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), :].groupby(pat_id_cols).ngroups\n",
    "    criterion1_missing_rate = 1 - criterion1_no_missing / ori_num_unique_combinations\n",
    "\n",
    "    # 2. pre-admission 365-7 day mean\n",
    "    #here we only care about SCr measurements within 1 year before hospitalization\n",
    "    one_year_prior_admission = complete_df[(complete_df.SPECIMEN_DATE < (complete_df.ADMIT_DATE - pd.Timedelta(days=7))) & \\\n",
    "                                     (complete_df.SPECIMEN_DATE >= (complete_df.ADMIT_DATE - pd.Timedelta(days=365.25)))].copy()\n",
    "    one_year_prior_admission = one_year_prior_admission.loc[:, pat_id_cols + [\"RESULT_NUM\"]]\n",
    "\n",
    "    one_year_prior_admission = one_year_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].mean().reset_index()\n",
    "\n",
    "    one_year_prior_admission.rename(columns = {\"RESULT_NUM\": \"ONE_YEAR_SCR\"}, inplace = True)\n",
    "\n",
    "    complete_df = complete_df.merge(one_year_prior_admission, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = \"left\")\n",
    "    #take the min between one week SCr and admission SCr\n",
    "    complete_df.loc[complete_df.ONE_YEAR_SCR.notna(), \"BASELINE_EST_2\"] = \\\n",
    "    np.min(complete_df.loc[complete_df.ONE_YEAR_SCR.notna(), [\"ONE_YEAR_SCR\",\"ADMISSION_SCR\"]], axis = 1)\n",
    "\n",
    "    # priority 1: 7day SCr, priority 2: one year SCr\n",
    "    complete_df[\"BASELINE_NO_MDRD\"] = \\\n",
    "    np.where(complete_df['BASELINE_EST_1'].isna(), complete_df['BASELINE_EST_2'], complete_df['BASELINE_EST_1'])\n",
    "\n",
    "    #get the percentage of encounters that do not have any 1-year records\n",
    "    criterion12_no_missing = complete_df.loc[complete_df.BASELINE_NO_MDRD.notna(), :].groupby(pat_id_cols).ngroups\n",
    "    criterion12_missing_rate = 1 - criterion12_no_missing / ori_num_unique_combinations\n",
    "\n",
    "    # 3. MDRD to estimate baseline (only for non-CKD patients)\n",
    "    #get those need to use MDRD to impute baseline\n",
    "    pat_to_MDRD = complete_df.loc[complete_df.BASELINE_NO_MDRD.isna(), pat_id_cols+[\"ADMIT_DATE\", \"ADMISSION_SCR\"]]\n",
    "    #one patient one row\n",
    "    pat_to_MDRD.drop_duplicates(subset=pat_id_cols, keep=\"first\", inplace = True)\n",
    "\n",
    "\n",
    "    #adjust name and data type\n",
    "    # dx[\"PATID\"] = dx[\"PATID\"].astype(str)\n",
    "    dx[\"DX\"] = dx[\"DX\"].astype(str)\n",
    "    dx[\"DX_TYPE\"] = dx[\"DX_TYPE\"].astype(str)\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].replace('09', '9')\n",
    "\n",
    "    # pat_to_MDRD[\"PATID\"] = pat_to_MDRD[\"PATID\"].astype(str)\n",
    "\n",
    "    pat_to_MDRD_check_CKD = pat_to_MDRD.merge(dx.drop(['ENCOUNTERID', 'ADMIT_DATE'], axis = 1), \n",
    "                                              on = \"PATID\", \n",
    "                                              how = \"left\")\n",
    "\n",
    "    pat_to_MDRD_check_CKD.DX_DATE.isna().mean()\n",
    "\n",
    "    #calculate DX_DATE for missing\n",
    "    pat_to_MDRD_check_CKD.loc[pat_to_MDRD_check_CKD.DX_DATE.isna(), \"DX_DATE\"] = \\\n",
    "    pat_to_MDRD_check_CKD.loc[pat_to_MDRD_check_CKD.DX_DATE.isna(), \"ADMIT_DATE\"] + \\\n",
    "    pd.to_timedelta(pat_to_MDRD_check_CKD.loc[pat_to_MDRD_check_CKD.DX_DATE.isna(), 'DAYS_SINCE_ADMIT'], unit='D')\n",
    "\n",
    "    \n",
    "    #still have patients that do not have DX in the database\n",
    "    pat_to_MDRD_check_CKD.DX_DATE.isna().mean()\n",
    "\n",
    "    # filter out those DX after admission\n",
    "    pat_to_MDRD_check_CKD = pat_to_MDRD_check_CKD[pat_to_MDRD_check_CKD.DX_DATE <= \\\n",
    "                                                 pat_to_MDRD_check_CKD.ADMIT_DATE]\n",
    "    pat_to_MDRD_check_CKD\n",
    "    #assert(pat_to_MDRD_check_CKD.DX_DATE.isna().mean() == 0)\n",
    "\n",
    "    CKD_code = [\"585.1\",\"585.2\",\"585.3\",\"585.4\",\"585.5\",\"585.9\",\n",
    "                \"N18.1\",\"N18.2\",\"N18.3\",\"N18.4\",\n",
    "                \"N18.5\",\"N18.6\",\"N18.9\"]\n",
    "\n",
    "    pat_to_MDRD_with_CKD = pat_to_MDRD_check_CKD[pat_to_MDRD_check_CKD.DX.isin(CKD_code)]\n",
    "\n",
    "    n_CKD_encounter = pat_to_MDRD_with_CKD.groupby(pat_id_cols).ngroups\n",
    "    n_CKD_encounter\n",
    "\n",
    "    CKD_no_history_rate = n_CKD_encounter / ori_num_unique_combinations\n",
    "    CKD_no_history_rate\n",
    "\n",
    "    patid_without_CKD = list(pat_to_MDRD.loc[~pat_to_MDRD.PATID.isin(pat_to_MDRD_with_CKD.PATID),\n",
    "                                             \"PATID\"].unique())\n",
    "\n",
    "    # Apply MDRD to NON-CKD patients\n",
    "\n",
    "    KDIGO_baseline = np.array([\n",
    "        [1.5, 1.3, 1.2, 1.0],\n",
    "        [1.5, 1.2, 1.1, 1.0],\n",
    "        [1.4, 1.2, 1.1, 0.9],\n",
    "        [1.3, 1.1, 1.0, 0.9],\n",
    "        [1.3, 1.1, 1.0, 0.8],\n",
    "        [1.2, 1.0, 0.9, 0.8]\n",
    "    ])\n",
    "    KDIGO_baseline = pd.DataFrame(KDIGO_baseline, columns = [\"Black males\", \"Other males\",\n",
    "                                                            \"Black females\", \"Other females\"],\n",
    "                                 index = [\"20-24\", \"25-29\", \"30-39\", \"40-54\", \"55-65\", \">65\"])\n",
    "\n",
    "    # pat_to_MDRD[\"PATID\"] = pat_to_MDRD[\"PATID\"].astype(str)\n",
    "    demo[\"AGE\"] = demo[\"AGE\"].astype(int)\n",
    "\n",
    "    #merge DEMO with pat_to_MRDR\n",
    "    pat_to_MDRD = pat_to_MDRD.merge(demo, on = pat_id_cols, how = \"left\")\n",
    "\n",
    "    #calculate on non CKD patient\n",
    "    pat_to_MDRD.loc[pat_to_MDRD.PATID.isin(patid_without_CKD), \"BASELINE_MDRD\"] = pat_to_MDRD.apply(inverse_MDRD, args = (KDIGO_baseline,), axis = 1)\n",
    "\n",
    "    #for CKD patients without prior history, use admission SCr\n",
    "    pat_to_MDRD[\"BASELINE_EST_3\"] = np.min(pat_to_MDRD[[\"ADMISSION_SCR\", \"BASELINE_MDRD\"]], axis = 1)\n",
    "\n",
    "\n",
    "    #merge back MRDR computation results\n",
    "    complete_df = complete_df.merge(pat_to_MDRD[pat_id_cols + [\"BASELINE_EST_3\"]], \n",
    "                                    on = pat_id_cols,\n",
    "                                   how = \"left\")\n",
    "\n",
    "\n",
    "    #replace the old baseline\n",
    "    # since BASELINE_NO_MDRD and BASELINE_EST_3 are mutually exclusive, just use min\n",
    "    complete_df[\"SERUM_CREAT_BASE\"] = np.min(complete_df[[\"BASELINE_NO_MDRD\", \"BASELINE_EST_3\"]], axis = 1)\n",
    "\n",
    "    #drop those still cannot find baseline\n",
    "    complete_df = complete_df.dropna(subset=['SERUM_CREAT_BASE'])\n",
    "    complete_df\n",
    "    return complete_df#.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ca93a-acaa-405b-aea9-708800ca2acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dx, demo = get_scr_baseline(path, df_onset, df_y,  ext, sep)\n",
    "dx.to_pickle(pdata+'/dx00.pkl')\n",
    "demo.to_pickle(pdata+'/demo00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b66b1-a5f1-4357-bcdd-3b6ab3fe49bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_base = get_scr_baseline2(path, df_onset, df_y,  ext, sep, dx, demo)\n",
    "df_base.to_pickle(pdata+'/df_base00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876c218-312f-48a8-b136-9d64a38f2d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rrt(path, ext, sep, df_y):\n",
    "    px = try_load_csv('AKI_PX', ext, sep, path) # pd.read_csv(path+\"AKI_PX.\"+ext, engine=\"pyarrow\", sep= sep)\n",
    "    px.columns = px.columns.str.upper()\n",
    "    px.rename(columns = {'PX_DATE\"+PD.DATE_SHIFT\"': 'PX_DATE'}, inplace = True)\n",
    "    px[\"ENCOUNTERID\"] = px[\"ONSETS_ENCOUNTERID\"]\n",
    "\n",
    "    idx_transplant = np.logical_or(np.logical_or(\n",
    "                           np.logical_and(px['PX_TYPE']=='CH',px['PX'].isin(['50300','50320','50323','50325','50327','50328','50329','50340','50360','50365','50370','50380'])),\n",
    "                           np.logical_and(px['PX_TYPE']=='09',px['PX'].isin(['55.51','55.52','55.53','55.54','55.61','55.69']))),np.logical_or(\n",
    "                           np.logical_and(px['PX_TYPE']=='9',px['PX'].isin(['55.51','55.52','55.53','55.54','55.61','55.69'])),                       \n",
    "                           np.logical_and(px['PX_TYPE']=='10',px['PX'].isin(['0TY00Z0','0TY00Z1','0TY00Z2','0TY10Z0','0TY10Z1','0TY10Z2','0TB00ZZ','0TB10ZZ','0TT00ZZ','0TT10ZZ','0TT20ZZ']))))\n",
    "\n",
    "    idx_dialysis =( ( (px['PX_TYPE']=='CH')  &  (px['PX'].isin(['90935', '90937'])))  |   # ( (px['PX_TYPE']=='CH')  & \n",
    "        ( (px['PX_TYPE']=='CH')  & (pd.to_numeric(px['PX'], errors='coerce').between(90940, 90999))) |   #(px['PX_TYPE']=='CH')  &\n",
    "        ( (px['PX_TYPE']=='9')  & ( (px['PX'].isin(['39.93','39.95','54.98', 'V45.11']))) )  |  #(px['PX_TYPE']=='9')  &\n",
    "        ( (px['PX_TYPE']=='09')  & (px['PX'].isin(['39.93','39.95','54.98', 'V45.11'])) ) |  #(px['PX_TYPE']=='09')  &\n",
    "        ( (px['PX_TYPE']=='10')  & (px['PX'].isin(['5A1D00Z','5A1D60Z','5A1D70Z','5A1D80Z','5A1D90Z', 'Z99.2']))) )  #(px['PX_TYPE']=='10')  &\n",
    "   \n",
    " \n",
    "    rrt_stage =  px[idx_transplant | idx_dialysis] \n",
    "\n",
    "    \n",
    "    rrt_stage['PX_DATE'] = pd.to_datetime(pd.to_datetime(rrt_stage['PX_DATE']).dt.date )\n",
    "    df_y['ADMIT_DATE'] = pd.to_datetime(pd.to_datetime(df_y['ADMIT_DATE']).dt.date ) \n",
    "    rrt_stage = rrt_stage[['PATID','ENCOUNTERID','PX_DATE']]\n",
    "    rrt_stage.columns = ['PATID','ENCOUNTERID','RRT3_ONSET_DATE']\n",
    "    rrt_stage['RRT3_ONSET_DATE'] = pd.to_datetime(pd.to_datetime(rrt_stage['RRT3_ONSET_DATE']).dt.date )\n",
    "\n",
    "    rrt_stage = rrt_stage.merge(df_y, on=['PATID', 'ENCOUNTERID'], how='inner')\n",
    "    \n",
    "    rrt_stage['RRT3_ONSET_DATE'] = pd.to_datetime(rrt_stage['RRT3_ONSET_DATE'])\n",
    "    rrt_stage['ADMIT_DATE'] = pd.to_datetime(rrt_stage['ADMIT_DATE'])\n",
    "    rrt_stage['RRT3_SINCE_ADMIT'] = (rrt_stage['RRT3_ONSET_DATE']-rrt_stage['ADMIT_DATE']).dt.total_seconds()/(3600*24)\n",
    "    rrt_stage = rrt_stage.loc[rrt_stage[['ENCOUNTERID', 'RRT3_SINCE_ADMIT']].groupby('ENCOUNTERID').idxmin().reset_index()['RRT3_SINCE_ADMIT']]\n",
    "    rrt_stage.drop('ADMIT_DATE', axis = 1, inplace = True)\n",
    "    return rrt_stage, px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b616910-91e1-4b2c-aec8-e90d84d51c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rrt, px = get_rrt(path, ext, sep, df_y)\n",
    "df_rrt.to_pickle(pdata+'/df_rrt00.pkl')\n",
    "px.to_pickle(pdata+'/px.pkl')\n",
    "df_rrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e197ee88-d28f-4330-81b9-24321dabd29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_onset_new(df_onset, df_y, df_rrt, df_base, part = 'either'):\n",
    "    xxx = df_onset.copy()\n",
    "    yyy = df_y.copy()\n",
    "\n",
    "    zzz = df_base[['PATID', 'ENCOUNTERID', 'SERUM_CREAT_BASE']].drop_duplicates()\n",
    "    zzz.columns= ['PATID', 'ENCOUNTERID',  'RESULT_NUM_BASE_7d']\n",
    "    xxx = xxx.merge(zzz, on = ['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "    zzz2 = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']].groupby(['PATID', 'ENCOUNTERID']).rolling('2d', on='SPECIMEN_DATE').min().reset_index()\n",
    "    zzz2 = zzz2[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    "    zzz2.columns= ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM_BASE_2d']\n",
    "    xxx = xxx.merge(zzz2, on = ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'], how='left')\n",
    "\n",
    "    #0.3 increase in 48 hours\n",
    "    #1.5 increase in 7 days\n",
    "    xxx['AKI1.5'] = (xxx['RESULT_NUM']>=1.5*xxx['RESULT_NUM_BASE_7d']) & (xxx['DAYS_SINCE_ADMIT']>=0)                        \n",
    "    xxx['AKI0.3'] = (xxx['RESULT_NUM']-xxx['RESULT_NUM_BASE_2d']>=0.3) & (xxx['DAYS_SINCE_ADMIT']>=0)      \n",
    "    \n",
    "    if part == '2d':\n",
    "        xxx = xxx[xxx['AKI0.3']]\n",
    "        xxx = xxx.sort_values('SPECIMEN_DATE')\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "        \n",
    "    elif part == '7d':\n",
    "        xxx = xxx[xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values('SPECIMEN_DATE')\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "\n",
    "    elif part == 'both':\n",
    "        xxx = xxx[xxx['AKI0.3'] & xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values('SPECIMEN_DATE')\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "        \n",
    "    elif part == 'either':\n",
    "        xxx = xxx[xxx['AKI0.3'] | xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI1.5'], 'RESULT_NUM_BASE_7d']\n",
    "\n",
    "    #xxx = xxx.apply(set_result_base,axis=1)\n",
    "    xxx['AKI1_SINCE_ADMIT'] = xxx['DAYS_SINCE_ADMIT'].copy()\n",
    "    xxx['AKI1_DATE'] = xxx['SPECIMEN_DATE'].copy()\n",
    "    #xxx['AKI1_DATETIME'] = xxx['SPECIMEN_DATETIME'].copy()\n",
    "    xxx['AKI1_SCR'] = xxx['RESULT_NUM'].copy()\n",
    "    xxx['SCR_BASELINE'] = xxx['RESULT_NUM_BASE'].copy()\n",
    "    xxx['SCR_REFERENCE'] = xxx['RESULT_NUM_BASE_2d'].copy()\n",
    "    xxx['AKI1_7D'] = xxx['AKI1.5'].copy()\n",
    "    xxx['AKI1_2D'] = xxx['AKI0.3'].copy()\n",
    "    xxx = xxx[['PATID', 'ENCOUNTERID', 'SCR_BASELINE', 'SCR_REFERENCE',  'AKI1_DATE', 'AKI1_SCR', 'AKI1_SINCE_ADMIT', 'AKI1_7D', 'AKI1_2D']]\n",
    "\n",
    "    # Check condition for AKI2: 2.0x - <3.0x\n",
    "    aki2 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki2 = aki2[aki2['SPECIMEN_DATE']>=aki2['AKI1_DATE']]\n",
    "    aki2 = aki2[aki2['RESULT_NUM']>=2*aki2['SCR_BASELINE']]\n",
    "    aki2 = aki2.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki2['AKI2_SINCE_ADMIT'] = aki2['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki2['AKI2_DATE'] = aki2['SPECIMEN_DATE'].copy()\n",
    "    #aki2['AKI2_DATETIME'] = aki2['SPECIMEN_DATETIME'].copy()\n",
    "    aki2['AKI2_SCR'] = aki2['RESULT_NUM'].copy()\n",
    "    aki2 = aki2[['PATID', 'ENCOUNTERID', 'AKI2_DATE', 'AKI2_SCR', 'AKI2_SINCE_ADMIT']]\n",
    "    \n",
    "    # Check condition for AKI3: SCR >= 3.0x Baseline\n",
    "    aki3 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki3 = aki3[aki3['SPECIMEN_DATE']>=aki3['AKI1_DATE']]\n",
    "    aki3 = aki3[(aki3['RESULT_NUM']>=3*aki3['SCR_BASELINE']) | (aki3['RESULT_NUM']>=4)]\n",
    "    aki3 = aki3.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki3['AKI3_SINCE_ADMIT'] = aki3['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki3['AKI3_DATE'] = aki3['SPECIMEN_DATE'].copy()\n",
    "    #aki3['AKI3_DATETIME'] = aki3['SPECIMEN_DATETIME'].copy()\n",
    "    aki3['AKI3_SCR'] = aki3['RESULT_NUM'].copy()\n",
    "    aki3 = aki3[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    \n",
    "    # Check condition for AKI3: initiation of RRT\n",
    "    #df_rrt = get_rrt(path, ext, sep, yyy)\n",
    "    rrt = df_rrt.merge(xxx[['PATID', 'ENCOUNTERID', 'AKI1_DATE']], on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    rrt = rrt[rrt['RRT3_ONSET_DATE'] >= rrt['AKI1_DATE']]\n",
    "    aki3b =  aki3.merge(rrt, on = ['PATID', 'ENCOUNTERID'], how = 'outer')\n",
    "    cond_rrt = (aki3b['RRT3_SINCE_ADMIT'] < aki3b['AKI3_SINCE_ADMIT']) | (aki3b['AKI3_SINCE_ADMIT'].isna() & aki3b['RRT3_SINCE_ADMIT'].notna())\n",
    "    aki3b.loc[cond_rrt, 'AKI3_SINCE_ADMIT'] = aki3b.loc[cond_rrt, 'RRT3_SINCE_ADMIT']\n",
    "    aki3b.loc[cond_rrt, 'AKI3_DATE'] = aki3b.loc[cond_rrt, 'RRT3_ONSET_DATE']\n",
    "    #aki3b.loc[cond_rrt, 'AKI3_DATETIME'] = aki3b.loc[cond_rrt, 'RRT3_ONSET_DATETIME']\n",
    "    \n",
    "    aki3_all = aki3b[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    \n",
    "    onset = xxx.merge(aki2, on=['PATID', 'ENCOUNTERID'], how='outer').merge(aki3_all, on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    onset = onset.merge(yyy, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "    onset.columns = onset.columns.str.upper()\n",
    "    onset['ONSET_DATE'] = onset['AKI1_DATE'].copy()  \n",
    "    \n",
    "    onset['SCR_ONSET'] = onset['AKI1_SCR'].copy() \n",
    "\n",
    "    onset = onset[[\"PATID\",\"ENCOUNTERID\", \"ADMIT_DATE\", \n",
    "                   'ONSET_DATE', \"AKI1_SINCE_ADMIT\", \"AKI2_SINCE_ADMIT\", \n",
    "                   \"AKI3_SINCE_ADMIT\", \"SCR_ONSET\", \n",
    "                   'SCR_BASELINE',  'SCR_REFERENCE', 'AKI1_7D', 'AKI1_2D']]\n",
    "\n",
    "    onset['FLAG'] = (onset['AKI2_SINCE_ADMIT'].notnull()) | (onset['AKI3_SINCE_ADMIT'].notnull())\n",
    "    onset['ONSET_SINCE_ADMIT'] = onset[\"AKI1_SINCE_ADMIT\"].copy()  #onset[[\"AKI1_SINCE_ADMIT\", \"AKI2_SINCE_ADMIT\", \"AKI3_SINCE_ADMIT\"]].min(axis=1)\n",
    "        \n",
    "    \n",
    "    onset['AKI_STAGE'] = 0\n",
    "    \n",
    "    aki_s3 = onset['AKI3_SINCE_ADMIT'].notna()\n",
    "    aki_s2 = onset['AKI2_SINCE_ADMIT'].notna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    aki_s1 = onset['AKI1_SINCE_ADMIT'].notna() & onset['AKI2_SINCE_ADMIT'].isna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    \n",
    "    onset.loc[aki_s3, 'AKI_STAGE'] = 3\n",
    "    onset.loc[aki_s2, 'AKI_STAGE'] = 2\n",
    "    onset.loc[aki_s1, 'AKI_STAGE'] = 1\n",
    "    \n",
    "    return onset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9baf863-20aa-4dc9-8cea-ae422f9fb2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onset = read_onset_new(df_onset = df_onset, df_y = df_y, df_rrt= df_rrt, df_base = df_base, part = 'either')\n",
    "onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b8d0b-acf8-4eb0-87e5-2f81e24b9452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onset.to_pickle(pdata+'/onset00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a7496-212f-4eb0-a038-f84292a23254",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset.to_pickle(pdata+'/onset00.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68671d0d-18fb-4e89-a88d-5a97f001d87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AKI_PM_TEMPORAL_MOEA",
   "language": "python",
   "name": "aki_pm_temporal_moea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
