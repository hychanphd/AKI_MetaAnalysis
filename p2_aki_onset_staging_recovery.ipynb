{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9e03ef-fcd8-48ed-b210-bd1aa7b42011",
   "metadata": {},
   "source": [
    "# AKI Onset Determination, Staging, and Recovery\n",
    "\n",
    "This document outlines the steps to determine AKI (Acute Kidney Injury) onset, stage, and resolving status for our cohort using the definitions from KDIGO guidelines (Kidney Disease: Improving Global Outcomes, 2012). The process involves multiple steps, including importing necessary packages, setting up data paths, defining helper functions, and applying the KDIGO criteria to identify and stage AKI events in the cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654029ea-730e-476f-aaa2-23034666164a",
   "metadata": {},
   "source": [
    "### 1. Importing Required Packages and Define Helper Functions.\n",
    "We begin by importing the necessary Python packages and define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936897e1-0e30-4bc7-a43a-32b8e1870b85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T23:37:41.318338Z",
     "start_time": "2023-12-03T23:37:39.324512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc119c4-861c-4831-85a1-4114c35077e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_file_path(home_path, site):\n",
    "    input_path = os.path.join(home_path, 'raw_data', site) + '/'\n",
    "    output_path = os.path.join(home_path, 'data', site) + '/'\n",
    "    aux_path = os.path.join(home_path, 'aux_files') + '/'\n",
    "    # Check if the output path exists, if not, create it\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    return [input_path, output_path, aux_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cb2f93-de67-42d6-8633-7b8b901cc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_scr(onset, filepath_lst):\n",
    "    xxx = pd.read_pickle(filepath_lst[0]+'AKI_LAB_SCR.pkl') \n",
    "    xxx = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE',  'RESULT_NUM']] \n",
    "    \n",
    "    xxx = xxx.groupby(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE']).mean()\n",
    "    xxx = xxx.sort_values(['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'])\n",
    "    xxx = xxx.reset_index()\n",
    "    # merge with onset data\n",
    "    xxx = onset.merge(xxx, on = ['ENCOUNTERID', 'PATID'], how='inner')\n",
    "    xxx['DAYS_SINCE_ONSET'] = (xxx['SPECIMEN_DATE']-xxx['ONSET_DATE']).dt.days\n",
    "    return xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2380fc52-7264-4f23-bf70-281527c757c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_MDRD(row, KDIGO_baseline):\n",
    "    age = row[\"AGE\"]\n",
    "    is_male = True if row[\"MALE\"]  else False\n",
    "    is_black = True if row[\"RACE_BLACK\"] else False\n",
    "        \n",
    "        \n",
    "    KDIGO_baseline = np.array([\n",
    "        [1.5, 1.3, 1.2, 1.0],\n",
    "        [1.5, 1.2, 1.1, 1.0],\n",
    "        [1.4, 1.2, 1.1, 0.9],\n",
    "        [1.3, 1.1, 1.0, 0.9],\n",
    "        [1.3, 1.1, 1.0, 0.8],\n",
    "        [1.2, 1.0, 0.9, 0.8]\n",
    "    ])\n",
    "    KDIGO_baseline = pd.DataFrame(KDIGO_baseline, columns = [\"Black males\", \"Other males\",\n",
    "                                                            \"Black females\", \"Other females\"],\n",
    "                                 index = [\"20-24\", \"25-29\", \"30-39\", \"40-54\", \"55-65\", \">65\"])\n",
    "\n",
    "    if is_male and is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Black males\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Black males\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Black males\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Black males\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Black males\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Black males\"]\n",
    "    \n",
    "    if is_male and not is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Other males\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Other males\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Other males\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Other males\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Other males\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Other males\"]\n",
    "\n",
    "    if not is_male and is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Black females\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Black females\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Black females\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Black females\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Black females\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Black females\"]\n",
    "    \n",
    "    if not is_male and not is_black:\n",
    "        if age <= 24:\n",
    "            return KDIGO_baseline.loc[\"20-24\", \"Other females\"]\n",
    "        elif 25 <= age <= 29:\n",
    "            return KDIGO_baseline.loc[\"25-29\", \"Other females\"]\n",
    "        elif 30 <= age <= 39:\n",
    "            return KDIGO_baseline.loc[\"30-39\", \"Other females\"]\n",
    "        elif 40 <= age <= 54:\n",
    "            return KDIGO_baseline.loc[\"40-54\", \"Other females\"]\n",
    "        elif 55 <= age <= 65:\n",
    "            return KDIGO_baseline.loc[\"55-65\", \"Other females\"]\n",
    "        elif age > 65:\n",
    "            return KDIGO_baseline.loc[\">65\", \"Other females\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da6452-3798-4eb8-b341-d9e0ea91f219",
   "metadata": {},
   "source": [
    "### 2. Determine AKI Onset, Stage, and Recovery Status\n",
    "To accurately identify and stage AKI, we adhere to the definitions in KDIGO guidelines. These guidelines provide specific criteria for determining AKI onset and categorizing the severity of the condition.\n",
    "\n",
    "#### **AKI Onset:**\n",
    "AKI onset is defined as any of the following (Not Graded):\n",
    "* Increase in SCr by >= 0.3 mg/dl (>= 26.5 μmol/l) within 48 hours; or\n",
    "* Increase in SCr to >= 1.5 times baseline, which is known or presumed to have occurred within the prior 7 days; or \n",
    "* Urine volume < 0.5 ml/kg/h for 6 hours.\n",
    "\n",
    "#### **AKI Staging:**\n",
    "AKI is staged for severity according to the following criteria:\n",
    "* **Stage 1**: If SCr increases by 1.5–2.0 (strictly less than 2.0) times the baseline value or increases by >= 0.3 mg/dl (>= 26.5 μmol/l); or if urine output <0.5 ml/kg/h for 6–12 hours.\n",
    "* **Stage 2**: If SCr increases by 2.0–3.0 (strictly less than 3.0) times the baseline value; or if urine output <0.5 ml/kg/h for >=12 hours.\n",
    "* **Stage 3**: If SCr increases by >= 3.0 times the baseline value or increases to 4.0 mg/dl (>= 353.6 μmol/l); or initiation of renal replacement therapy (RRT); or if urine output <0.3 ml/kg/h for >=24 hours or anuria for >=12 hours.\n",
    "\n",
    "#### **Resolving AKI:**\n",
    "Resolving AKI is defined as a decrease in serum creatinine concentration of 0.3 mg/dL or more, or 25% or more from the maximum in the first 72 hours after AKI onset.\n",
    "\n",
    "We are currently excluding the urine output condition from the definition due to the lack of relevant data. Instead, we are focusing on the SCr condition. To begin, we need to establish the AKI baseline value and ascertain whether a patient has been on RRT. In what follows, we will detail the functions used to define the baseline SCr value and to determine the RRT status and timing for patients undergoing RRT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba639a5b-5f4c-4680-85d3-f2223d64b134",
   "metadata": {},
   "source": [
    "#### 2.1. Calculating Baseline SCr Level\n",
    "\n",
    "To define AKI onset and stage AKI according to the KDIGO guidelines, we first need to determine the baseline SCr level for each patient. The following definition is used to determine the baseline SCr level.\n",
    "\n",
    "#### Workflow for Defining SCr Baseline:\n",
    "If SCr records exist from the past 7 days before admission:\n",
    "* Use the lower value between the lowest SCr from the past 7 days and the SCr value within the first 24 hours of admission as the SCr baseline.\n",
    "\n",
    "If no SCr records exist from the past 7 days but are available for the period 365 days to 7 days before admission:\n",
    "* Use the lower value between the most recent SCr from this period and the SCr value within the first 24 hours of admission as the SCr baseline.\n",
    "\n",
    "If no SCr records are available from the past 365 days:\n",
    "* For non-CKD patients: The SCr baseline is determined by the lower value between the SCr inferred using the [MDRD formula](https://www.kidney.org/content/mdrd-study-equation) with an eGFR of 75 mL/min/1.73 m² and the SCr value within the first 24 hours of admission.\n",
    "* For CKD patients: The SCr baseline is determined by the lower value between the SCr inferred using the MDRD formula with an eGFR corresponding to the CKD stage and the SCr value within the first 24 hours of admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d132279-eeaa-4053-a557-d9ca1a715f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scr_baseline(df_scr, df_admit, filepath_lst, c7day = 'MOST_RECENT', c365day = 'AVERAGE', cckd = 'DROP'):\n",
    "\n",
    "    cohort_table = dict()\n",
    "    \n",
    "    # load & process dx data\n",
    "    dx = pd.read_pickle(filepath_lst[0]+'AKI_DX.pkl')  \n",
    "    dx = dx[['PATID', 'ENCOUNTERID', 'DX', 'DX_DATE', 'DX_TYPE']] \n",
    "    dx = df_admit[['PATID', 'ENCOUNTERID', 'ADMIT_DATE']].merge(dx, on = ['PATID', 'ENCOUNTERID'], how = 'inner')\n",
    "    dx['DAYS_SINCE_ADMIT'] = (dx['DX_DATE']-dx['ADMIT_DATE']).dt.days\n",
    "\n",
    "    dx['DX'] = dx['DX'].astype(str)\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].astype(str)\n",
    "    dx['DX_TYPE'] = dx['DX_TYPE'].replace('09', '9')\n",
    "    \n",
    "    # load & process demo data\n",
    "    demo = pd.read_pickle(filepath_lst[0]+'AKI_DEMO'+'.pkl')  \n",
    "    demo['MALE'] = demo['SEX'] == 'M'\n",
    "\n",
    "    demo['RACE_WHITE'] = demo['RACE'] == '05'\n",
    "    demo['RACE_BLACK'] = demo['RACE'] == '03'\n",
    "    demo = demo[['PATID', 'ENCOUNTERID', 'AGE', 'MALE', 'RACE_WHITE', 'RACE_BLACK']]\n",
    "    demo = demo.drop_duplicates()\n",
    "    \n",
    "    # estimate SCr Baseline\n",
    "    pat_id_cols = ['PATID', 'ENCOUNTERID']\n",
    "    complete_df = df_scr[['ENCOUNTERID', 'PATID', 'ADMIT_DATE', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    " \n",
    "    # 1. min between the min of 1-week prior admission SCr and within 24 hour after admission SCr\n",
    "    # SCr within 24 hour after admission, that is admission day and one day after, get mean\n",
    "    admission_SCr = complete_df[(complete_df.SPECIMEN_DATE >= complete_df.ADMIT_DATE) & \\\n",
    "                                (complete_df.SPECIMEN_DATE <= (complete_df.ADMIT_DATE + pd.Timedelta(days=1)))].copy()\n",
    "\n",
    "    # Admission SCr is the mean of all the SCr within 24h admission\n",
    "    admission_SCr = admission_SCr.groupby(pat_id_cols)['RESULT_NUM'].mean().reset_index()\n",
    "\n",
    "    admission_SCr.rename(columns = {'RESULT_NUM': 'ADMISSION_SCR'}, inplace = True)\n",
    "\n",
    "    # merge the ADMISSION_SCR back to the main frame\n",
    "    complete_df = complete_df.merge(admission_SCr, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    # SCr within 7 days prior to admission\n",
    "    one_week_prior_admission = complete_df[(complete_df.SPECIMEN_DATE >= complete_df.ADMIT_DATE - pd.Timedelta(days=7)) & \\\n",
    "                                           (complete_df.SPECIMEN_DATE < complete_df.ADMIT_DATE)].copy()\n",
    "    one_week_prior_admission = one_week_prior_admission.sort_values(by = ['PATID', 'ENCOUNTERID','SPECIMEN_DATE'])\n",
    "    \n",
    "    if c7day == 'MOST_RECENT':\n",
    "        one_week_prior_admission = one_week_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].last().reset_index()\n",
    "    else:\n",
    "        one_week_prior_admission = one_week_prior_admission.groupby(pat_id_cols)[\"RESULT_NUM\"].min().reset_index()\n",
    "        \n",
    "    one_week_prior_admission.rename(columns = {'RESULT_NUM': 'ONE_WEEK_SCR'}, inplaced = True)\n",
    "\n",
    "    complete_df = complete_df.merge(one_week_prior_admission, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    # take the min between one week SCr and admission SCr\n",
    "    complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), 'BASELINE_EST_1'] = \\\n",
    "                np.min(complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), ['ONE_WEEK_SCR','ADMISSION_SCR']], axis = 1)\n",
    "    \n",
    "    cohort_table['ONE_WEEK_SCR_YES'] = complete_df.ONE_WEEK_SCR.notna().sum()\n",
    "    cohort_table['ONE_WEEK_SCR_NO'] = complete_df.ONE_WEEK_SCR.isna().sum()    \n",
    "    cohort_table['ONE_WEEK_SCR_ONE_WEEK_SCR'] = (complete_df.ONE_WEEK_SCR.notna() & (cohort_table['ONE_WEEK_SCR']==cohort_table['BASELINE_EST_1'])).sum()\n",
    "    cohort_table['ONE_WEEK_SCR_ADMISSION_SCR'] = (complete_df.ONE_WEEK_SCR.notna() & (cohort_table['ONE_WEEK_SCR']!=cohort_table['BASELINE_EST_1'])).sum()\n",
    "        \n",
    "    ori_num_unique_combinations = df_scr.groupby(['PATID', 'ENCOUNTERID']).ngroups\n",
    "    # get the percentage of encounters that do not have past 7-day records\n",
    "    criterion1_no_missing = complete_df.loc[complete_df.ONE_WEEK_SCR.notna(), :].groupby(pat_id_cols).ngroups\n",
    "    criterion1_missing_rate = 1 - criterion1_no_missing / ori_num_unique_combinations\n",
    "\n",
    "    # 2. pre-admission 365-7 day mean\n",
    "    # here we only care about SCr measurements within 1 year before hospitalization\n",
    "    one_year_prior_admission = complete_df[(complete_df.SPECIMEN_DATE < (complete_df.ADMIT_DATE - pd.Timedelta(days=7))) & \\\n",
    "                                     (complete_df.SPECIMEN_DATE >= (complete_df.ADMIT_DATE - pd.Timedelta(days=365.25)))].copy()\n",
    "    one_year_prior_admission = one_year_prior_admission.sort_values(by = ['PATID', 'ENCOUNTERID','SPECIMEN_DATE'])\n",
    "    one_year_prior_admission = one_year_prior_admission.loc[:, pat_id_cols + ['RESULT_NUM']]\n",
    "    \n",
    "    if c365day == 'AVERAGE':\n",
    "        one_year_prior_admission = one_year_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].mean().reset_index()\n",
    "    else:\n",
    "        one_year_prior_admission = one_year_prior_admission.groupby(pat_id_cols)['RESULT_NUM'].last().reset_index()  # or mean()\n",
    "    \n",
    "    one_year_prior_admission.rename(columns = {'RESULT_NUM': 'ONE_YEAR_SCR'}, inplace = True)\n",
    "    \n",
    "    complete_df = complete_df.merge(one_year_prior_admission, \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "    \n",
    "    # take the min between one week SCr and admission SCr\n",
    "    complete_df.loc[complete_df.ONE_YEAR_SCR.notna(), 'BASELINE_EST_2'] = \\\n",
    "                np.min(complete_df.loc[complete_df.ONE_YEAR_SCR.notna(), ['ONE_YEAR_SCR', 'ADMISSION_SCR']], axis = 1)\n",
    "\n",
    "    # priority 1: 7day SCr, priority 2: one year SCr\n",
    "    complete_df['BASELINE_NO_INVERT'] = \\\n",
    "                np.where(complete_df['BASELINE_EST_1'].isna(), complete_df['BASELINE_EST_2'], complete_df['BASELINE_EST_1'])\n",
    "\n",
    "    cohort_table['ONE_YEAR_SCR_YES'] = (complete_df.ONE_WEEK_SCR.notna() & complete_df.ONE_YEAR_SCR.notna()).sum()\n",
    "    cohort_table['ONE_YEAR_SCR_NO'] = (complete_df.ONE_WEEK_SCR.isna() & complete_df.ONE_YEAR_SCR.isna()).sum()\n",
    "    cohort_table['ONE_YEAR_SCR_ONE_WEEK_SCR'] = (complete_df.ONE_WEEK_SCR.notna() & complete_df.ONE_YEAR_SCR.notna() & (cohort_table['ONE_YEAR_SCR']==cohort_table['BASELINE_EST_2'])).sum()\n",
    "    cohort_table['ONE_YEAR_SCR_ADMISSION_SCR'] = (complete_df.ONE_WEEK_SCR.notna() & complete_df.ONE_YEAR_SCR.notna() & (cohort_table['ONE_YEAR_SCR']!=cohort_table['BASELINE_EST_2'])).sum()    \n",
    "    \n",
    "    # 3. Invert CKD-EPI (2021) to estimate baseline (only for non-CKD patients)\n",
    "    # get those encounters for which we need to impute baseline\n",
    "    pat_to_invert = complete_df.loc[complete_df.BASELINE_NO_INVERT.isna(), pat_id_cols+['ADMIT_DATE', 'ADMISSION_SCR']]\n",
    "    # one patient one row\n",
    "    pat_to_invert.drop_duplicates(subset=pat_id_cols, keep='first', inplace = True)\n",
    "\n",
    "\n",
    "    pat_dx = pat_to_invert.merge(dx.drop(['ENCOUNTERID', 'ADMIT_DATE'], axis = 1), \n",
    "                                              on = 'PATID', \n",
    "                                              how = 'left')\n",
    "\n",
    "    # calculate DX_DATE when it is missing\n",
    "    pat_dx.loc[pat_dx.DX_DATE.isna(), 'DX_DATE'] = \\\n",
    "            pat_dx.loc[pat_dx.DX_DATE.isna(), 'ADMIT_DATE'] + \\\n",
    "            pd.to_timedelta(pat_dx.loc[pat_dx.DX_DATE.isna(), 'DAYS_SINCE_ADMIT'], unit='D')\n",
    "\n",
    "    # check patients that do not have DX in the database\n",
    "    #pat_dx.DX_DATE.isna().mean()\n",
    "\n",
    "    # filter out those DX after admission\n",
    "    pat_dx = pat_dx[pat_dx.DX_DATE <= pat_dx.ADMIT_DATE]\n",
    "\n",
    "    # get the default eGFR for inversion: default to 75 for non-CKD patients, average of eGFR from staging criteria for CKD patients\n",
    "    pat_dx['DFLT_eGFR'] = 75\n",
    "\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.3', 'N18.3']), 'DFLT_eGFR'] = 90/2\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.4', 'N18.4']), 'DFLT_eGFR'] = 45/2\n",
    "    pat_dx.loc[pat_dx['DX'].isin(['585.5', 'N18.5']), 'DFLT_eGFR'] = 15/2\n",
    "#    pat_dx.loc[pat_dx['DX'].isin(['585.6', 'N18.6']), 'DFLT_eGFR'] = 15/2\n",
    "\n",
    "    pat_def_egfr = pat_dx.groupby(pat_id_cols)['DFLT_eGFR'].min().reset_index()\n",
    "\n",
    "    cohort_table['MDRD_NOCKD'] = (pat_def_egfr['DFLT_eGFR'] == 75)\n",
    "    cohort_table['MDRD_CKD3']  = (pat_def_egfr['DFLT_eGFR'] == 90/2)\n",
    "    cohort_table['MDRD_CKD4']  = (pat_def_egfr['DFLT_eGFR'] == 45/2)\n",
    "    cohort_table['MDRD_CKD5']  = (pat_def_egfr['DFLT_eGFR'] == 15/2)\n",
    "        \n",
    "    pat_to_invert= pat_to_invert.merge(pat_def_egfr, on = pat_id_cols, how = 'left')\n",
    "    pat_to_invert['DFLT_eGFR'] = pat_to_invert['DFLT_eGFR'].fillna(75)\n",
    "\n",
    "    pat_to_invert['DROPCKD'] = pat_to_invert['DFLT_eGFR'] != 75\n",
    "    \n",
    "    #pat_to_invert.DFLT_eGFR.value_counts()\n",
    "\n",
    "    # Backcalculation for patients\n",
    "    # merge DEMO with pat_to_invert\n",
    "    pat_to_invert = pat_to_invert.merge(demo, on = pat_id_cols, how = 'left')\n",
    "    \n",
    "    # estimate SCr from eGFR\n",
    "    pat_to_invert.loc[:, 'BASELINE_INVERT'] = pat_to_invert.apply(inverse_MDRD, args = (KDIGO_baseline,), axis = 1) #pat_to_invert.apply(inverse_CKDEPI21, axis = 1)\n",
    "\n",
    "    # take minimum of inverted SCr and admission SCr\n",
    "    pat_to_invert['BASELINE_EST_3'] = np.min(pat_to_invert[['ADMISSION_SCR', 'BASELINE_INVERT']], axis = 1)\n",
    "\n",
    "    # merge back the computation results\n",
    "    complete_df = complete_df.merge(pat_to_invert[pat_id_cols + ['BASELINE_EST_3']], \n",
    "                                    on = pat_id_cols,\n",
    "                                    how = 'left')\n",
    "\n",
    "    # replace the old baseline\n",
    "    complete_df['SERUM_CREAT_BASE'] = np.min(complete_df[['BASELINE_NO_INVERT', 'BASELINE_EST_3']], axis = 1)\n",
    "\n",
    "    if cckd:\n",
    "        complete_df = complete_df[complete_df['DROPCKD'] & complete_df['BASELINE_NO_INVERT'].isna()]\n",
    "        \n",
    "    # drop those still cannot find baseline\n",
    "    complete_df = complete_df.dropna(subset=['SERUM_CREAT_BASE'])\n",
    "\n",
    "    return complete_df.drop_duplicates(), cohort_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df4839-22a6-4af4-9329-6afdd5ad9294",
   "metadata": {},
   "source": [
    "#### 2.2. Determining RRT Status\n",
    "\n",
    "Determining whether a patient has received renal replacement therapy (RRT), which includes both dialysis procedures and kidney transplants, is essential for accurately staging AKI. The following function can be used to identify the sub-cohort of patients who have received RRT and to determine the timing of the RRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32d3dc74-0507-4f8e-94c5-55d818c0ef48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def determine_rrt_status(df_admit, filepath_lst):\n",
    "    px = pd.read_pickle(filepath_lst[0]+'AKI_PX.pkl')   \n",
    "\n",
    "    idx_transplant = np.logical_or(np.logical_or(\n",
    "                           np.logical_and(px['PX_TYPE']=='CH',px['PX'].isin(['50300','50320','50323','50325','50327','50328','50329','50340','50360','50365','50370','50380'])),\n",
    "                           np.logical_and(px['PX_TYPE']=='09',px['PX'].isin(['55.51','55.52','55.53','55.54','55.61','55.69']))),np.logical_or(\n",
    "                           np.logical_and(px['PX_TYPE']=='9',px['PX'].isin(['55.51','55.52','55.53','55.54','55.61','55.69'])),                       \n",
    "                           np.logical_and(px['PX_TYPE']=='10',px['PX'].isin(['0TY00Z0','0TY00Z1','0TY00Z2','0TY10Z0','0TY10Z1','0TY10Z2','0TB00ZZ','0TB10ZZ','0TT00ZZ','0TT10ZZ','0TT20ZZ']))))\n",
    "\n",
    "    idx_dialysis =(((px['PX_TYPE']=='CH') & (px['PX'].isin(['90935', '90937']))) |  \n",
    "                  ((px['PX_TYPE']=='CH') & (pd.to_numeric(px['PX'], errors='coerce').between(90940, 90999))) |   \n",
    "                  ((px['PX_TYPE']=='9') & ((px['PX'].isin(['39.93','39.95','54.98', 'V45.11'])))) | \n",
    "                  ((px['PX_TYPE']=='09') & (px['PX'].isin(['39.93','39.95','54.98', 'V45.11']))) |  \n",
    "                  ((px['PX_TYPE']=='10') & (px['PX'].isin(['5A1D00Z','5A1D60Z','5A1D70Z','5A1D80Z','5A1D90Z', 'Z99.2'])))) \n",
    " \n",
    "    rrt_stage =  px[idx_transplant | idx_dialysis] \n",
    "\n",
    "    rrt_stage = rrt_stage[['PATID','ENCOUNTERID','PX_DATE']]\n",
    "    rrt_stage.columns = ['PATID','ENCOUNTERID','RRT_ONSET_DATE']\n",
    "\n",
    "    rrt_stage = rrt_stage.merge(df_admit, on=['PATID', 'ENCOUNTERID'], how='inner')\n",
    "    rrt_stage['RRT_SINCE_ADMIT'] = (rrt_stage['RRT_ONSET_DATE']-rrt_stage['ADMIT_DATE']).dt.total_seconds()/(3600*24)\n",
    "    rrt_stage = rrt_stage.loc[rrt_stage[['ENCOUNTERID', 'RRT_SINCE_ADMIT']].groupby('ENCOUNTERID').idxmin().reset_index()['RRT_SINCE_ADMIT']]\n",
    "    rrt_stage.drop('ADMIT_DATE', axis = 1, inplace = True)\n",
    "    return rrt_stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16ad1c-5101-4067-a98f-ba8f687ef163",
   "metadata": {},
   "source": [
    "#### 2.3. Determining AKI Onset and Staging\n",
    "\n",
    "After preparing the necessary helper functions, we use the following function to determine AKI onset and staging based on the KDIGO definition presented before. This function combines the calculated baseline SCr values, admission data, and RRT status to identify the onset of AKI and stage its severity. \n",
    "\n",
    "The `get_aki_onset` function performs the following steps:\n",
    "1. Merges baseline SCr values with onset data.\n",
    "2. Calculates the minimum baseline SCr over the last 2 days.\n",
    "3. Checks for AKI Stage 1 criteria based on 1.5x increase from baseline within 7 days or 0.3 mg/dl increase from the 48-hr minimum.\n",
    "4. Identifies the earliest AKI Stage 1 onset for each patient.\n",
    "5. Checks for AKI Stage 2 (2.0x - <3.0x increase) and AKI Stage 3 (>=3.0x increase or >=4 mg/dl SCr) criteria.\n",
    "6. Identifies patients who initiated RRT after AKI onset.\n",
    "7. Merges AKI staging information and determines the final stage by taking the highest AKI stage recorded for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7876c218-312f-48a8-b136-9d64a38f2d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_aki_onset(df_scr, df_admit, df_rrt, df_baseline, aki_criteria = 'either'):\n",
    "    xxx = df_scr.copy()\n",
    "    yyy = df_admit.copy()\n",
    "\n",
    "    zzz = df_baseline[['PATID', 'ENCOUNTERID', 'SERUM_CREAT_BASE']].drop_duplicates()\n",
    "    zzz.columns= ['PATID', 'ENCOUNTERID',  'RESULT_NUM_BASE_7d']\n",
    "    xxx = xxx.merge(zzz, on = ['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "    zzz2 = xxx[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']].groupby(['PATID', 'ENCOUNTERID']).rolling('2d', on='SPECIMEN_DATE').min().reset_index()\n",
    "    zzz2 = zzz2[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM']]\n",
    "    zzz2.columns= ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM_BASE_2d']\n",
    "    xxx = xxx.merge(zzz2, on = ['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE'], how='left')\n",
    "\n",
    "    # Check condition for AKI1\n",
    "    #1.5 increase in 7 days\n",
    "    xxx['AKI1.5'] = (xxx['RESULT_NUM']>=1.5*xxx['RESULT_NUM_BASE_7d']) & (xxx['DAYS_SINCE_ADMIT']>=0) \n",
    "    #0.3 increase in 48 hours\n",
    "    xxx['AKI0.3'] = (xxx['RESULT_NUM']-xxx['RESULT_NUM_BASE_2d']>=0.3) & (xxx['DAYS_SINCE_ADMIT']>=0)      \n",
    "    \n",
    "    if aki_criteria == '2d':\n",
    "        xxx = xxx[xxx['AKI0.3']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        \n",
    "    elif aki_criteria == '7d':\n",
    "        xxx = xxx[xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_2d']\n",
    "        \n",
    "    elif aki_criteria == 'either':\n",
    "        xxx = xxx[xxx['AKI0.3'] | xxx['AKI1.5']]\n",
    "        xxx = xxx.sort_values(['SPECIMEN_DATE', 'RESULT_NUM'], ascending=[True, False])\n",
    "        xxx_backup = xxx.copy()\n",
    "        xxx = xxx.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "        xxx['RESULT_NUM_BASE'] = xxx['RESULT_NUM_BASE_7d']\n",
    "        #xxx.loc[xxx['AKI0.3'], 'RESULT_NUM_BASE'] = xxx.loc[xxx['AKI0.3'], 'RESULT_NUM_BASE_2d']\n",
    "\n",
    "    xxx['AKI1_SINCE_ADMIT'] = xxx['DAYS_SINCE_ADMIT'].copy()\n",
    "    xxx['AKI1_DATE'] = xxx['SPECIMEN_DATE'].copy()\n",
    "    xxx['AKI1_SCR'] = xxx['RESULT_NUM'].copy()\n",
    "    xxx['SCR_BASELINE'] = xxx['RESULT_NUM_BASE'].copy()\n",
    "    xxx['SCR_REFERENCE'] = xxx['RESULT_NUM_BASE_2d'].copy()\n",
    "    xxx['AKI1_7D'] = xxx['AKI1.5'].copy()\n",
    "    xxx['AKI1_2D'] = xxx['AKI0.3'].copy()\n",
    "    xxx = xxx[['PATID', 'ENCOUNTERID', 'SCR_BASELINE', 'SCR_REFERENCE',  'AKI1_DATE', 'AKI1_SCR', 'AKI1_SINCE_ADMIT', 'AKI1_7D', 'AKI1_2D']]\n",
    "\n",
    "    # Check condition for AKI2: 2.0x - <3.0x\n",
    "    aki2 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki2 = aki2[aki2['SPECIMEN_DATE']>=aki2['AKI1_DATE']]\n",
    "    aki2 = aki2[aki2['RESULT_NUM']>=2*aki2['SCR_BASELINE']]\n",
    "    aki2 = aki2.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki2['AKI2_SINCE_ADMIT'] = aki2['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki2['AKI2_DATE'] = aki2['SPECIMEN_DATE'].copy()\n",
    "    aki2['AKI2_SCR'] = aki2['RESULT_NUM'].copy()\n",
    "    aki2 = aki2[['PATID', 'ENCOUNTERID', 'AKI2_DATE', 'AKI2_SCR', 'AKI2_SINCE_ADMIT']]\n",
    "    \n",
    "    # Check condition for AKI3: SCR >= 3.0x Baseline\n",
    "    aki3 = xxx.merge(xxx_backup, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    aki3 = aki3[aki3['SPECIMEN_DATE']>=aki3['AKI1_DATE']]\n",
    "    aki3 = aki3[(aki3['RESULT_NUM']>=3*aki3['SCR_BASELINE']) | (aki3['RESULT_NUM']>=4)]\n",
    "    aki3 = aki3.groupby(['PATID', 'ENCOUNTERID']).first().reset_index()\n",
    "    aki3['AKI3_SINCE_ADMIT'] = aki3['DAYS_SINCE_ADMIT'].copy()\n",
    "    aki3['AKI3_DATE'] = aki3['SPECIMEN_DATE'].copy()\n",
    "    aki3['AKI3_SCR'] = aki3['RESULT_NUM'].copy()\n",
    "    aki3 = aki3[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    \n",
    "    # Check condition for AKI3: initiation of RRT\n",
    "    #df_rrt = get_rrt(path, ext, sep, yyy)\n",
    "    rrt = df_rrt.merge(xxx[['PATID', 'ENCOUNTERID', 'AKI1_DATE']], on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    rrt = rrt[rrt['RRT_ONSET_DATE'] >= rrt['AKI1_DATE']]\n",
    "    aki3b =  aki3.merge(rrt, on = ['PATID', 'ENCOUNTERID'], how = 'outer')\n",
    "    cond_rrt = (aki3b['RRT_SINCE_ADMIT'] < aki3b['AKI3_SINCE_ADMIT']) | (aki3b['AKI3_SINCE_ADMIT'].isna() & aki3b['RRT_SINCE_ADMIT'].notna())\n",
    "    aki3b.loc[cond_rrt, 'AKI3_SINCE_ADMIT'] = aki3b.loc[cond_rrt, 'RRT_SINCE_ADMIT']\n",
    "    aki3b.loc[cond_rrt, 'AKI3_DATE'] = aki3b.loc[cond_rrt, 'RRT_ONSET_DATE']\n",
    "    \n",
    "    aki3_all = aki3b[['PATID', 'ENCOUNTERID', 'AKI3_DATE', 'AKI3_SINCE_ADMIT', 'AKI3_SCR']]\n",
    "    \n",
    "    # Merge AKI staging information\n",
    "    onset = xxx.merge(aki2, on=['PATID', 'ENCOUNTERID'], how='outer').merge(aki3_all, on=['PATID', 'ENCOUNTERID'], how='outer')\n",
    "    onset = onset.merge(yyy, on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "\n",
    "    onset.columns = onset.columns.str.upper()\n",
    "    onset['ONSET_DATE'] = onset['AKI1_DATE'].copy()  \n",
    "    onset['SCR_ONSET'] = onset['AKI1_SCR'].copy() \n",
    "    \n",
    "    onset['DISCHARGE_SINCE_ONSET'] = (onset['DISCHARGE_DATE'] - onset['ONSET_DATE']).dt.days\n",
    "    \n",
    "    onset = onset[['PATID','ENCOUNTERID', 'ADMIT_DATE', 'DISCHARGE_DATE', \n",
    "                   'ONSET_DATE', 'AKI1_SINCE_ADMIT', 'AKI2_SINCE_ADMIT', \n",
    "                   'AKI3_SINCE_ADMIT',  'DISCHARGE_SINCE_ONSET','SCR_ONSET', \n",
    "                   'SCR_BASELINE',  'SCR_REFERENCE', 'AKI1_7D', 'AKI1_2D']]\n",
    "\n",
    "    onset['FLAG'] = (onset['AKI2_SINCE_ADMIT'].notna()) | (onset['AKI3_SINCE_ADMIT'].notna())\n",
    "    onset['ONSET_SINCE_ADMIT'] = onset['AKI1_SINCE_ADMIT'].copy()  \n",
    "    \n",
    "    # Get AKI stage by taking the final stage\n",
    "    onset['AKI_STAGE'] = 0\n",
    "    filter_aki3 = onset['AKI3_SINCE_ADMIT'].notna()\n",
    "    filter_aki2 = onset['AKI2_SINCE_ADMIT'].notna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    filter_aki1 = onset['AKI1_SINCE_ADMIT'].notna() & onset['AKI2_SINCE_ADMIT'].isna() & onset['AKI3_SINCE_ADMIT'].isna()\n",
    "    \n",
    "    onset.loc[filter_aki3, 'AKI_STAGE'] = 3\n",
    "    onset.loc[filter_aki2, 'AKI_STAGE'] = 2\n",
    "    onset.loc[filter_aki1, 'AKI_STAGE'] = 1\n",
    "    \n",
    "    return onset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94cad2-2fd7-49f8-9205-9c6d9387869e",
   "metadata": {},
   "source": [
    "#### 2.4. Determining AKI Recovery Status\n",
    "\n",
    "The following functions are used to determine whether an episode of AKI is resolving or non-resolving. Resolving AKI is defined as a decrease in serum creatinine concentration of 0.3 mg/dL or more, or 25% or more from the maximum in the first 72 hours after AKI onset. Non-resolving AKI does not meet these criteria.\n",
    "\n",
    "- The `akiresolving` function evaluates whether an AKI episode is resolving based on sCr values:\n",
    "    1. Filters the data to include only the first 72 hours after AKI onset.\n",
    "    2. Identifies the maximum sCr value during this period for each patient.\n",
    "    3. Evaluates whether the sCr decrease meets the criteria for resolving AKI (a decrease of ≥0.3 mg/dL or 25% or more from the maximum).\n",
    "    4. Flags patients who meet these criteria with an `AKI_resolving` status.\n",
    "\n",
    "\n",
    "- The `akiresolvingsustain` function checks if the resolving status is sustained within the 72-hour period:\n",
    "    1. Identifies the first time point when the resolving criteria were met.\n",
    "    2. Checks subsequent sCr values to ensure that the resolving status is sustained.\n",
    "    3. Flags patients with sustained resolving AKI using the `sustain` variable.\n",
    "\n",
    "\n",
    "- The `get_aki_resolving` function integrates the above steps to determine and summarize the AKI recovery status for each patient:\n",
    "    1. Filters the sCr data to include only observations after AKI onset.\n",
    "    2. Applies the `akiresolving` function to determine the resolving status.\n",
    "    3. Applies the `akiresolvingsustain` function to check if the resolving status is sustained.\n",
    "    4. Generates two outcome variables:\n",
    "       - `AKI_TRIGG`: Indicates whether the resolving condition was triggered.\n",
    "       - `AKI_RESOL`: Indicates whether the resolving condition was sustained.\n",
    "\n",
    "Finally, the function merges these outcomes with the onset data and returns the final dataset, summarizing the AKI recovery status for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "348c75b0-4744-4667-864c-1a8679b6b302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def akiresolving(dall, time, dlevel, ratio):\n",
    "    time_window = (dall['onset_day']<=3) & (dall['onset_day']>=time) \n",
    "    dall = dall[time_window].reset_index().drop('index',axis=1)\n",
    "    dall2 = dall.sort_values(['value', 'charttime'], ascending=[False, True])[['subject_id', 'hadm_id', 'charttime', 'value']]\n",
    "    max_idx = dall2.groupby(['subject_id', 'hadm_id'])['value'].idxmax()\n",
    "    dmax_sCr = dall2.loc[max_idx]\n",
    "    dmax_sCr.columns = ['subject_id', 'hadm_id', 'charttime_max', 'value_max']\n",
    "    dall = dall.merge(dmax_sCr, left_on=['subject_id', 'hadm_id'], right_on=['subject_id', 'hadm_id'], how='left')\n",
    "    # redefine time windows\n",
    "    time_window = (dall['onset_day']<=3) & (dall['onset_day']>=time)\n",
    "    dall['AKI_resolving']=0\n",
    "   \n",
    "    dall['ddays']= (dall['charttime']-dall['charttime_max'])\n",
    "    dall['dvalues'] = dall['value_max']-dall['value']\n",
    "    dall['dvalues2'] = dall['value']/dall['value_max']\n",
    "    # within 72 hrs, which means onset_day equals 1, 2, or 3\n",
    "    filter_lvl = (dall['ddays']>pd.Timedelta(value=0, unit='s')) & time_window & (dall['dvalues']>=dlevel)\n",
    "    filter_rat = (dall['ddays']>pd.Timedelta(value=0, unit='s')) & time_window & (dall['dvalues2']<=ratio)\n",
    "    dall.loc[filter_lvl,'AKI_resolving']=1\n",
    "    dall.loc[filter_rat,'AKI_resolving']=1\n",
    "    dall=dall.drop(['ddays', 'dvalues', 'dvalues2'],axis=1)\n",
    "    dall['AKI_resolving2']=dall['AKI_resolving'].copy()\n",
    "\n",
    "    dmaxRelv = dall[['subject_id', 'hadm_id', 'AKI_resolving']].groupby(['subject_id', 'hadm_id']).max().reset_index()\n",
    "    dmaxRelv.columns = ['subject_id', 'hadm_id', 'max_AKI_resolving']\n",
    "    dall = dall.merge(dmaxRelv, left_on=['subject_id', 'hadm_id'], right_on=['subject_id', 'hadm_id'], how='left')\n",
    "    dall['AKI_resolving'] = dall['max_AKI_resolving']\n",
    "    return dall.drop('max_AKI_resolving',axis = 1)\n",
    "\n",
    "def akiresolvingsustain(dall, time):\n",
    "    \n",
    "    dresolv = dall[dall['AKI_resolving2']==1]\n",
    "    dresolv = dresolv[['subject_id', 'hadm_id', 'charttime']].sort_values('charttime').groupby(['subject_id', 'hadm_id']).first().reset_index()\n",
    "    dresolv.columns = ['subject_id', 'hadm_id', 'charttime_resolv']\n",
    "\n",
    "    dall = dall.merge(dresolv, left_on=['subject_id', 'hadm_id'], right_on=['subject_id', 'hadm_id'], how='left')\n",
    "    time_window = (dall['onset_day']<=3) & (dall['onset_day']>=time) \n",
    "    dall['after_ddays'] = ((dall['charttime']-dall['charttime_resolv']) > pd.Timedelta(value=0, unit='s')) & time_window \n",
    "\n",
    "    dresolv2 = dall[dall['after_ddays']][['subject_id', 'hadm_id', 'AKI_resolving2']].groupby(['subject_id', 'hadm_id']).min().reset_index()\n",
    "    dresolv2.columns = ['subject_id', 'hadm_id', 'sustain']\n",
    "\n",
    "    dall = dall.merge(dresolv2, left_on=['subject_id', 'hadm_id'], right_on=['subject_id', 'hadm_id'], how='left')\n",
    "    dall['sustain'].fillna(True, inplace = True)\n",
    "    return dall\n",
    "\n",
    "def get_aki_resolving(scr, onset):\n",
    "    scr = scr[scr['DAYS_SINCE_ONSET']>=0]\n",
    "    scr = scr[['PATID', 'ENCOUNTERID', 'SPECIMEN_DATE', 'RESULT_NUM', 'DAYS_SINCE_ONSET']]\n",
    "    scr.columns = ['subject_id', 'hadm_id', 'charttime', 'value', 'onset_day']\n",
    "\n",
    "    scr = akiresolving(dall = scr, time = 0, dlevel = 0.3, ratio = 0.75)\n",
    "    scr = akiresolvingsustain(dall = scr, time = 0)\n",
    "\n",
    "    scr['AKI_sustain'] = scr['AKI_resolving']*scr['sustain']\n",
    "    scr['PATID'] = scr['subject_id']\n",
    "    scr['ENCOUNTERID'] = scr['hadm_id']\n",
    "    scr = scr[['PATID', 'ENCOUNTERID', 'AKI_resolving', 'AKI_sustain']].groupby(['PATID', 'ENCOUNTERID']).sum().reset_index()\n",
    "    scr['AKI_TRIGG'] = scr['AKI_resolving']>0\n",
    "    scr['AKI_RESOL'] = scr['AKI_sustain']>0\n",
    "    onset = onset.merge(scr[['PATID', 'ENCOUNTERID',  'AKI_TRIGG', 'AKI_RESOL']],on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "    return onset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8689a2e-7e9c-4977-9864-75ca47c672a4",
   "metadata": {},
   "source": [
    "#### 2.5. Processing AKI Onset and Recovery\n",
    "\n",
    "The `process_onset_and_recovery` function automates the processing of AKI onset and recovery data for multiple sites. This function integrates previously defined components to streamline the analysis, producing final datasets for both AKI onset and recovery status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5272ce1-4809-4fc1-921b-abe1cab32d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_onset_and_recovery(home_path, site):\n",
    "    filepath_lst = get_data_file_path(home_path, site)\n",
    "    df_scr, df_admit = load_onset_data(filepath_lst)\n",
    "    df_baseline, cohort_table = get_scr_baseline(df_scr, df_admit, filepath_lst)\n",
    "    df_rrt = determine_rrt_status(df_admit, filepath_lst)\n",
    "    onset = get_aki_onset(df_scr, df_admit, df_rrt, df_baseline)\n",
    "    onset.to_pickle(filepath_lst[1] + 'onset.pkl')\n",
    "    df_scr_filtered = load_and_filter_scr(onset, filepath_lst)\n",
    "    recovery = get_aki_resolving(scr = df_scr_filtered, onset = onset)\n",
    "    recovery.to_pickle(filepath_lst[1]+'recovery.pkl')\n",
    "    return onset, recovery, cohort_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09093c76-d435-4717-b8b8-3198d593e20e",
   "metadata": {},
   "source": [
    "#### 2.6. Main Script for Processing AKI Onset and Recovery Across Sites\n",
    "\n",
    "The script is designed to be executed iteratively across a list of sites, as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6252914-74e9-4a1f-8b96-76eb11adf397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing site KUMC: name 'load_onset_data' is not defined\n"
     ]
    }
   ],
   "source": [
    "home_path = '/blue/yonghui.wu/hoyinchan/Data/data2022/'\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename= home_path+'log/processing_errors.log', level=logging.ERROR)\n",
    "site_lst = ['KUMC', 'UTSW', 'MCW', 'UIOWA', 'UMHC', 'UTHSCSA', 'UPITT', 'UNMC', 'MCRI', 'UofU'] \n",
    "\n",
    "site_lst = ['KUMC']\n",
    "\n",
    "for site in site_lst:\n",
    "    try:\n",
    "        onset, recovery, cohort_table = process_onset_and_recovery(home_path, site)\n",
    "        print(f\"Finished generating AKI onset and recovery for {site}.\", flush = True)\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing site {site}: {str(e)}\"\n",
    "        logging.error(error_message)\n",
    "        print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcfbc91-a529-436e-a6f8-01cfc3be0bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
