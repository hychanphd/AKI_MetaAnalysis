{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.interpolate import BSpline, make_interp_spline, interp1d\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import csv\n",
    "from dfply import *\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'MCW'\n",
    "year = 2012\n",
    "datafolder = '/home/hchan2/AKI/data/'\n",
    "home_directory = \"/home/hchan2/AKI/AKI_Python/\"\n",
    "pred_end = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tables\n",
    "onset = pd.read_csv(datafolder+site+'/raw/'+'AKI_ONSETS'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "vital = pd.read_csv(datafolder+site+'/raw/'+'AKI_VITAL'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "demo = pd.read_csv(datafolder+site+'/raw/'+'AKI_DEMO'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "dx = pd.read_csv(datafolder+site+'/raw/'+'AKI_DX'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object', 'DX_TYPE': 'object', 'DX': 'object'}))\n",
    "px = pd.read_csv(datafolder+site+'/raw/'+'AKI_PX'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "lab = pd.read_csv(datafolder+site+'/raw/'+'AKI_LAB'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))\n",
    "amed = pd.read_csv(datafolder+site+'/raw/'+'AKI_AMED'+'.csv',sep=',', dtype=({'PATID': 'object', 'ENCOUNTERID': 'object'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rxnorm -> atc\n",
    "# pd.DataFrame(amed['MEDADMIN_CODE'].unique()).to_csv('/home/hchan2/AKI/AKI_Python/rxnormtmp.csv', sep=',', index=False, header = False)\n",
    "\n",
    "# Go to run rxnorm2atcR.ipynb NOW\n",
    "\n",
    "rxcui2atc = pd.read_csv(home_directory+'rxnorm_out_'+site+'.csv',sep=',') >> rename(MEDADMIN_CODE=X.Rxcui)\n",
    "amed = amed >> left_join(rxcui2atc, by='MEDADMIN_CODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICD10 -> ICD09\n",
    "#dx = pd.read_csv(datafolder+'/raw/'+'AKI_DX'+'.csv',sep=',', dtype=({'DX_TYPE': 'object', 'DX': 'object'}))\n",
    "icd10toicd09 = pd.read_csv(home_directory+'2018_I10gem.csv',sep=',')\n",
    "dx['DX'] = dx['DX'].map(lambda x: x.replace('.',''))\n",
    "icd10toicd09.columns = ['DX', 'DX09']\n",
    "#add fillna for unmatch ICD10\n",
    "dx3 = dx >> mask(X.DX_TYPE == '10') >> left_join(icd10toicd09, by='DX') >> mutate(DX_TYPE = '09', DX = X.DX09) >> select(~X.DX09)\n",
    "dx = pd.concat([dx >> mask(X.DX_TYPE != '10'), dx3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AKI1 paitients\n",
    "onset['ADMIT_DATE'] = pd.to_datetime(onset['ADMIT_DATE'])\n",
    "onset_yr = onset.query(\"ADMIT_DATE >= '\"+str(year)+\"/01/01' and ADMIT_DATE <= '\"+str(year)+\"/12/31'\")\n",
    "onset_yr_aki1 = onset_yr[onset_yr[\"AKI1_ONSET\"].notnull()]\n",
    "onset_yr_aki1_select = onset_yr_aki1[[\"PATID\", \"ENCOUNTERID\", \"AKI1_SINCE_ADMIT\"]]\n",
    "onset_yr_aki1_select.loc[onset_yr_aki1_select.AKI1_SINCE_ADMIT == 0, 'AKI1_SINCE_ADMIT'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATID = onset_yr_aki1_select['PATID'].tolist()\n",
    "ENCOUNTERID = onset_yr_aki1_select['ENCOUNTERID'].tolist()\n",
    "SINCE_ADMIT = onset_yr_aki1_select['AKI1_SINCE_ADMIT'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rethink if I need it TODO\n",
    "dx.loc[dx.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1\n",
    "px.loc[px.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1\n",
    "amed.loc[amed.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1\n",
    "lab.loc[lab.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1\n",
    "vital.loc[vital.DAYS_SINCE_ADMIT == 0, 'DAYS_SINCE_ADMIT'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expand data (Create row for each day for each encounter until pred end)\n",
    "newdf = pd.DataFrame(np.repeat(onset_yr_aki1_select.values,7,axis=0))\n",
    "newdf.columns = onset_yr_aki1_select.columns\n",
    "since_admit = [*range(7)]*onset_yr_aki1_select.shape[0]\n",
    "newdf['SINCE_ADMIT'] = since_admit\n",
    "conditions = [(newdf['SINCE_ADMIT'] < newdf['AKI1_SINCE_ADMIT']-1), (newdf['SINCE_ADMIT'] == newdf['AKI1_SINCE_ADMIT']-1), (newdf['SINCE_ADMIT'] > newdf['AKI1_SINCE_ADMIT']-1)]\n",
    "values = [0, 1, -1]\n",
    "newdf['FLAG'] = np.select(conditions, values)\n",
    "newdf = newdf.drop(newdf[newdf['FLAG']==-1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop unused column\n",
    "vital = vital.drop(['Row #','MEASURE_DATE_TIME','SMOKING', 'TOBACCO', 'TOBACCO_TYPE'],axis=1, errors='ignore')\n",
    "demo = demo.drop(['Row #','DEATH_DATE','BIRTH_DATE','DDAYS_SINCE_ENC','DEATH_DATE_IMPUTE','DEATH_SOURCE'],axis=1, errors='ignore')\n",
    "dx = dx.drop(['DX_SOURCE','DX_ORIGIN','PDX','DX_DATE'],axis=1, errors='ignore')\n",
    "px = px.drop(['Row #', 'PX_SOURCE','DX_ORIGIN','PDX','PX_DATE', 'PX_TYPE'],axis=1, errors='ignore')\n",
    "lab = lab.drop(['LAB_ORDER_DATE','SPECIMEN_DATE_TIME','RESULT_DATE_TIME','SPECIMEN_SOURCE','LAB_ORDER_DATE','SPECIMEN_DATE_TIME','RESULT_DATE_TIME','SPECIMEN_SOURCE','LAB_PX','LAB_PX_TYPE','RESULT_QUAL','RESULT_UNIT'],axis=1, errors='ignore')\n",
    "amed = amed.drop(['Row #','MEDADMIN_START_DATE_TIME','MEDADMIN_STOP_DATE_TIME','MEDADMIN_TYPE','MEDADMIN_CODE','MEDADMIN_DOSE_ADMIN','MEDADMIN_ROUTE','MEDADMIN_SOURCE'],axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate daily average\n",
    "vital_mean = vital.groupby(['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT']).mean().reset_index()\n",
    "lab_mean = lab.groupby(['PATID', 'ENCOUNTERID', 'LAB_LOINC', 'DAYS_SINCE_ADMIT']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Transform dx table (Historical data: Yes if any diagnoasis show up) (Assuming all DAYS_SINCE_ADMIT < 0) (Boolean)\n",
    "dx_t = dx >> mutate(DX=X.DX_TYPE+\":\"+X.DX) >> drop('DX_TYPE')\n",
    "dx_t = (dx_t >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='DX', values='dummy').fillna(False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Transform AMED Table (Exact date) (Boolean)\n",
    "amed_list = []\n",
    "for i in range(1,pred_end+1):\n",
    "    amed_p = (amed >> mask(X.DAYS_SINCE_ADMIT == i) >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).fillna('NI').pivot(index=['PATID', 'ENCOUNTERID'], columns='ATC4th', values='dummy').fillna(False).reset_index()\n",
    "    amed_p = amed_p >> mutate(SINCE_ADMIT = i)\n",
    "    amed_list.append(amed_p.copy())\n",
    "amed_t = pd.concat(amed_list, axis=0, ignore_index=True).fillna(False) \n",
    "amed_t = amed_t.astype({'PATID': 'float64', 'ENCOUNTERID':'float64'})\n",
    "#TODO test if not amed data in onset paitient\n",
    "# [x for x in newdf['PATID'] if x in amed_t['PATID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Transform LAB Table (Row over the previous value if unknown) (Continuous)\n",
    "lab_list = []\n",
    "lab_p = (lab_mean >> mask(X.DAYS_SINCE_ADMIT == 1) >> drop('DAYS_SINCE_ADMIT') >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='LAB_LOINC', values='RESULT_NUM').reset_index()\n",
    "lab_p = lab_p >> mutate(SINCE_ADMIT = 1)   \n",
    "lab_list.append(lab_p.copy())\n",
    "for i in range(2,pred_end+1):\n",
    "    lab_p = (lab_mean >> mask(X.DAYS_SINCE_ADMIT == i) >> drop('DAYS_SINCE_ADMIT') >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='LAB_LOINC', values='RESULT_NUM').reset_index()\n",
    "    # fill in previous day value (row over if unknown)\n",
    "    lab_p = lab_p.join(lab_list[-1], lsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "    lab_p = lab_p.combine_first(lab_list[-1])\n",
    "    lab_p = lab_p >> mutate(SINCE_ADMIT = i)    \n",
    "    lab_list.append(lab_p.copy())\n",
    "lab_t = pd.concat(lab_list, axis=0, ignore_index=True) \n",
    "lab_t = lab_t.astype({'PATID': 'float64', 'ENCOUNTERID':'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Transform PX Table (Exact Date) (Boolean)\n",
    "px_list = []\n",
    "for i in range(1,pred_end+1):\n",
    "    px_p = (px >> mask(X.DAYS_SINCE_ADMIT == i) >> drop('DAYS_SINCE_ADMIT') >> mutate(dummy = True) >> distinct()).pivot(index=['PATID', 'ENCOUNTERID'], columns='PX', values='dummy').fillna(False).reset_index()\n",
    "    px_p = px_p >> mutate(SINCE_ADMIT = i)\n",
    "    px_list.append(px_p.copy())\n",
    "px_t = pd.concat(px_list, axis=0, ignore_index=True).fillna(False) \n",
    "px_t = px_t.astype({'PATID': 'float64', 'ENCOUNTERID':'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join different Tables\n",
    "newdf1 = pd.merge(newdf, vital_mean, left_on=['PATID', 'ENCOUNTERID', 'AKI1_SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT'], how='left')\n",
    "newdf2 = pd.merge(newdf1, demo, left_on=['PATID', 'ENCOUNTERID'], right_on=['PATID', 'ENCOUNTERID'], how='left')\n",
    "newdf3 = pd.merge(newdf2, amed_t, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "newdf3 = newdf3.combine_first(newdf3[list(amed_t.select_dtypes('bool').columns)].fillna(False))\n",
    "newdf4 = pd.merge(newdf3, lab_t, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "newdf5 = pd.merge(newdf4, px_t, left_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'SINCE_ADMIT'], how='left')\n",
    "# Rethink if ENCOUNTERID needed for dx (Yes, since negative SINCE_ADMIT take care of that)\n",
    "newdf6 = pd.merge(newdf5, dx_t, left_on=['PATID', 'ENCOUNTERID', 'AKI1_SINCE_ADMIT'], right_on=['PATID', 'ENCOUNTERID', 'DAYS_SINCE_ADMIT'], how='left')\n",
    "newdf7 = newdf4.drop(['PATID', 'ENCOUNTERID', 'AKI1_SINCE_ADMIT', 'SINCE_ADMIT', 'DAYS_SINCE_ADMIT','DAYS_SINCE_ADMIT_x'],axis=1, errors='ignore')\n",
    "newdf8 = newdf7.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imputation for categorical data\n",
    "#newdf8.loc[:, newdf8.dtypes == 'object'] = newdf8.loc[:, newdf8.dtypes == 'object'].fillna('NI')\n",
    "newdf8 = newdf8.combine_first(newdf8.select_dtypes('object').fillna('NI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to onehotencoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(newdf8.select_dtypes('object'))\n",
    "newdf8_onehot_cat = pd.DataFrame(enc.transform(newdf8.select_dtypes('object')).toarray(), columns=enc.get_feature_names(newdf8.select_dtypes('object').columns)).astype('bool')\n",
    "data = pd.concat([newdf8.select_dtypes(['float64', 'int64']).reset_index(), newdf8_onehot_cat], axis=1).drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "label = data['FLAG']\n",
    "data = data[data.columns[data.columns!='FLAG']]\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(data, label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_fea = ['SEX', 'RACE', 'HISPANIC']\n",
    "#newdf4 = newdf4.drop(cat_fea)\n",
    "#cat_inx = [list(X_train.columns).index(x) for x in cat_fea]\n",
    "#cat_noninx = [list(X_train.columns).index(x) for x in list(X_train.columns) if not(x in cat_fea)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute continuous value for SMOTE\n",
    "imp_num = sklearn.impute.SimpleImputer()\n",
    "imp_num.fit(X_train.select_dtypes(['int64', 'float64']))\n",
    "X_train_imp = X_train.reset_index().combine_first(pd.DataFrame(imp_num.transform(X_train.select_dtypes(['int64', 'float64'])), columns=X_train.select_dtypes(['int64', 'float64']).columns)).drop('index',axis=1)\n",
    "#cat_fea = [X_train_imp.columns.get_loc(c) for c in list(X_train_imp.select_dtypes('bool').columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "sm = SMOTENC(categorical_features=X_train_imp.dtypes == 'bool')\n",
    "X_res, y_res = sm.fit_resample(X_train_imp, y_train)\n",
    "X_train_onehot_com = pd.concat([X_res, X_train], axis=0)\n",
    "y_train_com = pd.concat([y_res, y_train], axis=0)\n",
    "X_test = X_test[X_train_onehot_com.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train_onehot_com, label=y_train_com)\n",
    "dtest  = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': [10], 'objective': ['binary:logistic'], 'learning_rate': [0.01, 0.1]}\n",
    "params['nthread'] = [4]\n",
    "params['min_child_weight'] = [1]\n",
    "params['subsample'] = [0.8]\n",
    "params['colsample_bytree'] = [0.8]\n",
    "params['gamma'] = [1]\n",
    "params['eval_metric'] = ['auc']\n",
    "#params[\"print.every.n\"] = [50]\n",
    "#params[\"maximize\"] = [True]\n",
    "params[\"nthread\"] = [-1]\n",
    "#params[\"early_stopping_rounds\"] = [50]\n",
    "#params[\"num_boost_round\"] = [1000]\n",
    "\n",
    "order = params.keys()\n",
    "params = pd.DataFrame(itertools.product(*[params[k] for k in order]), columns=order).to_dict(orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, cvboosters):\n",
    "        self._cvboosters = cvboosters\n",
    "    \n",
    "    def after_training(self, model):\n",
    "        self._cvboosters[:] = [cvpack.bst for cvpack in model.cvfolds]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross validation get initial guess\n",
    "best_auc = 0\n",
    "for param in params:\n",
    "    cvboosters = []\n",
    "    bst_10 = xgb.cv(param, dtrain, nfold=10, stratified=True, callbacks=[SaveBestModel(cvboosters), ], num_boost_round=1000, early_stopping_rounds=50, maximize=True, verbose_eval = 50)    \n",
    "    if np.mean(bst_10['test-auc-mean']) > best_auc:\n",
    "        best_auc = np.mean(bst_10['test-auc-mean'])\n",
    "        best_params = param\n",
    "#    best_model = cvboosters[np.argmax(bst_10['test-auc-mean'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune\n",
    "#bst = xgb.train(param)\n",
    "bst = xgb.train(best_params, dtrain, evals=[(dtrain, 'Train'), (dtest, 'Test')], num_boost_round=1000, early_stopping_rounds=50, maximize=True, verbose_eval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'max_depth': 10, 'learning_rate' : 0.1, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'n_jobs': 23, 'verbosity': 1}\n",
    "params = best_params\n",
    "model = XGBClassifier(**params)\n",
    "eval_set = [(X_train_onehot_com, y_train_com), (X_test, y_test)]\n",
    "print(params)\n",
    "model.fit(X_train_onehot_com, y_train_com, eval_set=eval_set, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(model))\n",
    "model.n_classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(model.get_booster())\n",
    "ax.figure.savefig(datafolder+'/'+site+\"_\"+str(year)+\"_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_booster().get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP value\n",
    "#dtrain = xgb.DMatrix(X_train_onehot_com)\n",
    "shap = model.get_booster().predict(dtest, pred_contribs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect SHAP value\n",
    "shap_data = list()\n",
    "for i in range(X_train_onehot_com.columns.shape[0]):\n",
    "    df = pd.DataFrame(list(zip(X_train_onehot_com.iloc[:,i], shap[:, i])),columns =['Name', 'val'])\n",
    "    plot_data = df.groupby(\"Name\").agg([np.mean, np.std]).reset_index().fillna(0)\n",
    "    plot_data = plot_data >> mutate(Feature=X_train_onehot_com.columns[i])\n",
    "    plot_data.columns = [''.join(x) for x in plot_data.columns]\n",
    "    shap_data.append(plot_data.copy())\n",
    "shap_data = pd.concat(shap_data)\n",
    "used_feature = list(bst.get_score().keys())\n",
    "shap_data= shap_data[shap_data['Feature'].isin(used_feature)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print SHAP plot\n",
    "for f in np.unique(shap_data['Feature']):\n",
    "    plot_data = shap_data >> mask(X.Feature == f) >> select(X.Name, X.valmean, X.valstd)\n",
    "    plt.clf()\n",
    "    plt.figure()    \n",
    "    plt.scatter(x=plot_data['Name'],y=plot_data['valmean'])\n",
    "    plt.errorbar(plot_data['Name'],plot_data['valmean'], yerr=plot_data['valstd'], fmt=\"o\")\n",
    "    plt.title(f)\n",
    "    if plot_data.shape[0] > 2:\n",
    "        spl = np.polynomial.legendre.Legendre.fit(plot_data['Name'], plot_data['valmean'],5, full=True)\n",
    "        [spline_x, spline_y] = spl[0].linspace()\n",
    "        plt.plot(spline_x, spline_y)        \n",
    "    plt.show()\n",
    "    plt.savefig(datafolder+'/'+site+\"_\"+str(year)+\"_\"+X_train_onehot_com.columns[i]+'.png')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "model_data = pd.concat([pd.DataFrame(bst.get_score(importance_type='cover'), index=['Cover']), \\\n",
    "pd.DataFrame(bst.get_score(importance_type='gain'), index=['Gain']), \\\n",
    "pd.DataFrame(bst.get_score(importance_type='weight'), index=['Frequency'])]).transpose() >> mutate(Feature = X.index)\n",
    "model_data['rank'] = model_data['Frequency'].rank(method='min', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv for metaregression\n",
    "shap_data = shap_data >> left_join(model_data, by='Feature')\n",
    "siteyr = site+'_1d_no_fs_stg1up'+'_'+str(year)\n",
    "shap_data >> mutate(site=siteyr) >> rename(fval=X.Name) >> rename(mean_val=X.valmean) >> rename(se_val=X.valstd)\n",
    "shap_data.to_pickle(datafolder+'/model_explain/'+siteyr+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using shap package example\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(bst)\n",
    "shap_values = explainer.shap_values(X_train_onehot_com)\n",
    "shap.summary_plot(shap_values, X_train_onehot_com, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
